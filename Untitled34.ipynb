{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19941\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('KRfin01.csv')\n",
    "df2 = pd.read_csv('KRfin02.csv')\n",
    "df = df.append(df2, ignore_index=True)\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,val_X,test_X=np.array(df.iloc[:15952,1:-1]),np.array(df.iloc[15952::2,1:-1]),np.array(df.iloc[15953::2,1:-1])\n",
    "train_Y,val_Y,test_Y=np.array(df.iloc[:15952,-1]),np.array(df.iloc[15952::2,-1]),np.array(df.iloc[15953::2,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train set(15952): 7928 win and 8024 lose\n",
      "In val set(1995): 960 win and 1035 lose\n",
      "In test set(1994): 975 win and 1019 lose\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(f\"In train set(15952): {Counter(train_Y)[0]} win and {Counter(train_Y)[1]} lose\")\n",
    "print(f\"In val set(1995): {Counter(val_Y)[0]} win and {Counter(val_Y)[1]} lose\")\n",
    "print(f\"In test set(1994): {Counter(test_Y)[0]} win and {Counter(test_Y)[1]} lose\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=TensorDataset(torch.from_numpy(train_X),torch.from_numpy(train_Y))\n",
    "val_data=TensorDataset(torch.from_numpy(val_X),torch.from_numpy(val_Y))\n",
    "test_data=TensorDataset(torch.from_numpy(test_X),torch.from_numpy(test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find out the number positive and negative examples in the training set\n",
    "num_classes=[Counter(train_Y)[0],Counter(train_Y)[1]]\n",
    "#find the respective weights (mind that weights need to be a torch tensor)\n",
    "weights=1./torch.tensor(num_classes,dtype=float)\n",
    "#create a torch tensor associating the train_Y and the weights. Sample Weights is a torch tensor\n",
    "sample_weights=weights[train_Y]\n",
    "sampler=torch.utils.data.WeightedRandomSampler(weights=sample_weights,num_samples=len(sample_weights),replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "train_loader=DataLoader(train_data,sampler=sampler,batch_size=batch_size)\n",
    "val_loader=DataLoader(val_data,shuffle=True,batch_size=batch_size)\n",
    "test_loader=DataLoader(test_data,shuffle=False,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4902, 0.5414, 0.5335,  ..., 0.5032, 0.5615, 0.4982],\n",
      "        [0.5439, 0.5295, 0.5070,  ..., 0.4281, 0.4864, 0.5122],\n",
      "        [0.4708, 0.4774, 0.5718,  ..., 0.3555, 0.4096, 0.4899],\n",
      "        ...,\n",
      "        [0.5042, 0.4630, 0.4821,  ..., 0.4969, 0.4851, 0.5165],\n",
      "        [0.4990, 0.4870, 0.4820,  ..., 0.5456, 0.4477, 0.4538],\n",
      "        [0.4426, 0.4155, 0.4834,  ..., 0.5186, 0.4681, 0.5037]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "train_iter=iter(val_loader)\n",
    "inputs,outputs=train_iter.next()\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 23])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loader wrapping\n",
    "loaders={'train':train_loader, 'valid':val_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.fc1=nn.Linear(23,32)\n",
    "        self.norm1=nn.BatchNorm1d(32)\n",
    "        self.fc2=nn.Linear(32,16,bias=False)\n",
    "        self.norm2=nn.BatchNorm1d(16)\n",
    "        self.fc3=nn.Linear(16,8,bias=False)\n",
    "        self.norm3=nn.BatchNorm1d(8)\n",
    "        self.fc4=nn.Linear(8,5,bias=False)\n",
    "        self.norm4=nn.BatchNorm1d(5)\n",
    "        self.fc5=nn.Linear(5,1)\n",
    "         \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.norm1(self.fc1(x)))\n",
    "        x=F.relu(self.norm2(self.fc2(x)))\n",
    "        x=F.relu(self.norm3(self.fc3(x)))\n",
    "        x=F.relu(self.norm4(self.fc4(x)))\n",
    "        x=self.fc5(x)        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=23, out_features=32, bias=True)\n",
       "  (norm1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=32, out_features=16, bias=False)\n",
       "  (norm2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc3): Linear(in_features=16, out_features=8, bias=False)\n",
       "  (norm3): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc4): Linear(in_features=8, out_features=5, bias=False)\n",
       "  (norm4): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc5): Linear(in_features=5, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "criterion=nn.BCEWithLogitsLoss()\n",
    "Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.001,weight_decay=1e-4)\n",
    "scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode=\"min\",factor=0.75,patience=50,min_lr=1e-5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs,loaders,model,optimizer,criterion,scheduler):\n",
    "    valid_loss_min=np.Inf\n",
    "    list_train_loss=[]\n",
    "    list_valid_loss=[]\n",
    "    for epoch in range(1,n_epochs+1):\n",
    "        train_loss=0.0\n",
    "        valid_loss=0.0\n",
    "        model.train()\n",
    "        for batch_idx,(data,target) in enumerate(loaders[\"train\"]):\n",
    "            optimizer.zero_grad()\n",
    "            output=model(data.float())\n",
    "            loss=criterion(output.squeeze(),target.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss=train_loss+1/(batch_idx+1)*(loss.item()-train_loss)\n",
    "        list_train_loss.append(train_loss)\n",
    "        print(\"At {} epoch, Training Loss: {} \".format(epoch,train_loss))\n",
    "        model.eval()\n",
    "        for batch_idx,(data,target) in enumerate(loaders[\"valid\"]):\n",
    "            output=model(data.float())\n",
    "            loss=criterion(output.squeeze(),target.float())\n",
    "            valid_loss=valid_loss+1/(batch_idx+1)*(loss.item()-valid_loss)\n",
    "        scheduler.step(valid_loss)\n",
    "        list_valid_loss.append(valid_loss)\n",
    "        print(\"At {} epoch, Validation Loss: {} \".format(epoch,valid_loss))\n",
    "        #Save the model\n",
    "        if valid_loss < valid_loss_min:\n",
    "            torch.save(model.state_dict(),'ccFraud.pt')\n",
    "            print(\"Minimum validation loss detected, saving model......................................................................................\")\n",
    "            valid_loss_min=valid_loss\n",
    "    \n",
    "    return model, list_train_loss, list_valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 1 epoch, Training Loss: 0.6968989156186585 \n",
      "At 1 epoch, Validation Loss: 0.6942894250154495 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 2 epoch, Training Loss: 0.6894774150103331 \n",
      "At 2 epoch, Validation Loss: 0.6930162787437439 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 3 epoch, Training Loss: 0.6878647767007352 \n",
      "At 3 epoch, Validation Loss: 0.6900813877582548 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 4 epoch, Training Loss: 0.6863149490207433 \n",
      "At 4 epoch, Validation Loss: 0.696302291750908 \n",
      "At 5 epoch, Training Loss: 0.6849384076893328 \n",
      "At 5 epoch, Validation Loss: 0.6888509511947633 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 6 epoch, Training Loss: 0.6826772563159467 \n",
      "At 6 epoch, Validation Loss: 0.7014545261859895 \n",
      "At 7 epoch, Training Loss: 0.6815059985965488 \n",
      "At 7 epoch, Validation Loss: 0.6932431250810625 \n",
      "At 8 epoch, Training Loss: 0.6791995536535977 \n",
      "At 8 epoch, Validation Loss: 0.693494138121605 \n",
      "At 9 epoch, Training Loss: 0.6800436511635783 \n",
      "At 9 epoch, Validation Loss: 0.6940714657306671 \n",
      "At 10 epoch, Training Loss: 0.6778343282639976 \n",
      "At 10 epoch, Validation Loss: 0.6965016335248947 \n",
      "At 11 epoch, Training Loss: 0.6766481507569555 \n",
      "At 11 epoch, Validation Loss: 0.6998390257358553 \n",
      "At 12 epoch, Training Loss: 0.6780580025166273 \n",
      "At 12 epoch, Validation Loss: 0.6963786005973814 \n",
      "At 13 epoch, Training Loss: 0.6762044027447699 \n",
      "At 13 epoch, Validation Loss: 0.6949015557765961 \n",
      "At 14 epoch, Training Loss: 0.674876718223095 \n",
      "At 14 epoch, Validation Loss: 0.6965420752763749 \n",
      "At 15 epoch, Training Loss: 0.6692265685647725 \n",
      "At 15 epoch, Validation Loss: 0.7025300621986389 \n",
      "At 16 epoch, Training Loss: 0.6688500795513389 \n",
      "At 16 epoch, Validation Loss: 0.7076380133628847 \n",
      "At 17 epoch, Training Loss: 0.6716225516051049 \n",
      "At 17 epoch, Validation Loss: 0.704049152135849 \n",
      "At 18 epoch, Training Loss: 0.6675249438732866 \n",
      "At 18 epoch, Validation Loss: 0.7002069026231765 \n",
      "At 19 epoch, Training Loss: 0.6649758845567703 \n",
      "At 19 epoch, Validation Loss: 0.7096206456422804 \n",
      "At 20 epoch, Training Loss: 0.6676873736083506 \n",
      "At 20 epoch, Validation Loss: 0.7022432535886765 \n",
      "At 21 epoch, Training Loss: 0.6712147668004038 \n",
      "At 21 epoch, Validation Loss: 0.703567287325859 \n",
      "At 22 epoch, Training Loss: 0.6681530524045228 \n",
      "At 22 epoch, Validation Loss: 0.7037634760141372 \n",
      "At 23 epoch, Training Loss: 0.6628323007375002 \n",
      "At 23 epoch, Validation Loss: 0.712982937693596 \n",
      "At 24 epoch, Training Loss: 0.662947532907128 \n",
      "At 24 epoch, Validation Loss: 0.7132838547229767 \n",
      "At 25 epoch, Training Loss: 0.6627890333533286 \n",
      "At 25 epoch, Validation Loss: 0.7016774117946624 \n",
      "At 26 epoch, Training Loss: 0.6633874788880347 \n",
      "At 26 epoch, Validation Loss: 0.7066813379526138 \n",
      "At 27 epoch, Training Loss: 0.6622989442199471 \n",
      "At 27 epoch, Validation Loss: 0.7142415285110475 \n",
      "At 28 epoch, Training Loss: 0.6601218979805706 \n",
      "At 28 epoch, Validation Loss: 0.7086982727050781 \n",
      "At 29 epoch, Training Loss: 0.6589161656796931 \n",
      "At 29 epoch, Validation Loss: 0.7037639349699021 \n",
      "At 30 epoch, Training Loss: 0.6610343638807533 \n",
      "At 30 epoch, Validation Loss: 0.7000360548496246 \n",
      "At 31 epoch, Training Loss: 0.6591851055622099 \n",
      "At 31 epoch, Validation Loss: 0.6968343943357468 \n",
      "At 32 epoch, Training Loss: 0.6615047104656694 \n",
      "At 32 epoch, Validation Loss: 0.7130916446447373 \n",
      "At 33 epoch, Training Loss: 0.6568163212388753 \n",
      "At 33 epoch, Validation Loss: 0.7156199097633363 \n",
      "At 34 epoch, Training Loss: 0.6609649501740932 \n",
      "At 34 epoch, Validation Loss: 0.6973158568143845 \n",
      "At 35 epoch, Training Loss: 0.6562984172254804 \n",
      "At 35 epoch, Validation Loss: 0.7111076056957245 \n",
      "At 36 epoch, Training Loss: 0.6541729483753442 \n",
      "At 36 epoch, Validation Loss: 0.6998730212450028 \n",
      "At 37 epoch, Training Loss: 0.6569348633289341 \n",
      "At 37 epoch, Validation Loss: 0.7174323469400404 \n",
      "At 38 epoch, Training Loss: 0.6565918549895289 \n",
      "At 38 epoch, Validation Loss: 0.7002400964498521 \n",
      "At 39 epoch, Training Loss: 0.6524241734296081 \n",
      "At 39 epoch, Validation Loss: 0.7018083065748215 \n",
      "At 40 epoch, Training Loss: 0.6486147467046977 \n",
      "At 40 epoch, Validation Loss: 0.6968600302934647 \n",
      "At 41 epoch, Training Loss: 0.6566247139126064 \n",
      "At 41 epoch, Validation Loss: 0.7061545372009278 \n",
      "At 42 epoch, Training Loss: 0.6490577653050422 \n",
      "At 42 epoch, Validation Loss: 0.7102171719074248 \n",
      "At 43 epoch, Training Loss: 0.6498584263026712 \n",
      "At 43 epoch, Validation Loss: 0.6999750912189483 \n",
      "At 44 epoch, Training Loss: 0.6512010540813209 \n",
      "At 44 epoch, Validation Loss: 0.7047449409961701 \n",
      "At 45 epoch, Training Loss: 0.6525599196553231 \n",
      "At 45 epoch, Validation Loss: 0.7120434373617173 \n",
      "At 46 epoch, Training Loss: 0.6513397727161647 \n",
      "At 46 epoch, Validation Loss: 0.7183666110038758 \n",
      "At 47 epoch, Training Loss: 0.6503678377717734 \n",
      "At 47 epoch, Validation Loss: 0.6962184727191926 \n",
      "At 48 epoch, Training Loss: 0.6462839946150781 \n",
      "At 48 epoch, Validation Loss: 0.7018635153770447 \n",
      "At 49 epoch, Training Loss: 0.6488160524517299 \n",
      "At 49 epoch, Validation Loss: 0.7147804766893386 \n",
      "At 50 epoch, Training Loss: 0.6460760679095982 \n",
      "At 50 epoch, Validation Loss: 0.7226975500583649 \n",
      "At 51 epoch, Training Loss: 0.6543467748910186 \n",
      "At 51 epoch, Validation Loss: 0.7067806124687195 \n",
      "At 52 epoch, Training Loss: 0.6526711095124482 \n",
      "At 52 epoch, Validation Loss: 0.705244618654251 \n",
      "At 53 epoch, Training Loss: 0.64761206805706 \n",
      "At 53 epoch, Validation Loss: 0.7053837239742279 \n",
      "At 54 epoch, Training Loss: 0.6500249620527029 \n",
      "At 54 epoch, Validation Loss: 0.7043672531843184 \n",
      "At 55 epoch, Training Loss: 0.6459498882293706 \n",
      "At 55 epoch, Validation Loss: 0.7173289030790329 \n",
      "At 56 epoch, Training Loss: 0.6475389108061792 \n",
      "At 56 epoch, Validation Loss: 0.7101260483264922 \n",
      "At 57 epoch, Training Loss: 0.6464877080172297 \n",
      "At 57 epoch, Validation Loss: 0.7100442409515381 \n",
      "At 58 epoch, Training Loss: 0.6452190194278954 \n",
      "At 58 epoch, Validation Loss: 0.7068227648735047 \n",
      "At 59 epoch, Training Loss: 0.645738808065653 \n",
      "At 59 epoch, Validation Loss: 0.7010357916355133 \n",
      "At 60 epoch, Training Loss: 0.6422226365655659 \n",
      "At 60 epoch, Validation Loss: 0.7112108916044234 \n",
      "At 61 epoch, Training Loss: 0.6424400437623262 \n",
      "At 61 epoch, Validation Loss: 0.7121472120285035 \n",
      "At 62 epoch, Training Loss: 0.6434045042842624 \n",
      "At 62 epoch, Validation Loss: 0.7027848184108734 \n",
      "At 63 epoch, Training Loss: 0.6441151883453129 \n",
      "At 63 epoch, Validation Loss: 0.7065485239028931 \n",
      "At 64 epoch, Training Loss: 0.6387819726020095 \n",
      "At 64 epoch, Validation Loss: 0.7027942270040513 \n",
      "At 65 epoch, Training Loss: 0.6407031279057264 \n",
      "At 65 epoch, Validation Loss: 0.7180042922496794 \n",
      "At 66 epoch, Training Loss: 0.6417471759021287 \n",
      "At 66 epoch, Validation Loss: 0.7077552139759063 \n",
      "At 67 epoch, Training Loss: 0.6428378589451312 \n",
      "At 67 epoch, Validation Loss: 0.7071115016937256 \n",
      "At 68 epoch, Training Loss: 0.6379061236977579 \n",
      "At 68 epoch, Validation Loss: 0.7126528143882751 \n",
      "At 69 epoch, Training Loss: 0.6416435398161406 \n",
      "At 69 epoch, Validation Loss: 0.7135812699794769 \n",
      "At 70 epoch, Training Loss: 0.6388026975095272 \n",
      "At 70 epoch, Validation Loss: 0.7082029670476913 \n",
      "At 71 epoch, Training Loss: 0.6407739073038105 \n",
      "At 71 epoch, Validation Loss: 0.7080614984035491 \n",
      "At 72 epoch, Training Loss: 0.6414825640618801 \n",
      "At 72 epoch, Validation Loss: 0.7090568125247957 \n",
      "At 73 epoch, Training Loss: 0.6392394024878737 \n",
      "At 73 epoch, Validation Loss: 0.7096741825342178 \n",
      "At 74 epoch, Training Loss: 0.6352496091276408 \n",
      "At 74 epoch, Validation Loss: 0.7173964917659761 \n",
      "At 75 epoch, Training Loss: 0.6414404645562171 \n",
      "At 75 epoch, Validation Loss: 0.7005862832069395 \n",
      "At 76 epoch, Training Loss: 0.6357645757496359 \n",
      "At 76 epoch, Validation Loss: 0.7084189981222153 \n",
      "At 77 epoch, Training Loss: 0.6428281251341104 \n",
      "At 77 epoch, Validation Loss: 0.7143803417682647 \n",
      "At 78 epoch, Training Loss: 0.6342100497335196 \n",
      "At 78 epoch, Validation Loss: 0.7111044138669966 \n",
      "At 79 epoch, Training Loss: 0.6380889069288973 \n",
      "At 79 epoch, Validation Loss: 0.7149058043956757 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 80 epoch, Training Loss: 0.6413235347717999 \n",
      "At 80 epoch, Validation Loss: 0.7098043859004975 \n",
      "At 81 epoch, Training Loss: 0.6355007484555244 \n",
      "At 81 epoch, Validation Loss: 0.7152149766683579 \n",
      "At 82 epoch, Training Loss: 0.6419343229383226 \n",
      "At 82 epoch, Validation Loss: 0.7122632145881653 \n",
      "At 83 epoch, Training Loss: 0.6363024536520244 \n",
      "At 83 epoch, Validation Loss: 0.728238120675087 \n",
      "At 84 epoch, Training Loss: 0.6368251219391826 \n",
      "At 84 epoch, Validation Loss: 0.7162903606891633 \n",
      "At 85 epoch, Training Loss: 0.6365410361438986 \n",
      "At 85 epoch, Validation Loss: 0.717614522576332 \n",
      "At 86 epoch, Training Loss: 0.6369929704815147 \n",
      "At 86 epoch, Validation Loss: 0.7070386439561842 \n",
      "At 87 epoch, Training Loss: 0.6322432294487953 \n",
      "At 87 epoch, Validation Loss: 0.7101308763027193 \n",
      "At 88 epoch, Training Loss: 0.6354923136532308 \n",
      "At 88 epoch, Validation Loss: 0.7089558959007263 \n",
      "At 89 epoch, Training Loss: 0.6364710450172423 \n",
      "At 89 epoch, Validation Loss: 0.7287092268466948 \n",
      "At 90 epoch, Training Loss: 0.6350863777101042 \n",
      "At 90 epoch, Validation Loss: 0.7124463379383087 \n",
      "At 91 epoch, Training Loss: 0.6343341600149867 \n",
      "At 91 epoch, Validation Loss: 0.7079633384943006 \n",
      "At 92 epoch, Training Loss: 0.6348399486392736 \n",
      "At 92 epoch, Validation Loss: 0.7135328501462938 \n",
      "At 93 epoch, Training Loss: 0.6368145026266574 \n",
      "At 93 epoch, Validation Loss: 0.7128417938947675 \n",
      "At 94 epoch, Training Loss: 0.6339316077530384 \n",
      "At 94 epoch, Validation Loss: 0.7157252490520478 \n",
      "At 95 epoch, Training Loss: 0.6388952996581791 \n",
      "At 95 epoch, Validation Loss: 0.7065332740545273 \n",
      "At 96 epoch, Training Loss: 0.6297322399914264 \n",
      "At 96 epoch, Validation Loss: 0.7094998598098755 \n",
      "At 97 epoch, Training Loss: 0.6340205408632756 \n",
      "At 97 epoch, Validation Loss: 0.7137458175420761 \n",
      "At 98 epoch, Training Loss: 0.6380122285336258 \n",
      "At 98 epoch, Validation Loss: 0.70990808904171 \n",
      "At 99 epoch, Training Loss: 0.6288836412131786 \n",
      "At 99 epoch, Validation Loss: 0.7070164352655411 \n",
      "At 100 epoch, Training Loss: 0.6349381811916829 \n",
      "At 100 epoch, Validation Loss: 0.7048097103834152 \n",
      "At 101 epoch, Training Loss: 0.6293334376066922 \n",
      "At 101 epoch, Validation Loss: 0.7086228281259538 \n",
      "At 102 epoch, Training Loss: 0.6336273994296793 \n",
      "At 102 epoch, Validation Loss: 0.7255535066127777 \n",
      "At 103 epoch, Training Loss: 0.6361400470137594 \n",
      "At 103 epoch, Validation Loss: 0.7115499019622803 \n",
      "At 104 epoch, Training Loss: 0.6331449951976535 \n",
      "At 104 epoch, Validation Loss: 0.7128434032201767 \n",
      "At 105 epoch, Training Loss: 0.6336260184645647 \n",
      "At 105 epoch, Validation Loss: 0.7132646590471268 \n",
      "At 106 epoch, Training Loss: 0.6321827661246063 \n",
      "At 106 epoch, Validation Loss: 0.7156433314085006 \n",
      "At 107 epoch, Training Loss: 0.6359773401170972 \n",
      "At 107 epoch, Validation Loss: 0.7092194765806199 \n",
      "At 108 epoch, Training Loss: 0.6329324163496491 \n",
      "At 108 epoch, Validation Loss: 0.7106800556182862 \n",
      "At 109 epoch, Training Loss: 0.6310004442930225 \n",
      "At 109 epoch, Validation Loss: 0.7139336854219437 \n",
      "At 110 epoch, Training Loss: 0.6294885084033012 \n",
      "At 110 epoch, Validation Loss: 0.7197171449661256 \n",
      "At 111 epoch, Training Loss: 0.6261179819703104 \n",
      "At 111 epoch, Validation Loss: 0.709027111530304 \n",
      "At 112 epoch, Training Loss: 0.6303925920277833 \n",
      "At 112 epoch, Validation Loss: 0.7193033188581468 \n",
      "At 113 epoch, Training Loss: 0.6280566532164812 \n",
      "At 113 epoch, Validation Loss: 0.706587588787079 \n",
      "At 114 epoch, Training Loss: 0.6299447536468508 \n",
      "At 114 epoch, Validation Loss: 0.712891137599945 \n",
      "At 115 epoch, Training Loss: 0.6300340153276919 \n",
      "At 115 epoch, Validation Loss: 0.7141476601362228 \n",
      "At 116 epoch, Training Loss: 0.6246766041964295 \n",
      "At 116 epoch, Validation Loss: 0.7168660581111906 \n",
      "At 117 epoch, Training Loss: 0.6315920136868954 \n",
      "At 117 epoch, Validation Loss: 0.7150158911943435 \n",
      "At 118 epoch, Training Loss: 0.6268148213624952 \n",
      "At 118 epoch, Validation Loss: 0.7218356877565384 \n",
      "At 119 epoch, Training Loss: 0.6284070622175933 \n",
      "At 119 epoch, Validation Loss: 0.7133030295372009 \n",
      "At 120 epoch, Training Loss: 0.6264095053076746 \n",
      "At 120 epoch, Validation Loss: 0.7233742624521255 \n",
      "At 121 epoch, Training Loss: 0.6302322253584859 \n",
      "At 121 epoch, Validation Loss: 0.716686898469925 \n",
      "At 122 epoch, Training Loss: 0.6280664205551147 \n",
      "At 122 epoch, Validation Loss: 0.7189382255077361 \n",
      "At 123 epoch, Training Loss: 0.6287440471351149 \n",
      "At 123 epoch, Validation Loss: 0.7104446172714234 \n",
      "At 124 epoch, Training Loss: 0.6270909715443849 \n",
      "At 124 epoch, Validation Loss: 0.7161089211702347 \n",
      "At 125 epoch, Training Loss: 0.6292697973549364 \n",
      "At 125 epoch, Validation Loss: 0.7133488774299622 \n",
      "At 126 epoch, Training Loss: 0.6230299655348062 \n",
      "At 126 epoch, Validation Loss: 0.7146858364343641 \n",
      "At 127 epoch, Training Loss: 0.6323772616684444 \n",
      "At 127 epoch, Validation Loss: 0.7181696414947509 \n",
      "At 128 epoch, Training Loss: 0.6314376749098303 \n",
      "At 128 epoch, Validation Loss: 0.7161977291107178 \n",
      "At 129 epoch, Training Loss: 0.6297386799007653 \n",
      "At 129 epoch, Validation Loss: 0.7080593377351762 \n",
      "At 130 epoch, Training Loss: 0.6305074464529755 \n",
      "At 130 epoch, Validation Loss: 0.7215424746274949 \n",
      "At 131 epoch, Training Loss: 0.6253927297890187 \n",
      "At 131 epoch, Validation Loss: 0.7237931460142136 \n",
      "At 132 epoch, Training Loss: 0.6293504524976016 \n",
      "At 132 epoch, Validation Loss: 0.712961083650589 \n",
      "At 133 epoch, Training Loss: 0.6277851540595293 \n",
      "At 133 epoch, Validation Loss: 0.7097748547792435 \n",
      "At 134 epoch, Training Loss: 0.6278419747948649 \n",
      "At 134 epoch, Validation Loss: 0.7163971185684205 \n",
      "At 135 epoch, Training Loss: 0.6226232726126909 \n",
      "At 135 epoch, Validation Loss: 0.7163229107856749 \n",
      "At 136 epoch, Training Loss: 0.628177665174007 \n",
      "At 136 epoch, Validation Loss: 0.7189243465662003 \n",
      "At 137 epoch, Training Loss: 0.6280434068292381 \n",
      "At 137 epoch, Validation Loss: 0.707378762960434 \n",
      "At 138 epoch, Training Loss: 0.6281092647463083 \n",
      "At 138 epoch, Validation Loss: 0.7171184092760087 \n",
      "At 139 epoch, Training Loss: 0.6291434708982705 \n",
      "At 139 epoch, Validation Loss: 0.7101967334747317 \n",
      "At 140 epoch, Training Loss: 0.6294832408428193 \n",
      "At 140 epoch, Validation Loss: 0.7059915244579315 \n",
      "At 141 epoch, Training Loss: 0.6296386655420068 \n",
      "At 141 epoch, Validation Loss: 0.7148325592279433 \n",
      "At 142 epoch, Training Loss: 0.6321874599903825 \n",
      "At 142 epoch, Validation Loss: 0.712151038646698 \n",
      "At 143 epoch, Training Loss: 0.6202631793916228 \n",
      "At 143 epoch, Validation Loss: 0.7199989825487136 \n",
      "At 144 epoch, Training Loss: 0.6281972657889129 \n",
      "At 144 epoch, Validation Loss: 0.7222199559211732 \n",
      "At 145 epoch, Training Loss: 0.6224303733557458 \n",
      "At 145 epoch, Validation Loss: 0.7172143191099167 \n",
      "At 146 epoch, Training Loss: 0.6286110036075111 \n",
      "At 146 epoch, Validation Loss: 0.7067320436239243 \n",
      "At 147 epoch, Training Loss: 0.6260013863444325 \n",
      "At 147 epoch, Validation Loss: 0.7107092320919037 \n",
      "At 148 epoch, Training Loss: 0.6206671092659237 \n",
      "At 148 epoch, Validation Loss: 0.7156761974096298 \n",
      "At 149 epoch, Training Loss: 0.6217107985168696 \n",
      "At 149 epoch, Validation Loss: 0.7117921113967896 \n",
      "At 150 epoch, Training Loss: 0.6280541617423293 \n",
      "At 150 epoch, Validation Loss: 0.7201288521289826 \n",
      "At 151 epoch, Training Loss: 0.628265246376395 \n",
      "At 151 epoch, Validation Loss: 0.7135064601898193 \n",
      "At 152 epoch, Training Loss: 0.6307300549000497 \n",
      "At 152 epoch, Validation Loss: 0.7181352972984315 \n",
      "At 153 epoch, Training Loss: 0.626013344526291 \n",
      "At 153 epoch, Validation Loss: 0.7139363139867784 \n",
      "At 154 epoch, Training Loss: 0.6250920876860616 \n",
      "At 154 epoch, Validation Loss: 0.7171458750963212 \n",
      "At 155 epoch, Training Loss: 0.6255837734788656 \n",
      "At 155 epoch, Validation Loss: 0.7121674090623856 \n",
      "At 156 epoch, Training Loss: 0.6282548692077399 \n",
      "At 156 epoch, Validation Loss: 0.7170980304479597 \n",
      "At 157 epoch, Training Loss: 0.6248297326266762 \n",
      "At 157 epoch, Validation Loss: 0.7204972445964812 \n",
      "At 158 epoch, Training Loss: 0.6266731966286896 \n",
      "At 158 epoch, Validation Loss: 0.7166337460279465 \n",
      "At 159 epoch, Training Loss: 0.6249942984431981 \n",
      "At 159 epoch, Validation Loss: 0.7194574862718581 \n",
      "At 160 epoch, Training Loss: 0.6208995293825863 \n",
      "At 160 epoch, Validation Loss: 0.7204062879085541 \n",
      "At 161 epoch, Training Loss: 0.6262115277349949 \n",
      "At 161 epoch, Validation Loss: 0.7167392313480376 \n",
      "At 162 epoch, Training Loss: 0.6283972300589084 \n",
      "At 162 epoch, Validation Loss: 0.7176264435052873 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 163 epoch, Training Loss: 0.6248031266033651 \n",
      "At 163 epoch, Validation Loss: 0.7158639371395111 \n",
      "At 164 epoch, Training Loss: 0.6236687306314705 \n",
      "At 164 epoch, Validation Loss: 0.7138753294944763 \n",
      "At 165 epoch, Training Loss: 0.6212462738156316 \n",
      "At 165 epoch, Validation Loss: 0.7244864881038666 \n",
      "At 166 epoch, Training Loss: 0.6210833173245198 \n",
      "At 166 epoch, Validation Loss: 0.7180245786905289 \n",
      "At 167 epoch, Training Loss: 0.6214651081711052 \n",
      "At 167 epoch, Validation Loss: 0.7180314987897873 \n",
      "At 168 epoch, Training Loss: 0.6223237749189139 \n",
      "At 168 epoch, Validation Loss: 0.716399970650673 \n",
      "At 169 epoch, Training Loss: 0.6219939630478618 \n",
      "At 169 epoch, Validation Loss: 0.7144391894340516 \n",
      "At 170 epoch, Training Loss: 0.626780143380165 \n",
      "At 170 epoch, Validation Loss: 0.7156844884157181 \n",
      "At 171 epoch, Training Loss: 0.6231618825346231 \n",
      "At 171 epoch, Validation Loss: 0.7083192855119705 \n",
      "At 172 epoch, Training Loss: 0.6261656675487756 \n",
      "At 172 epoch, Validation Loss: 0.7189403206110001 \n",
      "At 173 epoch, Training Loss: 0.6218932155519725 \n",
      "At 173 epoch, Validation Loss: 0.7117085427045822 \n",
      "At 174 epoch, Training Loss: 0.6260581165552137 \n",
      "At 174 epoch, Validation Loss: 0.7120466202497482 \n",
      "At 175 epoch, Training Loss: 0.6234285749495028 \n",
      "At 175 epoch, Validation Loss: 0.710015669465065 \n",
      "At 176 epoch, Training Loss: 0.6234245963394641 \n",
      "At 176 epoch, Validation Loss: 0.7150678962469101 \n",
      "At 177 epoch, Training Loss: 0.6207844443619254 \n",
      "At 177 epoch, Validation Loss: 0.713607183098793 \n",
      "At 178 epoch, Training Loss: 0.6240702204406261 \n",
      "At 178 epoch, Validation Loss: 0.7231026321649552 \n",
      "At 179 epoch, Training Loss: 0.6248486530035736 \n",
      "At 179 epoch, Validation Loss: 0.7220478862524032 \n",
      "At 180 epoch, Training Loss: 0.621647826395929 \n",
      "At 180 epoch, Validation Loss: 0.7176860779523849 \n",
      "At 181 epoch, Training Loss: 0.6259435892105105 \n",
      "At 181 epoch, Validation Loss: 0.7156931221485137 \n",
      "At 182 epoch, Training Loss: 0.6234396312385796 \n",
      "At 182 epoch, Validation Loss: 0.7105137854814528 \n",
      "At 183 epoch, Training Loss: 0.6263292506337166 \n",
      "At 183 epoch, Validation Loss: 0.7123802453279496 \n",
      "At 184 epoch, Training Loss: 0.6254738546907902 \n",
      "At 184 epoch, Validation Loss: 0.70683733522892 \n",
      "At 185 epoch, Training Loss: 0.6280735090374947 \n",
      "At 185 epoch, Validation Loss: 0.7245849430561064 \n",
      "At 186 epoch, Training Loss: 0.6241052877157924 \n",
      "At 186 epoch, Validation Loss: 0.7222938150167467 \n",
      "At 187 epoch, Training Loss: 0.6278539940714835 \n",
      "At 187 epoch, Validation Loss: 0.7121754884719849 \n",
      "At 188 epoch, Training Loss: 0.6233302347362041 \n",
      "At 188 epoch, Validation Loss: 0.7112822294235229 \n",
      "At 189 epoch, Training Loss: 0.6239520687609907 \n",
      "At 189 epoch, Validation Loss: 0.7198665499687195 \n",
      "At 190 epoch, Training Loss: 0.6249284040182832 \n",
      "At 190 epoch, Validation Loss: 0.7181198626756669 \n",
      "At 191 epoch, Training Loss: 0.6269997958093882 \n",
      "At 191 epoch, Validation Loss: 0.7153690129518508 \n",
      "At 192 epoch, Training Loss: 0.6260706346482037 \n",
      "At 192 epoch, Validation Loss: 0.7125796228647231 \n",
      "At 193 epoch, Training Loss: 0.6237123239785436 \n",
      "At 193 epoch, Validation Loss: 0.7096243798732758 \n",
      "At 194 epoch, Training Loss: 0.6176750738173725 \n",
      "At 194 epoch, Validation Loss: 0.7225379467010498 \n",
      "At 195 epoch, Training Loss: 0.6243520773947241 \n",
      "At 195 epoch, Validation Loss: 0.7153658866882324 \n",
      "At 196 epoch, Training Loss: 0.626615020632744 \n",
      "At 196 epoch, Validation Loss: 0.7205139845609665 \n",
      "At 197 epoch, Training Loss: 0.6250335957854987 \n",
      "At 197 epoch, Validation Loss: 0.7252081453800203 \n",
      "At 198 epoch, Training Loss: 0.6272481851279731 \n",
      "At 198 epoch, Validation Loss: 0.7205437660217286 \n",
      "At 199 epoch, Training Loss: 0.6245588432997466 \n",
      "At 199 epoch, Validation Loss: 0.7068258613348006 \n",
      "At 200 epoch, Training Loss: 0.6259997636079787 \n",
      "At 200 epoch, Validation Loss: 0.7144858598709106 \n",
      "At 201 epoch, Training Loss: 0.623872309550643 \n",
      "At 201 epoch, Validation Loss: 0.7109452754259109 \n",
      "At 202 epoch, Training Loss: 0.618490944430232 \n",
      "At 202 epoch, Validation Loss: 0.7159843385219574 \n",
      "At 203 epoch, Training Loss: 0.6190770994871853 \n",
      "At 203 epoch, Validation Loss: 0.7229137480258941 \n",
      "At 204 epoch, Training Loss: 0.630029372870922 \n",
      "At 204 epoch, Validation Loss: 0.7147551178932191 \n",
      "At 205 epoch, Training Loss: 0.6229476656764742 \n",
      "At 205 epoch, Validation Loss: 0.7101339399814606 \n",
      "At 206 epoch, Training Loss: 0.6214112330228088 \n",
      "At 206 epoch, Validation Loss: 0.7180249691009521 \n",
      "At 207 epoch, Training Loss: 0.6257075581699607 \n",
      "At 207 epoch, Validation Loss: 0.7190686225891114 \n",
      "At 208 epoch, Training Loss: 0.6182773433625698 \n",
      "At 208 epoch, Validation Loss: 0.7115467220544813 \n",
      "At 209 epoch, Training Loss: 0.6234850209206341 \n",
      "At 209 epoch, Validation Loss: 0.7169723957777022 \n",
      "At 210 epoch, Training Loss: 0.6191879097372294 \n",
      "At 210 epoch, Validation Loss: 0.7150717884302139 \n",
      "At 211 epoch, Training Loss: 0.6199242468923329 \n",
      "At 211 epoch, Validation Loss: 0.7145968079566956 \n",
      "At 212 epoch, Training Loss: 0.619876157864928 \n",
      "At 212 epoch, Validation Loss: 0.7125179708003997 \n",
      "At 213 epoch, Training Loss: 0.6209332637488845 \n",
      "At 213 epoch, Validation Loss: 0.7107584625482559 \n",
      "At 214 epoch, Training Loss: 0.619132822006941 \n",
      "At 214 epoch, Validation Loss: 0.7143708974123001 \n",
      "At 215 epoch, Training Loss: 0.6210294730961323 \n",
      "At 215 epoch, Validation Loss: 0.7075229495763778 \n",
      "At 216 epoch, Training Loss: 0.6180020593106752 \n",
      "At 216 epoch, Validation Loss: 0.7099692314863205 \n",
      "At 217 epoch, Training Loss: 0.621536100283265 \n",
      "At 217 epoch, Validation Loss: 0.7121086478233337 \n",
      "At 218 epoch, Training Loss: 0.6201849386096 \n",
      "At 218 epoch, Validation Loss: 0.7137835800647736 \n",
      "At 219 epoch, Training Loss: 0.6209209099411963 \n",
      "At 219 epoch, Validation Loss: 0.7142425149679182 \n",
      "At 220 epoch, Training Loss: 0.6153290484100583 \n",
      "At 220 epoch, Validation Loss: 0.7141777634620667 \n",
      "At 221 epoch, Training Loss: 0.6192640453577043 \n",
      "At 221 epoch, Validation Loss: 0.7108098089694976 \n",
      "At 222 epoch, Training Loss: 0.6239181362092491 \n",
      "At 222 epoch, Validation Loss: 0.723822209239006 \n",
      "At 223 epoch, Training Loss: 0.621955402195454 \n",
      "At 223 epoch, Validation Loss: 0.7166508793830871 \n",
      "At 224 epoch, Training Loss: 0.6264632094651461 \n",
      "At 224 epoch, Validation Loss: 0.7146614253520964 \n",
      "At 225 epoch, Training Loss: 0.6182291992008686 \n",
      "At 225 epoch, Validation Loss: 0.7177328526973724 \n",
      "At 226 epoch, Training Loss: 0.6184398218989374 \n",
      "At 226 epoch, Validation Loss: 0.7126927942037582 \n",
      "At 227 epoch, Training Loss: 0.6198893751949073 \n",
      "At 227 epoch, Validation Loss: 0.7136170744895936 \n",
      "At 228 epoch, Training Loss: 0.6191097695380451 \n",
      "At 228 epoch, Validation Loss: 0.7158767700195313 \n",
      "At 229 epoch, Training Loss: 0.6168389912694694 \n",
      "At 229 epoch, Validation Loss: 0.718713614344597 \n",
      "At 230 epoch, Training Loss: 0.6215414766222243 \n",
      "At 230 epoch, Validation Loss: 0.7184795260429382 \n",
      "At 231 epoch, Training Loss: 0.6246609468013049 \n",
      "At 231 epoch, Validation Loss: 0.7134252101182939 \n",
      "At 232 epoch, Training Loss: 0.6218910418450828 \n",
      "At 232 epoch, Validation Loss: 0.7104206889867783 \n",
      "At 233 epoch, Training Loss: 0.6168877806514503 \n",
      "At 233 epoch, Validation Loss: 0.7167789846658708 \n",
      "At 234 epoch, Training Loss: 0.6181998316198589 \n",
      "At 234 epoch, Validation Loss: 0.7161408364772797 \n",
      "At 235 epoch, Training Loss: 0.6215011987835163 \n",
      "At 235 epoch, Validation Loss: 0.7170660108327866 \n",
      "At 236 epoch, Training Loss: 0.6188786804676057 \n",
      "At 236 epoch, Validation Loss: 0.7168703824281694 \n",
      "At 237 epoch, Training Loss: 0.6181147411465643 \n",
      "At 237 epoch, Validation Loss: 0.7143952429294586 \n",
      "At 238 epoch, Training Loss: 0.6199036091566087 \n",
      "At 238 epoch, Validation Loss: 0.7102303951978683 \n",
      "At 239 epoch, Training Loss: 0.6169811304658647 \n",
      "At 239 epoch, Validation Loss: 0.7170657813549042 \n",
      "At 240 epoch, Training Loss: 0.6203165814280511 \n",
      "At 240 epoch, Validation Loss: 0.7121945232152941 \n",
      "At 241 epoch, Training Loss: 0.6229341700673099 \n",
      "At 241 epoch, Validation Loss: 0.7162102192640305 \n",
      "At 242 epoch, Training Loss: 0.6183151151984934 \n",
      "At 242 epoch, Validation Loss: 0.7218536764383315 \n",
      "At 243 epoch, Training Loss: 0.619947997108102 \n",
      "At 243 epoch, Validation Loss: 0.7197599053382873 \n",
      "At 244 epoch, Training Loss: 0.6156646143645047 \n",
      "At 244 epoch, Validation Loss: 0.7178945571184158 \n",
      "At 245 epoch, Training Loss: 0.6206023141741754 \n",
      "At 245 epoch, Validation Loss: 0.7215130120515825 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 246 epoch, Training Loss: 0.6217742867767816 \n",
      "At 246 epoch, Validation Loss: 0.7249202698469162 \n",
      "At 247 epoch, Training Loss: 0.6149854023009538 \n",
      "At 247 epoch, Validation Loss: 0.7184948444366454 \n",
      "At 248 epoch, Training Loss: 0.6190918926149604 \n",
      "At 248 epoch, Validation Loss: 0.7200741916894913 \n",
      "At 249 epoch, Training Loss: 0.6156889971345664 \n",
      "At 249 epoch, Validation Loss: 0.7205547600984573 \n",
      "At 250 epoch, Training Loss: 0.6196083955466748 \n",
      "At 250 epoch, Validation Loss: 0.7153752386569978 \n",
      "At 251 epoch, Training Loss: 0.6189508792012931 \n",
      "At 251 epoch, Validation Loss: 0.717237651348114 \n",
      "At 252 epoch, Training Loss: 0.6243042573332783 \n",
      "At 252 epoch, Validation Loss: 0.7240010172128677 \n",
      "At 253 epoch, Training Loss: 0.6182387322187425 \n",
      "At 253 epoch, Validation Loss: 0.716113469004631 \n",
      "At 254 epoch, Training Loss: 0.6214586421847342 \n",
      "At 254 epoch, Validation Loss: 0.7205776482820512 \n",
      "At 255 epoch, Training Loss: 0.6223983239382505 \n",
      "At 255 epoch, Validation Loss: 0.7212528973817827 \n",
      "At 256 epoch, Training Loss: 0.6126810166984799 \n",
      "At 256 epoch, Validation Loss: 0.7151704013347625 \n",
      "At 257 epoch, Training Loss: 0.6177439931780099 \n",
      "At 257 epoch, Validation Loss: 0.7154992312192915 \n",
      "At 258 epoch, Training Loss: 0.6162989672273392 \n",
      "At 258 epoch, Validation Loss: 0.719036015868187 \n",
      "At 259 epoch, Training Loss: 0.620150831341743 \n",
      "At 259 epoch, Validation Loss: 0.7179547369480134 \n",
      "At 260 epoch, Training Loss: 0.6158842749893665 \n",
      "At 260 epoch, Validation Loss: 0.7193435579538344 \n",
      "At 261 epoch, Training Loss: 0.6175640922039747 \n",
      "At 261 epoch, Validation Loss: 0.7167375177145004 \n",
      "At 262 epoch, Training Loss: 0.6212843604385854 \n",
      "At 262 epoch, Validation Loss: 0.7216726243495942 \n",
      "At 263 epoch, Training Loss: 0.6143424436450005 \n",
      "At 263 epoch, Validation Loss: 0.7168837755918502 \n",
      "At 264 epoch, Training Loss: 0.6176742490381002 \n",
      "At 264 epoch, Validation Loss: 0.720216178894043 \n",
      "At 265 epoch, Training Loss: 0.6205015771090987 \n",
      "At 265 epoch, Validation Loss: 0.7191493630409241 \n",
      "At 266 epoch, Training Loss: 0.6179629143327473 \n",
      "At 266 epoch, Validation Loss: 0.7202828347682954 \n",
      "At 267 epoch, Training Loss: 0.6212898246943953 \n",
      "At 267 epoch, Validation Loss: 0.7190131038427353 \n",
      "At 268 epoch, Training Loss: 0.6116005728021267 \n",
      "At 268 epoch, Validation Loss: 0.7195644646883012 \n",
      "At 269 epoch, Training Loss: 0.615285477042198 \n",
      "At 269 epoch, Validation Loss: 0.7165380656719208 \n",
      "At 270 epoch, Training Loss: 0.6209011897444725 \n",
      "At 270 epoch, Validation Loss: 0.7175490617752075 \n",
      "At 271 epoch, Training Loss: 0.6147572580724953 \n",
      "At 271 epoch, Validation Loss: 0.723204055428505 \n",
      "At 272 epoch, Training Loss: 0.6192243129014967 \n",
      "At 272 epoch, Validation Loss: 0.7122512340545655 \n",
      "At 273 epoch, Training Loss: 0.6154253777116535 \n",
      "At 273 epoch, Validation Loss: 0.715481123328209 \n",
      "At 274 epoch, Training Loss: 0.6194991726428272 \n",
      "At 274 epoch, Validation Loss: 0.7178291499614716 \n",
      "At 275 epoch, Training Loss: 0.6109973490238191 \n",
      "At 275 epoch, Validation Loss: 0.7173996835947036 \n",
      "At 276 epoch, Training Loss: 0.6141075991094115 \n",
      "At 276 epoch, Validation Loss: 0.715718448162079 \n",
      "At 277 epoch, Training Loss: 0.6196053560823206 \n",
      "At 277 epoch, Validation Loss: 0.7177643358707426 \n",
      "At 278 epoch, Training Loss: 0.6181430876255039 \n",
      "At 278 epoch, Validation Loss: 0.7169597685337067 \n",
      "At 279 epoch, Training Loss: 0.6214945960789919 \n",
      "At 279 epoch, Validation Loss: 0.7182407528162001 \n",
      "At 280 epoch, Training Loss: 0.616233756020665 \n",
      "At 280 epoch, Validation Loss: 0.7165446490049363 \n",
      "At 281 epoch, Training Loss: 0.6155939333140853 \n",
      "At 281 epoch, Validation Loss: 0.7167271256446837 \n",
      "At 282 epoch, Training Loss: 0.6198898993432523 \n",
      "At 282 epoch, Validation Loss: 0.7154470384120941 \n",
      "At 283 epoch, Training Loss: 0.6158012248575686 \n",
      "At 283 epoch, Validation Loss: 0.7224529623985292 \n",
      "At 284 epoch, Training Loss: 0.6199547037482258 \n",
      "At 284 epoch, Validation Loss: 0.7222370684146882 \n",
      "At 285 epoch, Training Loss: 0.6147727787494663 \n",
      "At 285 epoch, Validation Loss: 0.7166221201419831 \n",
      "At 286 epoch, Training Loss: 0.6174080049619083 \n",
      "At 286 epoch, Validation Loss: 0.7219539254903794 \n",
      "At 287 epoch, Training Loss: 0.6196012552827596 \n",
      "At 287 epoch, Validation Loss: 0.7203326612710952 \n",
      "At 288 epoch, Training Loss: 0.6218036621809004 \n",
      "At 288 epoch, Validation Loss: 0.721213999390602 \n",
      "At 289 epoch, Training Loss: 0.6250211495906118 \n",
      "At 289 epoch, Validation Loss: 0.7159902155399323 \n",
      "At 290 epoch, Training Loss: 0.6145622782409194 \n",
      "At 290 epoch, Validation Loss: 0.7238076657056808 \n",
      "At 291 epoch, Training Loss: 0.6180668406188486 \n",
      "At 291 epoch, Validation Loss: 0.7218632936477661 \n",
      "At 292 epoch, Training Loss: 0.620467536896467 \n",
      "At 292 epoch, Validation Loss: 0.720720535516739 \n",
      "At 293 epoch, Training Loss: 0.6155501354485746 \n",
      "At 293 epoch, Validation Loss: 0.7210249930620195 \n",
      "At 294 epoch, Training Loss: 0.6198470678180458 \n",
      "At 294 epoch, Validation Loss: 0.7137247562408447 \n",
      "At 295 epoch, Training Loss: 0.6200543854385613 \n",
      "At 295 epoch, Validation Loss: 0.7201319217681885 \n",
      "At 296 epoch, Training Loss: 0.6128104668110614 \n",
      "At 296 epoch, Validation Loss: 0.7214844226837158 \n",
      "At 297 epoch, Training Loss: 0.6182053614407782 \n",
      "At 297 epoch, Validation Loss: 0.7215997397899626 \n",
      "At 298 epoch, Training Loss: 0.6218331456184387 \n",
      "At 298 epoch, Validation Loss: 0.7200709730386733 \n",
      "At 299 epoch, Training Loss: 0.6117643985897304 \n",
      "At 299 epoch, Validation Loss: 0.7219921916723251 \n",
      "At 300 epoch, Training Loss: 0.618845432624221 \n",
      "At 300 epoch, Validation Loss: 0.7195443570613862 \n",
      "At 301 epoch, Training Loss: 0.6166082676500081 \n",
      "At 301 epoch, Validation Loss: 0.7171075850725174 \n",
      "At 302 epoch, Training Loss: 0.616926448792219 \n",
      "At 302 epoch, Validation Loss: 0.7196135580539703 \n",
      "At 303 epoch, Training Loss: 0.6125734578818081 \n",
      "At 303 epoch, Validation Loss: 0.727236732840538 \n",
      "At 304 epoch, Training Loss: 0.6164123766124248 \n",
      "At 304 epoch, Validation Loss: 0.7249054133892059 \n",
      "At 305 epoch, Training Loss: 0.6154937125742432 \n",
      "At 305 epoch, Validation Loss: 0.7243655949831009 \n",
      "At 306 epoch, Training Loss: 0.6163437660783528 \n",
      "At 306 epoch, Validation Loss: 0.721167442202568 \n",
      "At 307 epoch, Training Loss: 0.6127023831009865 \n",
      "At 307 epoch, Validation Loss: 0.7216025948524475 \n",
      "At 308 epoch, Training Loss: 0.6222812473773958 \n",
      "At 308 epoch, Validation Loss: 0.7151412606239319 \n",
      "At 309 epoch, Training Loss: 0.6220848452299832 \n",
      "At 309 epoch, Validation Loss: 0.7217906147241593 \n",
      "At 310 epoch, Training Loss: 0.6209319941699504 \n",
      "At 310 epoch, Validation Loss: 0.7194814294576646 \n",
      "At 311 epoch, Training Loss: 0.6179010957479477 \n",
      "At 311 epoch, Validation Loss: 0.7245804578065872 \n",
      "At 312 epoch, Training Loss: 0.6147401366382839 \n",
      "At 312 epoch, Validation Loss: 0.7205461800098418 \n",
      "At 313 epoch, Training Loss: 0.6148432105779648 \n",
      "At 313 epoch, Validation Loss: 0.7256880342960358 \n",
      "At 314 epoch, Training Loss: 0.6149894278496506 \n",
      "At 314 epoch, Validation Loss: 0.7202453494071961 \n",
      "At 315 epoch, Training Loss: 0.6122837089002131 \n",
      "At 315 epoch, Validation Loss: 0.7196765989065169 \n",
      "At 316 epoch, Training Loss: 0.6211929272860283 \n",
      "At 316 epoch, Validation Loss: 0.7176436871290208 \n",
      "At 317 epoch, Training Loss: 0.6167433224618438 \n",
      "At 317 epoch, Validation Loss: 0.7222555965185166 \n",
      "At 318 epoch, Training Loss: 0.6164872311055657 \n",
      "At 318 epoch, Validation Loss: 0.7207679867744445 \n",
      "At 319 epoch, Training Loss: 0.6124963708221912 \n",
      "At 319 epoch, Validation Loss: 0.7209811866283417 \n",
      "At 320 epoch, Training Loss: 0.6193759620189666 \n",
      "At 320 epoch, Validation Loss: 0.7227437674999238 \n",
      "At 321 epoch, Training Loss: 0.6163141002878546 \n",
      "At 321 epoch, Validation Loss: 0.7223760962486266 \n",
      "At 322 epoch, Training Loss: 0.6178809218108652 \n",
      "At 322 epoch, Validation Loss: 0.7219182521104813 \n",
      "At 323 epoch, Training Loss: 0.6138761442154645 \n",
      "At 323 epoch, Validation Loss: 0.7217867970466613 \n",
      "At 324 epoch, Training Loss: 0.6151366956532001 \n",
      "At 324 epoch, Validation Loss: 0.7253638297319412 \n",
      "At 325 epoch, Training Loss: 0.6127177059650423 \n",
      "At 325 epoch, Validation Loss: 0.7209498941898346 \n",
      "At 326 epoch, Training Loss: 0.6153912082314489 \n",
      "At 326 epoch, Validation Loss: 0.7170173346996307 \n",
      "At 327 epoch, Training Loss: 0.6162965379655359 \n",
      "At 327 epoch, Validation Loss: 0.719998973608017 \n",
      "At 328 epoch, Training Loss: 0.6123652920126909 \n",
      "At 328 epoch, Validation Loss: 0.7233290702104567 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 329 epoch, Training Loss: 0.6153702065348623 \n",
      "At 329 epoch, Validation Loss: 0.7171317517757414 \n",
      "At 330 epoch, Training Loss: 0.6133919570595027 \n",
      "At 330 epoch, Validation Loss: 0.7215168595314025 \n",
      "At 331 epoch, Training Loss: 0.6156306125223641 \n",
      "At 331 epoch, Validation Loss: 0.7228265345096588 \n",
      "At 332 epoch, Training Loss: 0.6169431097805498 \n",
      "At 332 epoch, Validation Loss: 0.7168892800807953 \n",
      "At 333 epoch, Training Loss: 0.6162787023931744 \n",
      "At 333 epoch, Validation Loss: 0.7174959033727644 \n",
      "At 334 epoch, Training Loss: 0.6152462806552652 \n",
      "At 334 epoch, Validation Loss: 0.7215320318937302 \n",
      "At 335 epoch, Training Loss: 0.6147220611572269 \n",
      "At 335 epoch, Validation Loss: 0.7246250808238984 \n",
      "At 336 epoch, Training Loss: 0.6189152695238587 \n",
      "At 336 epoch, Validation Loss: 0.7213151186704636 \n",
      "At 337 epoch, Training Loss: 0.6094758853316307 \n",
      "At 337 epoch, Validation Loss: 0.7276899605989458 \n",
      "At 338 epoch, Training Loss: 0.6189084907993678 \n",
      "At 338 epoch, Validation Loss: 0.7215505033731462 \n",
      "At 339 epoch, Training Loss: 0.6182766664773226 \n",
      "At 339 epoch, Validation Loss: 0.7185403019189833 \n",
      "At 340 epoch, Training Loss: 0.6189691741019487 \n",
      "At 340 epoch, Validation Loss: 0.7250936955213546 \n",
      "At 341 epoch, Training Loss: 0.6169384565204381 \n",
      "At 341 epoch, Validation Loss: 0.7193920731544494 \n",
      "At 342 epoch, Training Loss: 0.6127654049545525 \n",
      "At 342 epoch, Validation Loss: 0.7200994879007339 \n",
      "At 343 epoch, Training Loss: 0.6149225778877738 \n",
      "At 343 epoch, Validation Loss: 0.7213194668293 \n",
      "At 344 epoch, Training Loss: 0.6135816521942614 \n",
      "At 344 epoch, Validation Loss: 0.7205829769372939 \n",
      "At 345 epoch, Training Loss: 0.6167644876986742 \n",
      "At 345 epoch, Validation Loss: 0.7188358128070832 \n",
      "At 346 epoch, Training Loss: 0.6198651511222126 \n",
      "At 346 epoch, Validation Loss: 0.7180704414844512 \n",
      "At 347 epoch, Training Loss: 0.6158610858023168 \n",
      "At 347 epoch, Validation Loss: 0.7137457311153411 \n",
      "At 348 epoch, Training Loss: 0.6175044760107994 \n",
      "At 348 epoch, Validation Loss: 0.7170697093009949 \n",
      "At 349 epoch, Training Loss: 0.6183960735797881 \n",
      "At 349 epoch, Validation Loss: 0.7152420818805696 \n",
      "At 350 epoch, Training Loss: 0.614562100917101 \n",
      "At 350 epoch, Validation Loss: 0.719826316833496 \n",
      "At 351 epoch, Training Loss: 0.6146517243236304 \n",
      "At 351 epoch, Validation Loss: 0.7164303362369537 \n",
      "At 352 epoch, Training Loss: 0.6146622687578202 \n",
      "At 352 epoch, Validation Loss: 0.7184423923492431 \n",
      "At 353 epoch, Training Loss: 0.6173647586256266 \n",
      "At 353 epoch, Validation Loss: 0.7157295316457747 \n",
      "At 354 epoch, Training Loss: 0.6148315157741311 \n",
      "At 354 epoch, Validation Loss: 0.7194500893354415 \n",
      "At 355 epoch, Training Loss: 0.6173208750784395 \n",
      "At 355 epoch, Validation Loss: 0.7244369208812712 \n",
      "At 356 epoch, Training Loss: 0.6114289075136187 \n",
      "At 356 epoch, Validation Loss: 0.7157528728246689 \n",
      "At 357 epoch, Training Loss: 0.6157241147011515 \n",
      "At 357 epoch, Validation Loss: 0.7224141091108321 \n",
      "At 358 epoch, Training Loss: 0.6072110775858162 \n",
      "At 358 epoch, Validation Loss: 0.7227553158998489 \n",
      "At 359 epoch, Training Loss: 0.6144862856715916 \n",
      "At 359 epoch, Validation Loss: 0.7161623895168305 \n",
      "At 360 epoch, Training Loss: 0.6195465102791786 \n",
      "At 360 epoch, Validation Loss: 0.7230646163225174 \n",
      "At 361 epoch, Training Loss: 0.6169608421623706 \n",
      "At 361 epoch, Validation Loss: 0.7229900658130645 \n",
      "At 362 epoch, Training Loss: 0.615558699890971 \n",
      "At 362 epoch, Validation Loss: 0.7195307850837708 \n",
      "At 363 epoch, Training Loss: 0.6211432602256537 \n",
      "At 363 epoch, Validation Loss: 0.7175898164510727 \n",
      "At 364 epoch, Training Loss: 0.6168038420379159 \n",
      "At 364 epoch, Validation Loss: 0.7199883013963698 \n",
      "At 365 epoch, Training Loss: 0.6167194467037915 \n",
      "At 365 epoch, Validation Loss: 0.7193862497806548 \n",
      "At 366 epoch, Training Loss: 0.6192243136465552 \n",
      "At 366 epoch, Validation Loss: 0.7234580755233765 \n",
      "At 367 epoch, Training Loss: 0.6187188234180208 \n",
      "At 367 epoch, Validation Loss: 0.7210722655057907 \n",
      "At 368 epoch, Training Loss: 0.612942089512944 \n",
      "At 368 epoch, Validation Loss: 0.7196187198162078 \n",
      "At 369 epoch, Training Loss: 0.6178343605250121 \n",
      "At 369 epoch, Validation Loss: 0.7209041565656662 \n",
      "At 370 epoch, Training Loss: 0.6139827718958258 \n",
      "At 370 epoch, Validation Loss: 0.7220811039209367 \n",
      "At 371 epoch, Training Loss: 0.6142983108758928 \n",
      "At 371 epoch, Validation Loss: 0.722592169046402 \n",
      "At 372 epoch, Training Loss: 0.6149885293096305 \n",
      "At 372 epoch, Validation Loss: 0.7181955456733705 \n",
      "At 373 epoch, Training Loss: 0.6099838130176067 \n",
      "At 373 epoch, Validation Loss: 0.7196442216634751 \n",
      "At 374 epoch, Training Loss: 0.6152032230049372 \n",
      "At 374 epoch, Validation Loss: 0.7179321616888049 \n",
      "At 375 epoch, Training Loss: 0.6125927336513994 \n",
      "At 375 epoch, Validation Loss: 0.7190580189228059 \n",
      "At 376 epoch, Training Loss: 0.6133113283663991 \n",
      "At 376 epoch, Validation Loss: 0.7258044540882111 \n",
      "At 377 epoch, Training Loss: 0.6157740186899902 \n",
      "At 377 epoch, Validation Loss: 0.7218091666698456 \n",
      "At 378 epoch, Training Loss: 0.6153710946440698 \n",
      "At 378 epoch, Validation Loss: 0.7158701479434967 \n",
      "At 379 epoch, Training Loss: 0.6142809573560952 \n",
      "At 379 epoch, Validation Loss: 0.7124838918447494 \n",
      "At 380 epoch, Training Loss: 0.6172758031636476 \n",
      "At 380 epoch, Validation Loss: 0.7162877380847931 \n",
      "At 381 epoch, Training Loss: 0.6144273623824122 \n",
      "At 381 epoch, Validation Loss: 0.7197900623083116 \n",
      "At 382 epoch, Training Loss: 0.6134192600846289 \n",
      "At 382 epoch, Validation Loss: 0.7200790226459504 \n",
      "At 383 epoch, Training Loss: 0.6095475990325211 \n",
      "At 383 epoch, Validation Loss: 0.716184377670288 \n",
      "At 384 epoch, Training Loss: 0.6133196558803321 \n",
      "At 384 epoch, Validation Loss: 0.7181538581848147 \n",
      "At 385 epoch, Training Loss: 0.613315889239311 \n",
      "At 385 epoch, Validation Loss: 0.7202284634113311 \n",
      "At 386 epoch, Training Loss: 0.6091174565255643 \n",
      "At 386 epoch, Validation Loss: 0.7220854401588439 \n",
      "At 387 epoch, Training Loss: 0.6149110943078991 \n",
      "At 387 epoch, Validation Loss: 0.7171093612909318 \n",
      "At 388 epoch, Training Loss: 0.6127934802323577 \n",
      "At 388 epoch, Validation Loss: 0.7164869904518127 \n",
      "At 389 epoch, Training Loss: 0.6182124547660353 \n",
      "At 389 epoch, Validation Loss: 0.7186755448579789 \n",
      "At 390 epoch, Training Loss: 0.6105778466910123 \n",
      "At 390 epoch, Validation Loss: 0.7169311434030531 \n",
      "At 391 epoch, Training Loss: 0.6163821168243888 \n",
      "At 391 epoch, Validation Loss: 0.7186852633953096 \n",
      "At 392 epoch, Training Loss: 0.6138636294752355 \n",
      "At 392 epoch, Validation Loss: 0.7175186812877655 \n",
      "At 393 epoch, Training Loss: 0.6168217271566393 \n",
      "At 393 epoch, Validation Loss: 0.7185224115848541 \n",
      "At 394 epoch, Training Loss: 0.6175716590136291 \n",
      "At 394 epoch, Validation Loss: 0.7171110719442367 \n",
      "At 395 epoch, Training Loss: 0.6165775381028655 \n",
      "At 395 epoch, Validation Loss: 0.7196476072072981 \n",
      "At 396 epoch, Training Loss: 0.6170840587466954 \n",
      "At 396 epoch, Validation Loss: 0.7157381892204285 \n",
      "At 397 epoch, Training Loss: 0.6103725422173736 \n",
      "At 397 epoch, Validation Loss: 0.714080783724785 \n",
      "At 398 epoch, Training Loss: 0.6173040840774773 \n",
      "At 398 epoch, Validation Loss: 0.7189276844263077 \n",
      "At 399 epoch, Training Loss: 0.6127768028527495 \n",
      "At 399 epoch, Validation Loss: 0.7166948318481445 \n",
      "At 400 epoch, Training Loss: 0.6161717526614667 \n",
      "At 400 epoch, Validation Loss: 0.7180530369281768 \n",
      "At 401 epoch, Training Loss: 0.6129612065851691 \n",
      "At 401 epoch, Validation Loss: 0.7200817465782166 \n",
      "At 402 epoch, Training Loss: 0.6145432405173776 \n",
      "At 402 epoch, Validation Loss: 0.7173512011766435 \n",
      "At 403 epoch, Training Loss: 0.6143934540450576 \n",
      "At 403 epoch, Validation Loss: 0.725824037194252 \n",
      "At 404 epoch, Training Loss: 0.6172218106687072 \n",
      "At 404 epoch, Validation Loss: 0.7224854528903963 \n",
      "At 405 epoch, Training Loss: 0.61729056686163 \n",
      "At 405 epoch, Validation Loss: 0.7233609825372697 \n",
      "At 406 epoch, Training Loss: 0.6109649281948802 \n",
      "At 406 epoch, Validation Loss: 0.7212000519037248 \n",
      "At 407 epoch, Training Loss: 0.6116209998726843 \n",
      "At 407 epoch, Validation Loss: 0.7243011862039566 \n",
      "At 408 epoch, Training Loss: 0.6153741676360369 \n",
      "At 408 epoch, Validation Loss: 0.7211756318807603 \n",
      "At 409 epoch, Training Loss: 0.6122146941721442 \n",
      "At 409 epoch, Validation Loss: 0.7218758642673493 \n",
      "At 410 epoch, Training Loss: 0.6132041864097116 \n",
      "At 410 epoch, Validation Loss: 0.7188479542732238 \n",
      "At 411 epoch, Training Loss: 0.6144254937767984 \n",
      "At 411 epoch, Validation Loss: 0.7217608034610749 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 412 epoch, Training Loss: 0.618827528133988 \n",
      "At 412 epoch, Validation Loss: 0.7183091491460799 \n",
      "At 413 epoch, Training Loss: 0.609567037969828 \n",
      "At 413 epoch, Validation Loss: 0.7228850930929185 \n",
      "At 414 epoch, Training Loss: 0.6134657781571148 \n",
      "At 414 epoch, Validation Loss: 0.7198781311511993 \n",
      "At 415 epoch, Training Loss: 0.6161957226693628 \n",
      "At 415 epoch, Validation Loss: 0.7252548307180404 \n",
      "At 416 epoch, Training Loss: 0.613948775455356 \n",
      "At 416 epoch, Validation Loss: 0.7214984834194184 \n",
      "At 417 epoch, Training Loss: 0.6129937693476677 \n",
      "At 417 epoch, Validation Loss: 0.7233455151319503 \n",
      "At 418 epoch, Training Loss: 0.6132862120866777 \n",
      "At 418 epoch, Validation Loss: 0.7168715447187424 \n",
      "At 419 epoch, Training Loss: 0.6128746509552003 \n",
      "At 419 epoch, Validation Loss: 0.7233286917209627 \n",
      "At 420 epoch, Training Loss: 0.6161786749958987 \n",
      "At 420 epoch, Validation Loss: 0.7219389975070953 \n",
      "At 421 epoch, Training Loss: 0.6129031989723444 \n",
      "At 421 epoch, Validation Loss: 0.7213851660490035 \n",
      "At 422 epoch, Training Loss: 0.6111943230032922 \n",
      "At 422 epoch, Validation Loss: 0.7206826269626617 \n",
      "At 423 epoch, Training Loss: 0.6174224045127629 \n",
      "At 423 epoch, Validation Loss: 0.7205689430236816 \n",
      "At 424 epoch, Training Loss: 0.6193627715110779 \n",
      "At 424 epoch, Validation Loss: 0.7214441984891891 \n",
      "At 425 epoch, Training Loss: 0.6103385388851168 \n",
      "At 425 epoch, Validation Loss: 0.7217539072036744 \n",
      "At 426 epoch, Training Loss: 0.6149972114711999 \n",
      "At 426 epoch, Validation Loss: 0.7204843372106553 \n",
      "At 427 epoch, Training Loss: 0.6145959768444299 \n",
      "At 427 epoch, Validation Loss: 0.7199189960956573 \n",
      "At 428 epoch, Training Loss: 0.6129693236202 \n",
      "At 428 epoch, Validation Loss: 0.7218072474002838 \n",
      "At 429 epoch, Training Loss: 0.6148763392120598 \n",
      "At 429 epoch, Validation Loss: 0.719241839647293 \n",
      "At 430 epoch, Training Loss: 0.6147249627858403 \n",
      "At 430 epoch, Validation Loss: 0.7200831115245818 \n",
      "At 431 epoch, Training Loss: 0.6119094640016554 \n",
      "At 431 epoch, Validation Loss: 0.7238318622112275 \n",
      "At 432 epoch, Training Loss: 0.615876914933324 \n",
      "At 432 epoch, Validation Loss: 0.7188326597213747 \n",
      "At 433 epoch, Training Loss: 0.6192149657756091 \n",
      "At 433 epoch, Validation Loss: 0.7174760073423385 \n",
      "At 434 epoch, Training Loss: 0.6129402499645946 \n",
      "At 434 epoch, Validation Loss: 0.7157178550958634 \n",
      "At 435 epoch, Training Loss: 0.6127601508051156 \n",
      "At 435 epoch, Validation Loss: 0.7205830067396165 \n",
      "At 436 epoch, Training Loss: 0.6127790935337543 \n",
      "At 436 epoch, Validation Loss: 0.7176932275295256 \n",
      "At 437 epoch, Training Loss: 0.6169627547264102 \n",
      "At 437 epoch, Validation Loss: 0.7211346209049224 \n",
      "At 438 epoch, Training Loss: 0.6169049341231582 \n",
      "At 438 epoch, Validation Loss: 0.7158468872308732 \n",
      "At 439 epoch, Training Loss: 0.6140487913042306 \n",
      "At 439 epoch, Validation Loss: 0.7151804625988007 \n",
      "At 440 epoch, Training Loss: 0.6132866103202108 \n",
      "At 440 epoch, Validation Loss: 0.7182392418384551 \n",
      "At 441 epoch, Training Loss: 0.6077527884393928 \n",
      "At 441 epoch, Validation Loss: 0.7162104547023773 \n",
      "At 442 epoch, Training Loss: 0.6166693747043608 \n",
      "At 442 epoch, Validation Loss: 0.7210137724876404 \n",
      "At 443 epoch, Training Loss: 0.6101599801331753 \n",
      "At 443 epoch, Validation Loss: 0.7157633543014525 \n",
      "At 444 epoch, Training Loss: 0.6094933483749629 \n",
      "At 444 epoch, Validation Loss: 0.7211865782737732 \n",
      "At 445 epoch, Training Loss: 0.6116859808564182 \n",
      "At 445 epoch, Validation Loss: 0.7184510171413422 \n",
      "At 446 epoch, Training Loss: 0.6155686713755132 \n",
      "At 446 epoch, Validation Loss: 0.7229010075330733 \n",
      "At 447 epoch, Training Loss: 0.6153500828891991 \n",
      "At 447 epoch, Validation Loss: 0.715292254090309 \n",
      "At 448 epoch, Training Loss: 0.6132533051073551 \n",
      "At 448 epoch, Validation Loss: 0.7149690657854081 \n",
      "At 449 epoch, Training Loss: 0.6147008962929249 \n",
      "At 449 epoch, Validation Loss: 0.7168273478746414 \n",
      "At 450 epoch, Training Loss: 0.612296807393432 \n",
      "At 450 epoch, Validation Loss: 0.7240382879972458 \n",
      "At 451 epoch, Training Loss: 0.6206761658191685 \n",
      "At 451 epoch, Validation Loss: 0.7207148313522339 \n",
      "At 452 epoch, Training Loss: 0.6112014446407555 \n",
      "At 452 epoch, Validation Loss: 0.7210968136787415 \n",
      "At 453 epoch, Training Loss: 0.6137562930583951 \n",
      "At 453 epoch, Validation Loss: 0.7190068453550339 \n",
      "At 454 epoch, Training Loss: 0.6106938373297455 \n",
      "At 454 epoch, Validation Loss: 0.7177766442298888 \n",
      "At 455 epoch, Training Loss: 0.614114741049707 \n",
      "At 455 epoch, Validation Loss: 0.7181507200002669 \n",
      "At 456 epoch, Training Loss: 0.6110122777521614 \n",
      "At 456 epoch, Validation Loss: 0.7175727993249894 \n",
      "At 457 epoch, Training Loss: 0.6173375163227316 \n",
      "At 457 epoch, Validation Loss: 0.7195994526147842 \n",
      "At 458 epoch, Training Loss: 0.6173879850655793 \n",
      "At 458 epoch, Validation Loss: 0.7170700073242188 \n",
      "At 459 epoch, Training Loss: 0.6071504563093184 \n",
      "At 459 epoch, Validation Loss: 0.7237321406602859 \n",
      "At 460 epoch, Training Loss: 0.6111386895179748 \n",
      "At 460 epoch, Validation Loss: 0.7180306851863862 \n",
      "At 461 epoch, Training Loss: 0.6113586500287058 \n",
      "At 461 epoch, Validation Loss: 0.7169996172189712 \n",
      "At 462 epoch, Training Loss: 0.6123776074498892 \n",
      "At 462 epoch, Validation Loss: 0.71782066822052 \n",
      "At 463 epoch, Training Loss: 0.6136989314109088 \n",
      "At 463 epoch, Validation Loss: 0.7212192863225937 \n",
      "At 464 epoch, Training Loss: 0.6128838323056699 \n",
      "At 464 epoch, Validation Loss: 0.7163473308086397 \n",
      "At 465 epoch, Training Loss: 0.6136497035622593 \n",
      "At 465 epoch, Validation Loss: 0.7216715663671494 \n",
      "At 466 epoch, Training Loss: 0.6121877517551186 \n",
      "At 466 epoch, Validation Loss: 0.7198407411575318 \n",
      "At 467 epoch, Training Loss: 0.6089949559420346 \n",
      "At 467 epoch, Validation Loss: 0.7233523845672608 \n",
      "At 468 epoch, Training Loss: 0.618167185410857 \n",
      "At 468 epoch, Validation Loss: 0.7191183388233184 \n",
      "At 469 epoch, Training Loss: 0.6037550125271082 \n",
      "At 469 epoch, Validation Loss: 0.713942363858223 \n",
      "At 470 epoch, Training Loss: 0.6121891688555479 \n",
      "At 470 epoch, Validation Loss: 0.720029303431511 \n",
      "At 471 epoch, Training Loss: 0.6138892859220507 \n",
      "At 471 epoch, Validation Loss: 0.7211124509572984 \n",
      "At 472 epoch, Training Loss: 0.613062920048833 \n",
      "At 472 epoch, Validation Loss: 0.7223498731851578 \n",
      "At 473 epoch, Training Loss: 0.6130078166723255 \n",
      "At 473 epoch, Validation Loss: 0.7213832080364228 \n",
      "At 474 epoch, Training Loss: 0.6129532057791949 \n",
      "At 474 epoch, Validation Loss: 0.7235303163528444 \n",
      "At 475 epoch, Training Loss: 0.6099200788885358 \n",
      "At 475 epoch, Validation Loss: 0.7221951574087143 \n",
      "At 476 epoch, Training Loss: 0.613859896734357 \n",
      "At 476 epoch, Validation Loss: 0.718867865204811 \n",
      "At 477 epoch, Training Loss: 0.6064391292631626 \n",
      "At 477 epoch, Validation Loss: 0.7191201090812682 \n",
      "At 478 epoch, Training Loss: 0.6083789318799975 \n",
      "At 478 epoch, Validation Loss: 0.7224825143814088 \n",
      "At 479 epoch, Training Loss: 0.6171853348612786 \n",
      "At 479 epoch, Validation Loss: 0.7203604847192765 \n",
      "At 480 epoch, Training Loss: 0.6065368868410589 \n",
      "At 480 epoch, Validation Loss: 0.720335277915001 \n",
      "At 481 epoch, Training Loss: 0.6161782409995793 \n",
      "At 481 epoch, Validation Loss: 0.7213514029979706 \n",
      "At 482 epoch, Training Loss: 0.6148771680891514 \n",
      "At 482 epoch, Validation Loss: 0.7245448797941207 \n",
      "At 483 epoch, Training Loss: 0.6091464515775444 \n",
      "At 483 epoch, Validation Loss: 0.7193055242300034 \n",
      "At 484 epoch, Training Loss: 0.6104129996150734 \n",
      "At 484 epoch, Validation Loss: 0.7217177033424377 \n",
      "At 485 epoch, Training Loss: 0.6151773873716596 \n",
      "At 485 epoch, Validation Loss: 0.7230324655771255 \n",
      "At 486 epoch, Training Loss: 0.611360207945108 \n",
      "At 486 epoch, Validation Loss: 0.7222939431667329 \n",
      "At 487 epoch, Training Loss: 0.6124556377530096 \n",
      "At 487 epoch, Validation Loss: 0.7237456768751144 \n",
      "At 488 epoch, Training Loss: 0.609419012069702 \n",
      "At 488 epoch, Validation Loss: 0.7156560391187669 \n",
      "At 489 epoch, Training Loss: 0.6182798769325016 \n",
      "At 489 epoch, Validation Loss: 0.7229662746191026 \n",
      "At 490 epoch, Training Loss: 0.6115772865712642 \n",
      "At 490 epoch, Validation Loss: 0.7196122258901594 \n",
      "At 491 epoch, Training Loss: 0.6120991948992011 \n",
      "At 491 epoch, Validation Loss: 0.7214631676673889 \n",
      "At 492 epoch, Training Loss: 0.6114025689661505 \n",
      "At 492 epoch, Validation Loss: 0.7228947728872299 \n",
      "At 493 epoch, Training Loss: 0.6137991059571501 \n",
      "At 493 epoch, Validation Loss: 0.723323541879654 \n",
      "At 494 epoch, Training Loss: 0.6166726931929591 \n",
      "At 494 epoch, Validation Loss: 0.7239351838827133 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 495 epoch, Training Loss: 0.6144832164049147 \n",
      "At 495 epoch, Validation Loss: 0.7230188131332399 \n",
      "At 496 epoch, Training Loss: 0.6090324021875855 \n",
      "At 496 epoch, Validation Loss: 0.7226788073778154 \n",
      "At 497 epoch, Training Loss: 0.6197492334991691 \n",
      "At 497 epoch, Validation Loss: 0.7239201456308366 \n",
      "At 498 epoch, Training Loss: 0.6169153884053235 \n",
      "At 498 epoch, Validation Loss: 0.7233923345804214 \n",
      "At 499 epoch, Training Loss: 0.612482510879636 \n",
      "At 499 epoch, Validation Loss: 0.72438582777977 \n",
      "At 500 epoch, Training Loss: 0.6164582718163727 \n",
      "At 500 epoch, Validation Loss: 0.7246517628431319 \n",
      "At 501 epoch, Training Loss: 0.6098561756312851 \n",
      "At 501 epoch, Validation Loss: 0.7239580690860749 \n",
      "At 502 epoch, Training Loss: 0.6164132088422776 \n",
      "At 502 epoch, Validation Loss: 0.7175603359937668 \n",
      "At 503 epoch, Training Loss: 0.6135357648134232 \n",
      "At 503 epoch, Validation Loss: 0.7197738468647001 \n",
      "At 504 epoch, Training Loss: 0.6142724849283699 \n",
      "At 504 epoch, Validation Loss: 0.7213130444288254 \n",
      "At 505 epoch, Training Loss: 0.6137627426534892 \n",
      "At 505 epoch, Validation Loss: 0.7183149397373199 \n",
      "At 506 epoch, Training Loss: 0.6107489697635177 \n",
      "At 506 epoch, Validation Loss: 0.7212230145931242 \n",
      "At 507 epoch, Training Loss: 0.6078265137970452 \n",
      "At 507 epoch, Validation Loss: 0.7155223280191422 \n",
      "At 508 epoch, Training Loss: 0.6119525037705899 \n",
      "At 508 epoch, Validation Loss: 0.7211325973272324 \n",
      "At 509 epoch, Training Loss: 0.6145057987421755 \n",
      "At 509 epoch, Validation Loss: 0.7193779438734055 \n",
      "At 510 epoch, Training Loss: 0.6107968257740142 \n",
      "At 510 epoch, Validation Loss: 0.7209131956100463 \n",
      "At 511 epoch, Training Loss: 0.6106753058731559 \n",
      "At 511 epoch, Validation Loss: 0.7226417541503906 \n",
      "At 512 epoch, Training Loss: 0.6131416581571102 \n",
      "At 512 epoch, Validation Loss: 0.7220035821199418 \n",
      "At 513 epoch, Training Loss: 0.6091447044163939 \n",
      "At 513 epoch, Validation Loss: 0.7206247031688691 \n",
      "At 514 epoch, Training Loss: 0.6151890862733127 \n",
      "At 514 epoch, Validation Loss: 0.7215492248535158 \n",
      "At 515 epoch, Training Loss: 0.6096488088369368 \n",
      "At 515 epoch, Validation Loss: 0.7261015981435777 \n",
      "At 516 epoch, Training Loss: 0.6136453267186882 \n",
      "At 516 epoch, Validation Loss: 0.7215386897325515 \n",
      "At 517 epoch, Training Loss: 0.6152264297008512 \n",
      "At 517 epoch, Validation Loss: 0.7219166725873948 \n",
      "At 518 epoch, Training Loss: 0.613327644392848 \n",
      "At 518 epoch, Validation Loss: 0.7198619067668917 \n",
      "At 519 epoch, Training Loss: 0.6130421511828897 \n",
      "At 519 epoch, Validation Loss: 0.7240119755268096 \n",
      "At 520 epoch, Training Loss: 0.6093604344874619 \n",
      "At 520 epoch, Validation Loss: 0.7197264403104782 \n",
      "At 521 epoch, Training Loss: 0.6106128737330434 \n",
      "At 521 epoch, Validation Loss: 0.7234376192092896 \n",
      "At 522 epoch, Training Loss: 0.6064806289970875 \n",
      "At 522 epoch, Validation Loss: 0.7275465071201324 \n",
      "At 523 epoch, Training Loss: 0.6095883548259736 \n",
      "At 523 epoch, Validation Loss: 0.7173607677221299 \n",
      "At 524 epoch, Training Loss: 0.608683067560196 \n",
      "At 524 epoch, Validation Loss: 0.7233416467905045 \n",
      "At 525 epoch, Training Loss: 0.6162995617836714 \n",
      "At 525 epoch, Validation Loss: 0.7206171035766601 \n",
      "At 526 epoch, Training Loss: 0.6035330735146999 \n",
      "At 526 epoch, Validation Loss: 0.7214875698089599 \n",
      "At 527 epoch, Training Loss: 0.6063959922641513 \n",
      "At 527 epoch, Validation Loss: 0.7239547282457351 \n",
      "At 528 epoch, Training Loss: 0.6065228819847108 \n",
      "At 528 epoch, Validation Loss: 0.721607756614685 \n",
      "At 529 epoch, Training Loss: 0.6162790760397906 \n",
      "At 529 epoch, Validation Loss: 0.721195873618126 \n",
      "At 530 epoch, Training Loss: 0.6134356405586004 \n",
      "At 530 epoch, Validation Loss: 0.7238036483526231 \n",
      "At 531 epoch, Training Loss: 0.612425999715924 \n",
      "At 531 epoch, Validation Loss: 0.7227719753980636 \n",
      "At 532 epoch, Training Loss: 0.6148720383644101 \n",
      "At 532 epoch, Validation Loss: 0.721061059832573 \n",
      "At 533 epoch, Training Loss: 0.6134819820523264 \n",
      "At 533 epoch, Validation Loss: 0.7229216277599334 \n",
      "At 534 epoch, Training Loss: 0.611609001085162 \n",
      "At 534 epoch, Validation Loss: 0.7212470531463623 \n",
      "At 535 epoch, Training Loss: 0.6123644426465035 \n",
      "At 535 epoch, Validation Loss: 0.7235395133495331 \n",
      "At 536 epoch, Training Loss: 0.6133890632539987 \n",
      "At 536 epoch, Validation Loss: 0.7210407406091689 \n",
      "At 537 epoch, Training Loss: 0.6165186036378146 \n",
      "At 537 epoch, Validation Loss: 0.7253941237926482 \n",
      "At 538 epoch, Training Loss: 0.6154152840375902 \n",
      "At 538 epoch, Validation Loss: 0.7222186237573623 \n",
      "At 539 epoch, Training Loss: 0.6061071176081898 \n",
      "At 539 epoch, Validation Loss: 0.7224104076623916 \n",
      "At 540 epoch, Training Loss: 0.6119683451950552 \n",
      "At 540 epoch, Validation Loss: 0.722197476029396 \n",
      "At 541 epoch, Training Loss: 0.6140131488442422 \n",
      "At 541 epoch, Validation Loss: 0.7245027512311936 \n",
      "At 542 epoch, Training Loss: 0.6138930924236772 \n",
      "At 542 epoch, Validation Loss: 0.7217897325754166 \n",
      "At 543 epoch, Training Loss: 0.6080811213701963 \n",
      "At 543 epoch, Validation Loss: 0.7246540427207948 \n",
      "At 544 epoch, Training Loss: 0.6097767166793348 \n",
      "At 544 epoch, Validation Loss: 0.7257937669754029 \n",
      "At 545 epoch, Training Loss: 0.6130772843956949 \n",
      "At 545 epoch, Validation Loss: 0.7277991473674774 \n",
      "At 546 epoch, Training Loss: 0.6075022608041766 \n",
      "At 546 epoch, Validation Loss: 0.7251882642507553 \n",
      "At 547 epoch, Training Loss: 0.6116108451038599 \n",
      "At 547 epoch, Validation Loss: 0.7237706571817398 \n",
      "At 548 epoch, Training Loss: 0.6103111337870364 \n",
      "At 548 epoch, Validation Loss: 0.7250084817409514 \n",
      "At 549 epoch, Training Loss: 0.6118919532746074 \n",
      "At 549 epoch, Validation Loss: 0.7198420524597168 \n",
      "At 550 epoch, Training Loss: 0.6079768246039754 \n",
      "At 550 epoch, Validation Loss: 0.7238061517477035 \n",
      "At 551 epoch, Training Loss: 0.6104058701545 \n",
      "At 551 epoch, Validation Loss: 0.7312662780284882 \n",
      "At 552 epoch, Training Loss: 0.6135817147791388 \n",
      "At 552 epoch, Validation Loss: 0.7223419040441512 \n",
      "At 553 epoch, Training Loss: 0.6149827647954224 \n",
      "At 553 epoch, Validation Loss: 0.7231134593486785 \n",
      "At 554 epoch, Training Loss: 0.6157732557505374 \n",
      "At 554 epoch, Validation Loss: 0.725630047917366 \n",
      "At 555 epoch, Training Loss: 0.6074127528816466 \n",
      "At 555 epoch, Validation Loss: 0.7214841187000274 \n",
      "At 556 epoch, Training Loss: 0.6084679346531634 \n",
      "At 556 epoch, Validation Loss: 0.7248336493968963 \n",
      "At 557 epoch, Training Loss: 0.613315371051431 \n",
      "At 557 epoch, Validation Loss: 0.7242267310619352 \n",
      "At 558 epoch, Training Loss: 0.6131502382457256 \n",
      "At 558 epoch, Validation Loss: 0.7250007718801499 \n",
      "At 559 epoch, Training Loss: 0.6062325874343515 \n",
      "At 559 epoch, Validation Loss: 0.7249068021774294 \n",
      "At 560 epoch, Training Loss: 0.6156487949192526 \n",
      "At 560 epoch, Validation Loss: 0.7195207089185713 \n",
      "At 561 epoch, Training Loss: 0.6094510022550822 \n",
      "At 561 epoch, Validation Loss: 0.7217166244983674 \n",
      "At 562 epoch, Training Loss: 0.6130435004830364 \n",
      "At 562 epoch, Validation Loss: 0.7216962277889252 \n",
      "At 563 epoch, Training Loss: 0.6155603528022768 \n",
      "At 563 epoch, Validation Loss: 0.7238615989685059 \n",
      "At 564 epoch, Training Loss: 0.6123750247061253 \n",
      "At 564 epoch, Validation Loss: 0.7273523718118666 \n",
      "At 565 epoch, Training Loss: 0.61015729829669 \n",
      "At 565 epoch, Validation Loss: 0.7198696106672287 \n",
      "At 566 epoch, Training Loss: 0.6138953953981398 \n",
      "At 566 epoch, Validation Loss: 0.7263734489679338 \n",
      "At 567 epoch, Training Loss: 0.6128392428159711 \n",
      "At 567 epoch, Validation Loss: 0.7228118479251862 \n",
      "At 568 epoch, Training Loss: 0.6104873128235343 \n",
      "At 568 epoch, Validation Loss: 0.7245216488838195 \n",
      "At 569 epoch, Training Loss: 0.6150259163230657 \n",
      "At 569 epoch, Validation Loss: 0.7274300009012222 \n",
      "At 570 epoch, Training Loss: 0.6170614462345836 \n",
      "At 570 epoch, Validation Loss: 0.7261646002531051 \n",
      "At 571 epoch, Training Loss: 0.613905365765095 \n",
      "At 571 epoch, Validation Loss: 0.7202710539102555 \n",
      "At 572 epoch, Training Loss: 0.6091501411050556 \n",
      "At 572 epoch, Validation Loss: 0.7243680506944656 \n",
      "At 573 epoch, Training Loss: 0.6152023695409298 \n",
      "At 573 epoch, Validation Loss: 0.726520085334778 \n",
      "At 574 epoch, Training Loss: 0.6114581897854799 \n",
      "At 574 epoch, Validation Loss: 0.7210344702005386 \n",
      "At 575 epoch, Training Loss: 0.6088052347302433 \n",
      "At 575 epoch, Validation Loss: 0.7221851050853729 \n",
      "At 576 epoch, Training Loss: 0.6149292401969431 \n",
      "At 576 epoch, Validation Loss: 0.7259650856256484 \n",
      "At 577 epoch, Training Loss: 0.6140256844460968 \n",
      "At 577 epoch, Validation Loss: 0.7214584141969681 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 578 epoch, Training Loss: 0.610926683247089 \n",
      "At 578 epoch, Validation Loss: 0.7241243481636047 \n",
      "At 579 epoch, Training Loss: 0.6120078485459091 \n",
      "At 579 epoch, Validation Loss: 0.7274895071983337 \n",
      "At 580 epoch, Training Loss: 0.6084715489298106 \n",
      "At 580 epoch, Validation Loss: 0.7191528499126434 \n",
      "At 581 epoch, Training Loss: 0.6114512465894225 \n",
      "At 581 epoch, Validation Loss: 0.7283814042806627 \n",
      "At 582 epoch, Training Loss: 0.613940661400556 \n",
      "At 582 epoch, Validation Loss: 0.7193027883768083 \n",
      "At 583 epoch, Training Loss: 0.6151361264288426 \n",
      "At 583 epoch, Validation Loss: 0.7201322585344315 \n",
      "At 584 epoch, Training Loss: 0.6117267385125159 \n",
      "At 584 epoch, Validation Loss: 0.7273557245731354 \n",
      "At 585 epoch, Training Loss: 0.6057400871068237 \n",
      "At 585 epoch, Validation Loss: 0.7231995195150375 \n",
      "At 586 epoch, Training Loss: 0.6134650528430938 \n",
      "At 586 epoch, Validation Loss: 0.727240392565727 \n",
      "At 587 epoch, Training Loss: 0.6170032776892183 \n",
      "At 587 epoch, Validation Loss: 0.7205557793378831 \n",
      "At 588 epoch, Training Loss: 0.6198758687824011 \n",
      "At 588 epoch, Validation Loss: 0.7200195491313934 \n",
      "At 589 epoch, Training Loss: 0.6061884444206953 \n",
      "At 589 epoch, Validation Loss: 0.7229965239763262 \n",
      "At 590 epoch, Training Loss: 0.6096766028553249 \n",
      "At 590 epoch, Validation Loss: 0.7211460828781127 \n",
      "At 591 epoch, Training Loss: 0.6129159938544031 \n",
      "At 591 epoch, Validation Loss: 0.7203459113836288 \n",
      "At 592 epoch, Training Loss: 0.6128944788128137 \n",
      "At 592 epoch, Validation Loss: 0.7195355743169782 \n",
      "At 593 epoch, Training Loss: 0.6120579387992621 \n",
      "At 593 epoch, Validation Loss: 0.7191467642784118 \n",
      "At 594 epoch, Training Loss: 0.6156416077166796 \n",
      "At 594 epoch, Validation Loss: 0.7191163659095764 \n",
      "At 595 epoch, Training Loss: 0.6087145149707794 \n",
      "At 595 epoch, Validation Loss: 0.7212061911821365 \n",
      "At 596 epoch, Training Loss: 0.6109180636703966 \n",
      "At 596 epoch, Validation Loss: 0.7197420150041579 \n",
      "At 597 epoch, Training Loss: 0.6157338894903656 \n",
      "At 597 epoch, Validation Loss: 0.7152245700359342 \n",
      "At 598 epoch, Training Loss: 0.6133685313165187 \n",
      "At 598 epoch, Validation Loss: 0.7207651138305664 \n",
      "At 599 epoch, Training Loss: 0.6130486432462933 \n",
      "At 599 epoch, Validation Loss: 0.7218275785446167 \n",
      "At 600 epoch, Training Loss: 0.6074538927525283 \n",
      "At 600 epoch, Validation Loss: 0.7206318110227585 \n",
      "At 601 epoch, Training Loss: 0.6158640280365941 \n",
      "At 601 epoch, Validation Loss: 0.7177070170640945 \n",
      "At 602 epoch, Training Loss: 0.6174131551757457 \n",
      "At 602 epoch, Validation Loss: 0.7217831552028655 \n",
      "At 603 epoch, Training Loss: 0.6118119262158869 \n",
      "At 603 epoch, Validation Loss: 0.7238274723291398 \n",
      "At 604 epoch, Training Loss: 0.6114580817520617 \n",
      "At 604 epoch, Validation Loss: 0.7192882835865022 \n",
      "At 605 epoch, Training Loss: 0.6102106731384992 \n",
      "At 605 epoch, Validation Loss: 0.7199000030755995 \n",
      "At 606 epoch, Training Loss: 0.61381467320025 \n",
      "At 606 epoch, Validation Loss: 0.7221273720264435 \n",
      "At 607 epoch, Training Loss: 0.6090057447552681 \n",
      "At 607 epoch, Validation Loss: 0.7198927283287049 \n",
      "At 608 epoch, Training Loss: 0.6117936156690121 \n",
      "At 608 epoch, Validation Loss: 0.7215124666690825 \n",
      "At 609 epoch, Training Loss: 0.6074246518313882 \n",
      "At 609 epoch, Validation Loss: 0.7192889630794524 \n",
      "At 610 epoch, Training Loss: 0.6124661829322574 \n",
      "At 610 epoch, Validation Loss: 0.7198208570480344 \n",
      "At 611 epoch, Training Loss: 0.609839180111885 \n",
      "At 611 epoch, Validation Loss: 0.7231023311614991 \n",
      "At 612 epoch, Training Loss: 0.6144801218062638 \n",
      "At 612 epoch, Validation Loss: 0.718518289923668 \n",
      "At 613 epoch, Training Loss: 0.616749491915107 \n",
      "At 613 epoch, Validation Loss: 0.717159080505371 \n",
      "At 614 epoch, Training Loss: 0.6155942291021348 \n",
      "At 614 epoch, Validation Loss: 0.7206482321023941 \n",
      "At 615 epoch, Training Loss: 0.6114047070965166 \n",
      "At 615 epoch, Validation Loss: 0.7224941194057467 \n",
      "At 616 epoch, Training Loss: 0.6071729831397533 \n",
      "At 616 epoch, Validation Loss: 0.7239803463220595 \n",
      "At 617 epoch, Training Loss: 0.6125562310218806 \n",
      "At 617 epoch, Validation Loss: 0.7213609278202057 \n",
      "At 618 epoch, Training Loss: 0.6109666593372823 \n",
      "At 618 epoch, Validation Loss: 0.7212657570838928 \n",
      "At 619 epoch, Training Loss: 0.6157416347414252 \n",
      "At 619 epoch, Validation Loss: 0.7214080989360808 \n",
      "At 620 epoch, Training Loss: 0.6106997162103658 \n",
      "At 620 epoch, Validation Loss: 0.7195139169692992 \n",
      "At 621 epoch, Training Loss: 0.6121116220951078 \n",
      "At 621 epoch, Validation Loss: 0.721355339884758 \n",
      "At 622 epoch, Training Loss: 0.6087378609925508 \n",
      "At 622 epoch, Validation Loss: 0.7186580419540406 \n",
      "At 623 epoch, Training Loss: 0.6123871207237244 \n",
      "At 623 epoch, Validation Loss: 0.7188482791185379 \n",
      "At 624 epoch, Training Loss: 0.6130334924906493 \n",
      "At 624 epoch, Validation Loss: 0.7197801649570464 \n",
      "At 625 epoch, Training Loss: 0.6128269672393799 \n",
      "At 625 epoch, Validation Loss: 0.7206683963537216 \n",
      "At 626 epoch, Training Loss: 0.6090548019856212 \n",
      "At 626 epoch, Validation Loss: 0.7216653168201446 \n",
      "At 627 epoch, Training Loss: 0.6096546171233055 \n",
      "At 627 epoch, Validation Loss: 0.7247689962387085 \n",
      "At 628 epoch, Training Loss: 0.6087617956101892 \n",
      "At 628 epoch, Validation Loss: 0.7205219447612763 \n",
      "At 629 epoch, Training Loss: 0.6083030186593532 \n",
      "At 629 epoch, Validation Loss: 0.7192702084779738 \n",
      "At 630 epoch, Training Loss: 0.612955648079514 \n",
      "At 630 epoch, Validation Loss: 0.7226311922073364 \n",
      "At 631 epoch, Training Loss: 0.6082639757543801 \n",
      "At 631 epoch, Validation Loss: 0.722104835510254 \n",
      "At 632 epoch, Training Loss: 0.6135559685528279 \n",
      "At 632 epoch, Validation Loss: 0.7229284942150115 \n",
      "At 633 epoch, Training Loss: 0.6082082450389864 \n",
      "At 633 epoch, Validation Loss: 0.7240132182836533 \n",
      "At 634 epoch, Training Loss: 0.6106438029557466 \n",
      "At 634 epoch, Validation Loss: 0.7237646967172623 \n",
      "At 635 epoch, Training Loss: 0.6146668110042809 \n",
      "At 635 epoch, Validation Loss: 0.7186512231826783 \n",
      "At 636 epoch, Training Loss: 0.6138073764741419 \n",
      "At 636 epoch, Validation Loss: 0.7185613572597503 \n",
      "At 637 epoch, Training Loss: 0.6096043501049278 \n",
      "At 637 epoch, Validation Loss: 0.7199552267789843 \n",
      "At 638 epoch, Training Loss: 0.6100130058825016 \n",
      "At 638 epoch, Validation Loss: 0.7232238292694092 \n",
      "At 639 epoch, Training Loss: 0.6127784263342618 \n",
      "At 639 epoch, Validation Loss: 0.7187048435211182 \n",
      "At 640 epoch, Training Loss: 0.6116664156317707 \n",
      "At 640 epoch, Validation Loss: 0.7238854318857193 \n",
      "At 641 epoch, Training Loss: 0.6119153879582884 \n",
      "At 641 epoch, Validation Loss: 0.7205271273851395 \n",
      "At 642 epoch, Training Loss: 0.6097235560417177 \n",
      "At 642 epoch, Validation Loss: 0.7207205027341843 \n",
      "At 643 epoch, Training Loss: 0.6181525856256485 \n",
      "At 643 epoch, Validation Loss: 0.7228875547647476 \n",
      "At 644 epoch, Training Loss: 0.6132746919989585 \n",
      "At 644 epoch, Validation Loss: 0.725541153550148 \n",
      "At 645 epoch, Training Loss: 0.6092904567718507 \n",
      "At 645 epoch, Validation Loss: 0.7207834154367446 \n",
      "At 646 epoch, Training Loss: 0.6158356372267006 \n",
      "At 646 epoch, Validation Loss: 0.7210903286933898 \n",
      "At 647 epoch, Training Loss: 0.6090915706008673 \n",
      "At 647 epoch, Validation Loss: 0.7209597647190094 \n",
      "At 648 epoch, Training Loss: 0.6091996025294066 \n",
      "At 648 epoch, Validation Loss: 0.7194582939147951 \n",
      "At 649 epoch, Training Loss: 0.6082996763288975 \n",
      "At 649 epoch, Validation Loss: 0.7242045491933823 \n",
      "At 650 epoch, Training Loss: 0.612574813142419 \n",
      "At 650 epoch, Validation Loss: 0.7241679519414901 \n",
      "At 651 epoch, Training Loss: 0.6125915966928007 \n",
      "At 651 epoch, Validation Loss: 0.7219031929969788 \n",
      "At 652 epoch, Training Loss: 0.6085314951837065 \n",
      "At 652 epoch, Validation Loss: 0.7216861665248872 \n",
      "At 653 epoch, Training Loss: 0.6106972888112068 \n",
      "At 653 epoch, Validation Loss: 0.72138592004776 \n",
      "At 654 epoch, Training Loss: 0.6099118065088988 \n",
      "At 654 epoch, Validation Loss: 0.7231775522232056 \n",
      "At 655 epoch, Training Loss: 0.6155564155429597 \n",
      "At 655 epoch, Validation Loss: 0.7195944845676423 \n",
      "At 656 epoch, Training Loss: 0.6080782838165758 \n",
      "At 656 epoch, Validation Loss: 0.7242284595966338 \n",
      "At 657 epoch, Training Loss: 0.6078651946038005 \n",
      "At 657 epoch, Validation Loss: 0.722668194770813 \n",
      "At 658 epoch, Training Loss: 0.608651316910982 \n",
      "At 658 epoch, Validation Loss: 0.7212968230247497 \n",
      "At 659 epoch, Training Loss: 0.6092472627758978 \n",
      "At 659 epoch, Validation Loss: 0.7242720425128938 \n",
      "At 660 epoch, Training Loss: 0.6148661721497773 \n",
      "At 660 epoch, Validation Loss: 0.7247927039861678 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 661 epoch, Training Loss: 0.6086872644722466 \n",
      "At 661 epoch, Validation Loss: 0.7236738801002502 \n",
      "At 662 epoch, Training Loss: 0.6119376692920919 \n",
      "At 662 epoch, Validation Loss: 0.7212843030691146 \n",
      "At 663 epoch, Training Loss: 0.6132638674229381 \n",
      "At 663 epoch, Validation Loss: 0.7231481730937958 \n",
      "At 664 epoch, Training Loss: 0.6083779368549586 \n",
      "At 664 epoch, Validation Loss: 0.7219176560640335 \n",
      "At 665 epoch, Training Loss: 0.6039722794666882 \n",
      "At 665 epoch, Validation Loss: 0.7243023335933685 \n",
      "At 666 epoch, Training Loss: 0.6127240393310786 \n",
      "At 666 epoch, Validation Loss: 0.7218194723129272 \n",
      "At 667 epoch, Training Loss: 0.6105855319648983 \n",
      "At 667 epoch, Validation Loss: 0.7233348786830902 \n",
      "At 668 epoch, Training Loss: 0.6101315036416052 \n",
      "At 668 epoch, Validation Loss: 0.7247628688812255 \n",
      "At 669 epoch, Training Loss: 0.6125596590340142 \n",
      "At 669 epoch, Validation Loss: 0.7243145525455474 \n",
      "At 670 epoch, Training Loss: 0.612723985686898 \n",
      "At 670 epoch, Validation Loss: 0.7226919859647751 \n",
      "At 671 epoch, Training Loss: 0.6115499082952736 \n",
      "At 671 epoch, Validation Loss: 0.7222655862569811 \n",
      "At 672 epoch, Training Loss: 0.6132528372108936 \n",
      "At 672 epoch, Validation Loss: 0.7198547929525375 \n",
      "At 673 epoch, Training Loss: 0.6064684927463533 \n",
      "At 673 epoch, Validation Loss: 0.7227883756160736 \n",
      "At 674 epoch, Training Loss: 0.616276082396507 \n",
      "At 674 epoch, Validation Loss: 0.7243846029043198 \n",
      "At 675 epoch, Training Loss: 0.6087118722498414 \n",
      "At 675 epoch, Validation Loss: 0.7218512803316115 \n",
      "At 676 epoch, Training Loss: 0.6119925316423181 \n",
      "At 676 epoch, Validation Loss: 0.7237732410430907 \n",
      "At 677 epoch, Training Loss: 0.6088591583073141 \n",
      "At 677 epoch, Validation Loss: 0.7198861598968505 \n",
      "At 678 epoch, Training Loss: 0.6139175560325381 \n",
      "At 678 epoch, Validation Loss: 0.7238675117492676 \n",
      "At 679 epoch, Training Loss: 0.6148060873150826 \n",
      "At 679 epoch, Validation Loss: 0.72469781935215 \n",
      "At 680 epoch, Training Loss: 0.6120417643338437 \n",
      "At 680 epoch, Validation Loss: 0.7232989609241486 \n",
      "At 681 epoch, Training Loss: 0.6139547105878596 \n",
      "At 681 epoch, Validation Loss: 0.7212318301200865 \n",
      "At 682 epoch, Training Loss: 0.6160361662507056 \n",
      "At 682 epoch, Validation Loss: 0.722747775912285 \n",
      "At 683 epoch, Training Loss: 0.6121922183781862 \n",
      "At 683 epoch, Validation Loss: 0.7193452328443527 \n",
      "At 684 epoch, Training Loss: 0.6116810150444507 \n",
      "At 684 epoch, Validation Loss: 0.7226108312606812 \n",
      "At 685 epoch, Training Loss: 0.6045773468911645 \n",
      "At 685 epoch, Validation Loss: 0.7210639536380767 \n",
      "At 686 epoch, Training Loss: 0.6149439275264736 \n",
      "At 686 epoch, Validation Loss: 0.7226175636053085 \n",
      "At 687 epoch, Training Loss: 0.61222410723567 \n",
      "At 687 epoch, Validation Loss: 0.7253546506166458 \n",
      "At 688 epoch, Training Loss: 0.6083979442715646 \n",
      "At 688 epoch, Validation Loss: 0.7191322028636934 \n",
      "At 689 epoch, Training Loss: 0.616125060990453 \n",
      "At 689 epoch, Validation Loss: 0.7196747630834579 \n",
      "At 690 epoch, Training Loss: 0.6128015674650673 \n",
      "At 690 epoch, Validation Loss: 0.7238974779844284 \n",
      "At 691 epoch, Training Loss: 0.6124972283840178 \n",
      "At 691 epoch, Validation Loss: 0.7186478197574616 \n",
      "At 692 epoch, Training Loss: 0.6160613507032396 \n",
      "At 692 epoch, Validation Loss: 0.7210323005914688 \n",
      "At 693 epoch, Training Loss: 0.6126317217946055 \n",
      "At 693 epoch, Validation Loss: 0.7208285063505172 \n",
      "At 694 epoch, Training Loss: 0.610142529010773 \n",
      "At 694 epoch, Validation Loss: 0.721661251783371 \n",
      "At 695 epoch, Training Loss: 0.6144311681389811 \n",
      "At 695 epoch, Validation Loss: 0.7221668243408204 \n",
      "At 696 epoch, Training Loss: 0.6130851596593857 \n",
      "At 696 epoch, Validation Loss: 0.718361034989357 \n",
      "At 697 epoch, Training Loss: 0.6090232871472832 \n",
      "At 697 epoch, Validation Loss: 0.7197900950908661 \n",
      "At 698 epoch, Training Loss: 0.6091305185109376 \n",
      "At 698 epoch, Validation Loss: 0.721975350379944 \n",
      "At 699 epoch, Training Loss: 0.6078824862837795 \n",
      "At 699 epoch, Validation Loss: 0.7203640192747115 \n",
      "At 700 epoch, Training Loss: 0.6084187667816877 \n",
      "At 700 epoch, Validation Loss: 0.721665495634079 \n",
      "At 701 epoch, Training Loss: 0.6089085113257173 \n",
      "At 701 epoch, Validation Loss: 0.7247294753789901 \n",
      "At 702 epoch, Training Loss: 0.6088705748319626 \n",
      "At 702 epoch, Validation Loss: 0.72347272336483 \n",
      "At 703 epoch, Training Loss: 0.6126836761832238 \n",
      "At 703 epoch, Validation Loss: 0.7211757391691207 \n",
      "At 704 epoch, Training Loss: 0.6139625445008278 \n",
      "At 704 epoch, Validation Loss: 0.7191295832395553 \n",
      "At 705 epoch, Training Loss: 0.6100605528801679 \n",
      "At 705 epoch, Validation Loss: 0.7232050359249116 \n",
      "At 706 epoch, Training Loss: 0.6069380551576613 \n",
      "At 706 epoch, Validation Loss: 0.7250154912471771 \n",
      "At 707 epoch, Training Loss: 0.6129595389589672 \n",
      "At 707 epoch, Validation Loss: 0.7202122151851652 \n",
      "At 708 epoch, Training Loss: 0.614017276838422 \n",
      "At 708 epoch, Validation Loss: 0.719996201992035 \n",
      "At 709 epoch, Training Loss: 0.6116152752190828 \n",
      "At 709 epoch, Validation Loss: 0.7239351153373719 \n",
      "At 710 epoch, Training Loss: 0.6099121022969488 \n",
      "At 710 epoch, Validation Loss: 0.7239850401878356 \n",
      "At 711 epoch, Training Loss: 0.6101842381060123 \n",
      "At 711 epoch, Validation Loss: 0.723136979341507 \n",
      "At 712 epoch, Training Loss: 0.6089136697351932 \n",
      "At 712 epoch, Validation Loss: 0.7231126308441161 \n",
      "At 713 epoch, Training Loss: 0.6093178939074279 \n",
      "At 713 epoch, Validation Loss: 0.7230110228061677 \n",
      "At 714 epoch, Training Loss: 0.6207896970212455 \n",
      "At 714 epoch, Validation Loss: 0.7224483817815781 \n",
      "At 715 epoch, Training Loss: 0.6139781001955271 \n",
      "At 715 epoch, Validation Loss: 0.7229603201150894 \n",
      "At 716 epoch, Training Loss: 0.6105122737586501 \n",
      "At 716 epoch, Validation Loss: 0.7215918868780136 \n",
      "At 717 epoch, Training Loss: 0.6095950763672591 \n",
      "At 717 epoch, Validation Loss: 0.7246791154146194 \n",
      "At 718 epoch, Training Loss: 0.6153208773583171 \n",
      "At 718 epoch, Validation Loss: 0.7201391369104384 \n",
      "At 719 epoch, Training Loss: 0.6119964681565763 \n",
      "At 719 epoch, Validation Loss: 0.7223612308502197 \n",
      "At 720 epoch, Training Loss: 0.6090136989951134 \n",
      "At 720 epoch, Validation Loss: 0.721296975016594 \n",
      "At 721 epoch, Training Loss: 0.6110959064215421 \n",
      "At 721 epoch, Validation Loss: 0.7200158268213271 \n",
      "At 722 epoch, Training Loss: 0.609018561989069 \n",
      "At 722 epoch, Validation Loss: 0.7184198319911957 \n",
      "At 723 epoch, Training Loss: 0.6176422256976365 \n",
      "At 723 epoch, Validation Loss: 0.7230716288089752 \n",
      "At 724 epoch, Training Loss: 0.6070684045553205 \n",
      "At 724 epoch, Validation Loss: 0.7221121966838836 \n",
      "At 725 epoch, Training Loss: 0.6119417652487757 \n",
      "At 725 epoch, Validation Loss: 0.7194646924734115 \n",
      "At 726 epoch, Training Loss: 0.6068169333040717 \n",
      "At 726 epoch, Validation Loss: 0.7213413804769517 \n",
      "At 727 epoch, Training Loss: 0.6137221582233907 \n",
      "At 727 epoch, Validation Loss: 0.7241223454475404 \n",
      "At 728 epoch, Training Loss: 0.6076383931562301 \n",
      "At 728 epoch, Validation Loss: 0.7210542589426042 \n",
      "At 729 epoch, Training Loss: 0.6064216986298564 \n",
      "At 729 epoch, Validation Loss: 0.7191637963056563 \n",
      "At 730 epoch, Training Loss: 0.6136140950024132 \n",
      "At 730 epoch, Validation Loss: 0.7237744837999344 \n",
      "At 731 epoch, Training Loss: 0.6091888792812826 \n",
      "At 731 epoch, Validation Loss: 0.7196534693241119 \n",
      "At 732 epoch, Training Loss: 0.6104918770492079 \n",
      "At 732 epoch, Validation Loss: 0.7214749038219451 \n",
      "At 733 epoch, Training Loss: 0.6102436427026989 \n",
      "At 733 epoch, Validation Loss: 0.7227531969547272 \n",
      "At 734 epoch, Training Loss: 0.6145036928355693 \n",
      "At 734 epoch, Validation Loss: 0.7200679838657379 \n",
      "At 735 epoch, Training Loss: 0.6123455401509996 \n",
      "At 735 epoch, Validation Loss: 0.721270728111267 \n",
      "At 736 epoch, Training Loss: 0.6093620523810388 \n",
      "At 736 epoch, Validation Loss: 0.7207301735877989 \n",
      "At 737 epoch, Training Loss: 0.6108706839382649 \n",
      "At 737 epoch, Validation Loss: 0.7204094231128694 \n",
      "At 738 epoch, Training Loss: 0.6084994994103906 \n",
      "At 738 epoch, Validation Loss: 0.7262117743492128 \n",
      "At 739 epoch, Training Loss: 0.6086807109415532 \n",
      "At 739 epoch, Validation Loss: 0.720560923218727 \n",
      "At 740 epoch, Training Loss: 0.6134409192949534 \n",
      "At 740 epoch, Validation Loss: 0.7249033570289612 \n",
      "At 741 epoch, Training Loss: 0.6108943540602921 \n",
      "At 741 epoch, Validation Loss: 0.7186295509338378 \n",
      "At 742 epoch, Training Loss: 0.6102954912930728 \n",
      "At 742 epoch, Validation Loss: 0.7232710480690001 \n",
      "At 743 epoch, Training Loss: 0.6125554725527762 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 743 epoch, Validation Loss: 0.7201574206352235 \n",
      "At 744 epoch, Training Loss: 0.6088167082518341 \n",
      "At 744 epoch, Validation Loss: 0.7244009524583818 \n",
      "At 745 epoch, Training Loss: 0.6160840682685376 \n",
      "At 745 epoch, Validation Loss: 0.7213070273399353 \n",
      "At 746 epoch, Training Loss: 0.609142495691776 \n",
      "At 746 epoch, Validation Loss: 0.7200026899576188 \n",
      "At 747 epoch, Training Loss: 0.6095338352024559 \n",
      "At 747 epoch, Validation Loss: 0.7248051702976226 \n",
      "At 748 epoch, Training Loss: 0.6076769143342973 \n",
      "At 748 epoch, Validation Loss: 0.7228044927120209 \n",
      "At 749 epoch, Training Loss: 0.6085852704942222 \n",
      "At 749 epoch, Validation Loss: 0.7229914247989655 \n",
      "At 750 epoch, Training Loss: 0.6189129866659646 \n",
      "At 750 epoch, Validation Loss: 0.7236293762922287 \n",
      "At 751 epoch, Training Loss: 0.6058870840817693 \n",
      "At 751 epoch, Validation Loss: 0.7214290708303452 \n",
      "At 752 epoch, Training Loss: 0.6118309844285252 \n",
      "At 752 epoch, Validation Loss: 0.7210446298122407 \n",
      "At 753 epoch, Training Loss: 0.6154495365917684 \n",
      "At 753 epoch, Validation Loss: 0.7194166541099549 \n",
      "At 754 epoch, Training Loss: 0.6140389636158943 \n",
      "At 754 epoch, Validation Loss: 0.7189677774906159 \n",
      "At 755 epoch, Training Loss: 0.6096965931355951 \n",
      "At 755 epoch, Validation Loss: 0.7211181968450545 \n",
      "At 756 epoch, Training Loss: 0.610227020457387 \n",
      "At 756 epoch, Validation Loss: 0.7176031976938249 \n",
      "At 757 epoch, Training Loss: 0.6080688815563919 \n",
      "At 757 epoch, Validation Loss: 0.7234546422958376 \n",
      "At 758 epoch, Training Loss: 0.6157317850738763 \n",
      "At 758 epoch, Validation Loss: 0.7223622500896454 \n",
      "At 759 epoch, Training Loss: 0.6141293212771413 \n",
      "At 759 epoch, Validation Loss: 0.7192444145679474 \n",
      "At 760 epoch, Training Loss: 0.6149498522281646 \n",
      "At 760 epoch, Validation Loss: 0.7246116429567336 \n",
      "At 761 epoch, Training Loss: 0.610842876136303 \n",
      "At 761 epoch, Validation Loss: 0.7212506949901581 \n",
      "At 762 epoch, Training Loss: 0.6072823729366064 \n",
      "At 762 epoch, Validation Loss: 0.7209848940372466 \n",
      "At 763 epoch, Training Loss: 0.6123650848865505 \n",
      "At 763 epoch, Validation Loss: 0.7234739929437637 \n",
      "At 764 epoch, Training Loss: 0.6122118573635814 \n",
      "At 764 epoch, Validation Loss: 0.7209681868553162 \n",
      "At 765 epoch, Training Loss: 0.6074638452380892 \n",
      "At 765 epoch, Validation Loss: 0.7183351993560791 \n",
      "At 766 epoch, Training Loss: 0.6142809759825464 \n",
      "At 766 epoch, Validation Loss: 0.7241916388273238 \n",
      "At 767 epoch, Training Loss: 0.6103837616741661 \n",
      "At 767 epoch, Validation Loss: 0.7231278777122496 \n",
      "At 768 epoch, Training Loss: 0.6133408669382335 \n",
      "At 768 epoch, Validation Loss: 0.7222928375005722 \n",
      "At 769 epoch, Training Loss: 0.6175384938716889 \n",
      "At 769 epoch, Validation Loss: 0.723387384414673 \n",
      "At 770 epoch, Training Loss: 0.6092600505799053 \n",
      "At 770 epoch, Validation Loss: 0.7217634916305542 \n",
      "At 771 epoch, Training Loss: 0.613223882392049 \n",
      "At 771 epoch, Validation Loss: 0.7218080282211304 \n",
      "At 772 epoch, Training Loss: 0.607290263473988 \n",
      "At 772 epoch, Validation Loss: 0.7216410964727401 \n",
      "At 773 epoch, Training Loss: 0.6070118796080353 \n",
      "At 773 epoch, Validation Loss: 0.7217975229024887 \n",
      "At 774 epoch, Training Loss: 0.6069935631006955 \n",
      "At 774 epoch, Validation Loss: 0.7247221797704695 \n",
      "At 775 epoch, Training Loss: 0.6124033443629741 \n",
      "At 775 epoch, Validation Loss: 0.7199861198663712 \n",
      "At 776 epoch, Training Loss: 0.6144936248660086 \n",
      "At 776 epoch, Validation Loss: 0.7223183184862138 \n",
      "At 777 epoch, Training Loss: 0.609962061047554 \n",
      "At 777 epoch, Validation Loss: 0.7232894957065582 \n",
      "At 778 epoch, Training Loss: 0.6142796207219362 \n",
      "At 778 epoch, Validation Loss: 0.7221211254596711 \n",
      "At 779 epoch, Training Loss: 0.6186167981475593 \n",
      "At 779 epoch, Validation Loss: 0.7220747143030167 \n",
      "At 780 epoch, Training Loss: 0.6087800942361358 \n",
      "At 780 epoch, Validation Loss: 0.721812292933464 \n",
      "At 781 epoch, Training Loss: 0.6103532999753953 \n",
      "At 781 epoch, Validation Loss: 0.7220758140087129 \n",
      "At 782 epoch, Training Loss: 0.6033586382865906 \n",
      "At 782 epoch, Validation Loss: 0.7203156381845474 \n",
      "At 783 epoch, Training Loss: 0.612111807987094 \n",
      "At 783 epoch, Validation Loss: 0.7228800594806671 \n",
      "At 784 epoch, Training Loss: 0.6030168037861589 \n",
      "At 784 epoch, Validation Loss: 0.7256593972444534 \n",
      "At 785 epoch, Training Loss: 0.6092321217060087 \n",
      "At 785 epoch, Validation Loss: 0.7189014524221421 \n",
      "At 786 epoch, Training Loss: 0.6107970848679538 \n",
      "At 786 epoch, Validation Loss: 0.7193772405385972 \n",
      "At 787 epoch, Training Loss: 0.611165590211749 \n",
      "At 787 epoch, Validation Loss: 0.7201073348522188 \n",
      "At 788 epoch, Training Loss: 0.6098131181672215 \n",
      "At 788 epoch, Validation Loss: 0.7216130971908569 \n",
      "At 789 epoch, Training Loss: 0.6086216147989032 \n",
      "At 789 epoch, Validation Loss: 0.7221214443445205 \n",
      "At 790 epoch, Training Loss: 0.6167370785027745 \n",
      "At 790 epoch, Validation Loss: 0.7241171002388 \n",
      "At 791 epoch, Training Loss: 0.6059868410229686 \n",
      "At 791 epoch, Validation Loss: 0.7225226998329163 \n",
      "At 792 epoch, Training Loss: 0.6064007781445981 \n",
      "At 792 epoch, Validation Loss: 0.7232836812734603 \n",
      "At 793 epoch, Training Loss: 0.6079779136925938 \n",
      "At 793 epoch, Validation Loss: 0.7212919890880584 \n",
      "At 794 epoch, Training Loss: 0.6102400094270705 \n",
      "At 794 epoch, Validation Loss: 0.7224173635244368 \n",
      "At 795 epoch, Training Loss: 0.6076731199398635 \n",
      "At 795 epoch, Validation Loss: 0.723363995552063 \n",
      "At 796 epoch, Training Loss: 0.6117007749155164 \n",
      "At 796 epoch, Validation Loss: 0.7245634526014327 \n",
      "At 797 epoch, Training Loss: 0.6113941717892887 \n",
      "At 797 epoch, Validation Loss: 0.7199509173631669 \n",
      "At 798 epoch, Training Loss: 0.6092023838311432 \n",
      "At 798 epoch, Validation Loss: 0.725071793794632 \n",
      "At 799 epoch, Training Loss: 0.6109059948474171 \n",
      "At 799 epoch, Validation Loss: 0.723287546634674 \n",
      "At 800 epoch, Training Loss: 0.6073115386068819 \n",
      "At 800 epoch, Validation Loss: 0.7244650840759278 \n",
      "At 801 epoch, Training Loss: 0.6129166599363092 \n",
      "At 801 epoch, Validation Loss: 0.7211544334888457 \n",
      "At 802 epoch, Training Loss: 0.6061993230134249 \n",
      "At 802 epoch, Validation Loss: 0.723309224843979 \n",
      "At 803 epoch, Training Loss: 0.6125308472663163 \n",
      "At 803 epoch, Validation Loss: 0.7231594622135162 \n",
      "At 804 epoch, Training Loss: 0.6109393950551747 \n",
      "At 804 epoch, Validation Loss: 0.7216390907764435 \n",
      "At 805 epoch, Training Loss: 0.6121001571416851 \n",
      "At 805 epoch, Validation Loss: 0.7211073338985441 \n",
      "At 806 epoch, Training Loss: 0.6117193274199965 \n",
      "At 806 epoch, Validation Loss: 0.7262766718864442 \n",
      "At 807 epoch, Training Loss: 0.6102262809872628 \n",
      "At 807 epoch, Validation Loss: 0.7222069054841996 \n",
      "At 808 epoch, Training Loss: 0.6133245626464487 \n",
      "At 808 epoch, Validation Loss: 0.724680632352829 \n",
      "At 809 epoch, Training Loss: 0.6158482871949672 \n",
      "At 809 epoch, Validation Loss: 0.7221275031566621 \n",
      "At 810 epoch, Training Loss: 0.6051281638443466 \n",
      "At 810 epoch, Validation Loss: 0.7185164242982865 \n",
      "At 811 epoch, Training Loss: 0.6062591753900053 \n",
      "At 811 epoch, Validation Loss: 0.7203569024801254 \n",
      "At 812 epoch, Training Loss: 0.6127418924123049 \n",
      "At 812 epoch, Validation Loss: 0.7214399039745331 \n",
      "At 813 epoch, Training Loss: 0.6163085456937548 \n",
      "At 813 epoch, Validation Loss: 0.7207749426364899 \n",
      "At 814 epoch, Training Loss: 0.6114651303738354 \n",
      "At 814 epoch, Validation Loss: 0.7229320049285889 \n",
      "At 815 epoch, Training Loss: 0.6128138095140458 \n",
      "At 815 epoch, Validation Loss: 0.7194166094064713 \n",
      "At 816 epoch, Training Loss: 0.6099302243441344 \n",
      "At 816 epoch, Validation Loss: 0.7253732144832611 \n",
      "At 817 epoch, Training Loss: 0.6089091479778287 \n",
      "At 817 epoch, Validation Loss: 0.7193335682153703 \n",
      "At 818 epoch, Training Loss: 0.6147904984653001 \n",
      "At 818 epoch, Validation Loss: 0.7259223133325579 \n",
      "At 819 epoch, Training Loss: 0.6105545066297056 \n",
      "At 819 epoch, Validation Loss: 0.7202134251594545 \n",
      "At 820 epoch, Training Loss: 0.6119063574820759 \n",
      "At 820 epoch, Validation Loss: 0.7213114261627197 \n",
      "At 821 epoch, Training Loss: 0.603628531470895 \n",
      "At 821 epoch, Validation Loss: 0.7216234475374222 \n",
      "At 822 epoch, Training Loss: 0.6084318369627 \n",
      "At 822 epoch, Validation Loss: 0.723224428296089 \n",
      "At 823 epoch, Training Loss: 0.6204978469759226 \n",
      "At 823 epoch, Validation Loss: 0.7212272107601165 \n",
      "At 824 epoch, Training Loss: 0.6153467264026399 \n",
      "At 824 epoch, Validation Loss: 0.7207981139421462 \n",
      "At 825 epoch, Training Loss: 0.6129993513226513 \n",
      "At 825 epoch, Validation Loss: 0.7216447353363038 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 826 epoch, Training Loss: 0.6056242920458318 \n",
      "At 826 epoch, Validation Loss: 0.7243220448493957 \n",
      "At 827 epoch, Training Loss: 0.6095398664474485 \n",
      "At 827 epoch, Validation Loss: 0.7186287432909012 \n",
      "At 828 epoch, Training Loss: 0.6135884296149016 \n",
      "At 828 epoch, Validation Loss: 0.721951201558113 \n",
      "At 829 epoch, Training Loss: 0.6131197810173037 \n",
      "At 829 epoch, Validation Loss: 0.7217304915189742 \n",
      "At 830 epoch, Training Loss: 0.6086288940161466 \n",
      "At 830 epoch, Validation Loss: 0.7208458334207535 \n",
      "At 831 epoch, Training Loss: 0.6132332403212785 \n",
      "At 831 epoch, Validation Loss: 0.7203055113554002 \n",
      "At 832 epoch, Training Loss: 0.6111345101147891 \n",
      "At 832 epoch, Validation Loss: 0.721568602323532 \n",
      "At 833 epoch, Training Loss: 0.6105530437082051 \n",
      "At 833 epoch, Validation Loss: 0.7219147980213165 \n",
      "At 834 epoch, Training Loss: 0.6082577053457501 \n",
      "At 834 epoch, Validation Loss: 0.7197629183530808 \n",
      "At 835 epoch, Training Loss: 0.6130827873945234 \n",
      "At 835 epoch, Validation Loss: 0.7179722934961318 \n",
      "At 836 epoch, Training Loss: 0.6078243106603622 \n",
      "At 836 epoch, Validation Loss: 0.7247474670410157 \n",
      "At 837 epoch, Training Loss: 0.6104159828275443 \n",
      "At 837 epoch, Validation Loss: 0.7220138549804688 \n",
      "At 838 epoch, Training Loss: 0.6148604124784472 \n",
      "At 838 epoch, Validation Loss: 0.7225339114665985 \n",
      "At 839 epoch, Training Loss: 0.6100292390212414 \n",
      "At 839 epoch, Validation Loss: 0.7250227630138397 \n",
      "At 840 epoch, Training Loss: 0.6146994858980176 \n",
      "At 840 epoch, Validation Loss: 0.7257709115743637 \n",
      "At 841 epoch, Training Loss: 0.6078581474721436 \n",
      "At 841 epoch, Validation Loss: 0.7237545460462571 \n",
      "At 842 epoch, Training Loss: 0.6106325209140776 \n",
      "At 842 epoch, Validation Loss: 0.7268688589334488 \n",
      "At 843 epoch, Training Loss: 0.6167761724442239 \n",
      "At 843 epoch, Validation Loss: 0.7232411086559295 \n",
      "At 844 epoch, Training Loss: 0.609161545336247 \n",
      "At 844 epoch, Validation Loss: 0.7226907968521118 \n",
      "At 845 epoch, Training Loss: 0.6069097563624387 \n",
      "At 845 epoch, Validation Loss: 0.7192053705453874 \n",
      "At 846 epoch, Training Loss: 0.6118202250450849 \n",
      "At 846 epoch, Validation Loss: 0.7249616175889969 \n",
      "At 847 epoch, Training Loss: 0.6078848965466019 \n",
      "At 847 epoch, Validation Loss: 0.7205867886543273 \n",
      "At 848 epoch, Training Loss: 0.6049815744161605 \n",
      "At 848 epoch, Validation Loss: 0.7248029500246049 \n",
      "At 849 epoch, Training Loss: 0.6111563380807635 \n",
      "At 849 epoch, Validation Loss: 0.7252908676862716 \n",
      "At 850 epoch, Training Loss: 0.6149802699685092 \n",
      "At 850 epoch, Validation Loss: 0.7234803795814515 \n",
      "At 851 epoch, Training Loss: 0.6127144597470756 \n",
      "At 851 epoch, Validation Loss: 0.7234320014715194 \n",
      "At 852 epoch, Training Loss: 0.6100332945585248 \n",
      "At 852 epoch, Validation Loss: 0.7217414528131486 \n",
      "At 853 epoch, Training Loss: 0.6114403598010535 \n",
      "At 853 epoch, Validation Loss: 0.7221761047840118 \n",
      "At 854 epoch, Training Loss: 0.6139541871845725 \n",
      "At 854 epoch, Validation Loss: 0.7250646442174912 \n",
      "At 855 epoch, Training Loss: 0.6118144499137999 \n",
      "At 855 epoch, Validation Loss: 0.7204889804124832 \n",
      "At 856 epoch, Training Loss: 0.6114687442779543 \n",
      "At 856 epoch, Validation Loss: 0.7261281311511992 \n",
      "At 857 epoch, Training Loss: 0.605245894193649 \n",
      "At 857 epoch, Validation Loss: 0.7216152012348176 \n",
      "At 858 epoch, Training Loss: 0.614160353317857 \n",
      "At 858 epoch, Validation Loss: 0.7255116790533066 \n",
      "At 859 epoch, Training Loss: 0.6154219172894954 \n",
      "At 859 epoch, Validation Loss: 0.7235441684722901 \n",
      "At 860 epoch, Training Loss: 0.6060139868408438 \n",
      "At 860 epoch, Validation Loss: 0.7230011343955994 \n",
      "At 861 epoch, Training Loss: 0.6135921407490973 \n",
      "At 861 epoch, Validation Loss: 0.725094211101532 \n",
      "At 862 epoch, Training Loss: 0.6082239411771294 \n",
      "At 862 epoch, Validation Loss: 0.7239721953868866 \n",
      "At 863 epoch, Training Loss: 0.6086086917668581 \n",
      "At 863 epoch, Validation Loss: 0.7236621618270873 \n",
      "At 864 epoch, Training Loss: 0.6180506184697149 \n",
      "At 864 epoch, Validation Loss: 0.7258426487445832 \n",
      "At 865 epoch, Training Loss: 0.611071866005659 \n",
      "At 865 epoch, Validation Loss: 0.7189392268657685 \n",
      "At 866 epoch, Training Loss: 0.6063453994691368 \n",
      "At 866 epoch, Validation Loss: 0.7245197743177415 \n",
      "At 867 epoch, Training Loss: 0.6093667801469564 \n",
      "At 867 epoch, Validation Loss: 0.7220280051231384 \n",
      "At 868 epoch, Training Loss: 0.6081564854830501 \n",
      "At 868 epoch, Validation Loss: 0.7224404811859131 \n",
      "At 869 epoch, Training Loss: 0.6148755654692653 \n",
      "At 869 epoch, Validation Loss: 0.7269735872745513 \n",
      "At 870 epoch, Training Loss: 0.6140919532626865 \n",
      "At 870 epoch, Validation Loss: 0.721977111697197 \n",
      "At 871 epoch, Training Loss: 0.6129075683653353 \n",
      "At 871 epoch, Validation Loss: 0.7222269475460052 \n",
      "At 872 epoch, Training Loss: 0.6128073927015064 \n",
      "At 872 epoch, Validation Loss: 0.7188040018081665 \n",
      "At 873 epoch, Training Loss: 0.6117703247815371 \n",
      "At 873 epoch, Validation Loss: 0.7204631090164184 \n",
      "At 874 epoch, Training Loss: 0.6123067840933806 \n",
      "At 874 epoch, Validation Loss: 0.7184571862220764 \n",
      "At 875 epoch, Training Loss: 0.6092583235353232 \n",
      "At 875 epoch, Validation Loss: 0.7233932554721832 \n",
      "At 876 epoch, Training Loss: 0.6068074237555264 \n",
      "At 876 epoch, Validation Loss: 0.7219319850206376 \n",
      "At 877 epoch, Training Loss: 0.6107221759855749 \n",
      "At 877 epoch, Validation Loss: 0.7243383347988127 \n",
      "At 878 epoch, Training Loss: 0.606462476029992 \n",
      "At 878 epoch, Validation Loss: 0.7200054347515107 \n",
      "At 879 epoch, Training Loss: 0.6022998731583358 \n",
      "At 879 epoch, Validation Loss: 0.7222456008195877 \n",
      "At 880 epoch, Training Loss: 0.6081570647656916 \n",
      "At 880 epoch, Validation Loss: 0.7224666118621824 \n",
      "At 881 epoch, Training Loss: 0.6093300089240072 \n",
      "At 881 epoch, Validation Loss: 0.720884895324707 \n",
      "At 882 epoch, Training Loss: 0.6143931202590466 \n",
      "At 882 epoch, Validation Loss: 0.7220153748989103 \n",
      "At 883 epoch, Training Loss: 0.6048273567110299 \n",
      "At 883 epoch, Validation Loss: 0.7213211297988892 \n",
      "At 884 epoch, Training Loss: 0.6080000847578049 \n",
      "At 884 epoch, Validation Loss: 0.7218299448490143 \n",
      "At 885 epoch, Training Loss: 0.605022452399135 \n",
      "At 885 epoch, Validation Loss: 0.7197470724582673 \n",
      "At 886 epoch, Training Loss: 0.6115670256316661 \n",
      "At 886 epoch, Validation Loss: 0.7220554679632187 \n",
      "At 887 epoch, Training Loss: 0.6118123203516007 \n",
      "At 887 epoch, Validation Loss: 0.7226832538843154 \n",
      "At 888 epoch, Training Loss: 0.6135148767381906 \n",
      "At 888 epoch, Validation Loss: 0.720246994495392 \n",
      "At 889 epoch, Training Loss: 0.6137032967060799 \n",
      "At 889 epoch, Validation Loss: 0.7222386121749877 \n",
      "At 890 epoch, Training Loss: 0.6095326568931341 \n",
      "At 890 epoch, Validation Loss: 0.7245542138814927 \n",
      "At 891 epoch, Training Loss: 0.608633952960372 \n",
      "At 891 epoch, Validation Loss: 0.7210382252931595 \n",
      "At 892 epoch, Training Loss: 0.6124057814478873 \n",
      "At 892 epoch, Validation Loss: 0.7239098608493807 \n",
      "At 893 epoch, Training Loss: 0.6154784444719551 \n",
      "At 893 epoch, Validation Loss: 0.7203307211399078 \n",
      "At 894 epoch, Training Loss: 0.60511478446424 \n",
      "At 894 epoch, Validation Loss: 0.7196846157312393 \n",
      "At 895 epoch, Training Loss: 0.6108016140758994 \n",
      "At 895 epoch, Validation Loss: 0.7226952254772187 \n",
      "At 896 epoch, Training Loss: 0.6102592609822753 \n",
      "At 896 epoch, Validation Loss: 0.7227780878543855 \n",
      "At 897 epoch, Training Loss: 0.6101265851408244 \n",
      "At 897 epoch, Validation Loss: 0.7287186264991761 \n",
      "At 898 epoch, Training Loss: 0.6144724149256943 \n",
      "At 898 epoch, Validation Loss: 0.7216399937868118 \n",
      "At 899 epoch, Training Loss: 0.6068400420248508 \n",
      "At 899 epoch, Validation Loss: 0.7242945313453674 \n",
      "At 900 epoch, Training Loss: 0.6092820990830662 \n",
      "At 900 epoch, Validation Loss: 0.7240829259157181 \n",
      "At 901 epoch, Training Loss: 0.6139528073370457 \n",
      "At 901 epoch, Validation Loss: 0.7221844136714934 \n",
      "At 902 epoch, Training Loss: 0.6117714129388331 \n",
      "At 902 epoch, Validation Loss: 0.7243427962064743 \n",
      "At 903 epoch, Training Loss: 0.6072053465992214 \n",
      "At 903 epoch, Validation Loss: 0.7230154365301132 \n",
      "At 904 epoch, Training Loss: 0.6116042096167804 \n",
      "At 904 epoch, Validation Loss: 0.7254995226860046 \n",
      "At 905 epoch, Training Loss: 0.6129158832132817 \n",
      "At 905 epoch, Validation Loss: 0.7227634489536285 \n",
      "At 906 epoch, Training Loss: 0.615427875891328 \n",
      "At 906 epoch, Validation Loss: 0.7263975232839585 \n",
      "At 907 epoch, Training Loss: 0.6149203911423684 \n",
      "At 907 epoch, Validation Loss: 0.7222766727209092 \n",
      "At 908 epoch, Training Loss: 0.6123849557712673 \n",
      "At 908 epoch, Validation Loss: 0.7215319663286209 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 909 epoch, Training Loss: 0.6129948403686288 \n",
      "At 909 epoch, Validation Loss: 0.7220064133405687 \n",
      "At 910 epoch, Training Loss: 0.6073191203176975 \n",
      "At 910 epoch, Validation Loss: 0.7246246784925461 \n",
      "At 911 epoch, Training Loss: 0.6097664922475815 \n",
      "At 911 epoch, Validation Loss: 0.7221743464469909 \n",
      "At 912 epoch, Training Loss: 0.612285327911377 \n",
      "At 912 epoch, Validation Loss: 0.7261979907751083 \n",
      "At 913 epoch, Training Loss: 0.6123335439711811 \n",
      "At 913 epoch, Validation Loss: 0.723003363609314 \n",
      "At 914 epoch, Training Loss: 0.611158415675163 \n",
      "At 914 epoch, Validation Loss: 0.7226858109235763 \n",
      "At 915 epoch, Training Loss: 0.6198913637548683 \n",
      "At 915 epoch, Validation Loss: 0.7263158649206162 \n",
      "At 916 epoch, Training Loss: 0.6106904614716768 \n",
      "At 916 epoch, Validation Loss: 0.7239669561386108 \n",
      "At 917 epoch, Training Loss: 0.6032572519034145 \n",
      "At 917 epoch, Validation Loss: 0.7215598315000533 \n",
      "At 918 epoch, Training Loss: 0.6092650126665827 \n",
      "At 918 epoch, Validation Loss: 0.7247405976057053 \n",
      "At 919 epoch, Training Loss: 0.6103569820523262 \n",
      "At 919 epoch, Validation Loss: 0.7236317634582519 \n",
      "At 920 epoch, Training Loss: 0.6072058044373987 \n",
      "At 920 epoch, Validation Loss: 0.7227305084466934 \n",
      "At 921 epoch, Training Loss: 0.6106413848698142 \n",
      "At 921 epoch, Validation Loss: 0.7242599815130235 \n",
      "At 922 epoch, Training Loss: 0.6143555872142312 \n",
      "At 922 epoch, Validation Loss: 0.7224181830883025 \n",
      "At 923 epoch, Training Loss: 0.6114435832947493 \n",
      "At 923 epoch, Validation Loss: 0.7238497465848922 \n",
      "At 924 epoch, Training Loss: 0.6091125689446925 \n",
      "At 924 epoch, Validation Loss: 0.7221617996692656 \n",
      "At 925 epoch, Training Loss: 0.61557006649673 \n",
      "At 925 epoch, Validation Loss: 0.7239678978919984 \n",
      "At 926 epoch, Training Loss: 0.6131087977439162 \n",
      "At 926 epoch, Validation Loss: 0.7213821262121201 \n",
      "At 927 epoch, Training Loss: 0.6118841068819166 \n",
      "At 927 epoch, Validation Loss: 0.7265474528074265 \n",
      "At 928 epoch, Training Loss: 0.6114565264433619 \n",
      "At 928 epoch, Validation Loss: 0.7226107031106949 \n",
      "At 929 epoch, Training Loss: 0.6120784543454649 \n",
      "At 929 epoch, Validation Loss: 0.723442381620407 \n",
      "At 930 epoch, Training Loss: 0.6067289240658283 \n",
      "At 930 epoch, Validation Loss: 0.7195660948753357 \n",
      "At 931 epoch, Training Loss: 0.6063075564801697 \n",
      "At 931 epoch, Validation Loss: 0.7248223364353179 \n",
      "At 932 epoch, Training Loss: 0.6074811279773707 \n",
      "At 932 epoch, Validation Loss: 0.7223436266183854 \n",
      "At 933 epoch, Training Loss: 0.6114001195877792 \n",
      "At 933 epoch, Validation Loss: 0.7162496894598007 \n",
      "At 934 epoch, Training Loss: 0.6086271349340675 \n",
      "At 934 epoch, Validation Loss: 0.7194947332143785 \n",
      "At 935 epoch, Training Loss: 0.611940445750952 \n",
      "At 935 epoch, Validation Loss: 0.7211779117584229 \n",
      "At 936 epoch, Training Loss: 0.606878300011158 \n",
      "At 936 epoch, Validation Loss: 0.7237148642539979 \n",
      "At 937 epoch, Training Loss: 0.6120515685528521 \n",
      "At 937 epoch, Validation Loss: 0.7226827532052994 \n",
      "At 938 epoch, Training Loss: 0.6051010347902774 \n",
      "At 938 epoch, Validation Loss: 0.7219850063323976 \n",
      "At 939 epoch, Training Loss: 0.6114506173878913 \n",
      "At 939 epoch, Validation Loss: 0.7226258337497712 \n",
      "At 940 epoch, Training Loss: 0.6060173328965903 \n",
      "At 940 epoch, Validation Loss: 0.7230632603168489 \n",
      "At 941 epoch, Training Loss: 0.6117877166718247 \n",
      "At 941 epoch, Validation Loss: 0.7253237307071686 \n",
      "At 942 epoch, Training Loss: 0.609140120819211 \n",
      "At 942 epoch, Validation Loss: 0.723239979147911 \n",
      "At 943 epoch, Training Loss: 0.6105093408375976 \n",
      "At 943 epoch, Validation Loss: 0.7235898673534394 \n",
      "At 944 epoch, Training Loss: 0.6110511612147093 \n",
      "At 944 epoch, Validation Loss: 0.7251623511314393 \n",
      "At 945 epoch, Training Loss: 0.6118753233924509 \n",
      "At 945 epoch, Validation Loss: 0.7208151578903198 \n",
      "At 946 epoch, Training Loss: 0.6049165531992915 \n",
      "At 946 epoch, Validation Loss: 0.7245768278837204 \n",
      "At 947 epoch, Training Loss: 0.6117636278271674 \n",
      "At 947 epoch, Validation Loss: 0.7213257938623429 \n",
      "At 948 epoch, Training Loss: 0.6108607869595286 \n",
      "At 948 epoch, Validation Loss: 0.7244870334863663 \n",
      "At 949 epoch, Training Loss: 0.6104746688157323 \n",
      "At 949 epoch, Validation Loss: 0.7226981490850448 \n",
      "At 950 epoch, Training Loss: 0.6039674010127785 \n",
      "At 950 epoch, Validation Loss: 0.7205884456634521 \n",
      "At 951 epoch, Training Loss: 0.6110660925507548 \n",
      "At 951 epoch, Validation Loss: 0.7258145689964296 \n",
      "At 952 epoch, Training Loss: 0.6123027343302967 \n",
      "At 952 epoch, Validation Loss: 0.7231718301773072 \n",
      "At 953 epoch, Training Loss: 0.6130697607994081 \n",
      "At 953 epoch, Validation Loss: 0.721672374010086 \n",
      "At 954 epoch, Training Loss: 0.6118854660540816 \n",
      "At 954 epoch, Validation Loss: 0.7191025912761688 \n",
      "At 955 epoch, Training Loss: 0.6054499074816708 \n",
      "At 955 epoch, Validation Loss: 0.7201195895671845 \n",
      "At 956 epoch, Training Loss: 0.6086875285953284 \n",
      "At 956 epoch, Validation Loss: 0.7238007158041 \n",
      "At 957 epoch, Training Loss: 0.6146383605897427 \n",
      "At 957 epoch, Validation Loss: 0.7245820701122283 \n",
      "At 958 epoch, Training Loss: 0.6110988080501558 \n",
      "At 958 epoch, Validation Loss: 0.7252033442258835 \n",
      "At 959 epoch, Training Loss: 0.6161930836737161 \n",
      "At 959 epoch, Validation Loss: 0.7237317770719528 \n",
      "At 960 epoch, Training Loss: 0.6122470151633023 \n",
      "At 960 epoch, Validation Loss: 0.7242134779691696 \n",
      "At 961 epoch, Training Loss: 0.6098279584199188 \n",
      "At 961 epoch, Validation Loss: 0.7194852560758589 \n",
      "At 962 epoch, Training Loss: 0.6129242941737177 \n",
      "At 962 epoch, Validation Loss: 0.7240346252918244 \n",
      "At 963 epoch, Training Loss: 0.6062278967350722 \n",
      "At 963 epoch, Validation Loss: 0.7244756817817689 \n",
      "At 964 epoch, Training Loss: 0.6030699029564857 \n",
      "At 964 epoch, Validation Loss: 0.7222960323095321 \n",
      "At 965 epoch, Training Loss: 0.616871461644769 \n",
      "At 965 epoch, Validation Loss: 0.7235741466283798 \n",
      "At 966 epoch, Training Loss: 0.6079217374324802 \n",
      "At 966 epoch, Validation Loss: 0.7237213730812073 \n",
      "At 967 epoch, Training Loss: 0.6167682476341726 \n",
      "At 967 epoch, Validation Loss: 0.7198302954435349 \n",
      "At 968 epoch, Training Loss: 0.6114661458879708 \n",
      "At 968 epoch, Validation Loss: 0.7229764372110368 \n",
      "At 969 epoch, Training Loss: 0.6084078740328552 \n",
      "At 969 epoch, Validation Loss: 0.7217051774263383 \n",
      "At 970 epoch, Training Loss: 0.6120896741747857 \n",
      "At 970 epoch, Validation Loss: 0.7223719865083694 \n",
      "At 971 epoch, Training Loss: 0.6153328962624072 \n",
      "At 971 epoch, Validation Loss: 0.7232911378145218 \n",
      "At 972 epoch, Training Loss: 0.6092868868261576 \n",
      "At 972 epoch, Validation Loss: 0.7226206690073013 \n",
      "At 973 epoch, Training Loss: 0.6166545525193216 \n",
      "At 973 epoch, Validation Loss: 0.7198756486177444 \n",
      "At 974 epoch, Training Loss: 0.6108031578361989 \n",
      "At 974 epoch, Validation Loss: 0.7237970113754273 \n",
      "At 975 epoch, Training Loss: 0.6116582533344622 \n",
      "At 975 epoch, Validation Loss: 0.7192245781421662 \n",
      "At 976 epoch, Training Loss: 0.6132680997252464 \n",
      "At 976 epoch, Validation Loss: 0.7230538606643676 \n",
      "At 977 epoch, Training Loss: 0.6121314335614444 \n",
      "At 977 epoch, Validation Loss: 0.7203524112701417 \n",
      "At 978 epoch, Training Loss: 0.608325294777751 \n",
      "At 978 epoch, Validation Loss: 0.7215338498353957 \n",
      "At 979 epoch, Training Loss: 0.6081054110080005 \n",
      "At 979 epoch, Validation Loss: 0.726445072889328 \n",
      "At 980 epoch, Training Loss: 0.6084044292569163 \n",
      "At 980 epoch, Validation Loss: 0.7200351685285569 \n",
      "At 981 epoch, Training Loss: 0.613644089922309 \n",
      "At 981 epoch, Validation Loss: 0.7251843035221099 \n",
      "At 982 epoch, Training Loss: 0.6091340653598308 \n",
      "At 982 epoch, Validation Loss: 0.7192230820655822 \n",
      "At 983 epoch, Training Loss: 0.6072603832930329 \n",
      "At 983 epoch, Validation Loss: 0.7208805561065672 \n",
      "At 984 epoch, Training Loss: 0.6133390285074708 \n",
      "At 984 epoch, Validation Loss: 0.7199917942285537 \n",
      "At 985 epoch, Training Loss: 0.6073649302124976 \n",
      "At 985 epoch, Validation Loss: 0.723171877861023 \n",
      "At 986 epoch, Training Loss: 0.6099652126431463 \n",
      "At 986 epoch, Validation Loss: 0.7241744846105577 \n",
      "At 987 epoch, Training Loss: 0.6125224828720098 \n",
      "At 987 epoch, Validation Loss: 0.7213289499282837 \n",
      "At 988 epoch, Training Loss: 0.6043460607528687 \n",
      "At 988 epoch, Validation Loss: 0.722752183675766 \n",
      "At 989 epoch, Training Loss: 0.6072443615645174 \n",
      "At 989 epoch, Validation Loss: 0.7191328823566436 \n",
      "At 990 epoch, Training Loss: 0.6124539490789173 \n",
      "At 990 epoch, Validation Loss: 0.7269069194793703 \n",
      "At 991 epoch, Training Loss: 0.6083353299647571 \n",
      "At 991 epoch, Validation Loss: 0.7202208131551743 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 992 epoch, Training Loss: 0.6119495864957568 \n",
      "At 992 epoch, Validation Loss: 0.7202891707420348 \n",
      "At 993 epoch, Training Loss: 0.6101593941450114 \n",
      "At 993 epoch, Validation Loss: 0.7211503177881241 \n",
      "At 994 epoch, Training Loss: 0.608771627396345 \n",
      "At 994 epoch, Validation Loss: 0.7203352212905885 \n",
      "At 995 epoch, Training Loss: 0.6128819700330493 \n",
      "At 995 epoch, Validation Loss: 0.7222470641136169 \n",
      "At 996 epoch, Training Loss: 0.6039847161620855 \n",
      "At 996 epoch, Validation Loss: 0.7226149171590804 \n",
      "At 997 epoch, Training Loss: 0.6089586667716503 \n",
      "At 997 epoch, Validation Loss: 0.7219446271657944 \n",
      "At 998 epoch, Training Loss: 0.6062532473355533 \n",
      "At 998 epoch, Validation Loss: 0.7220250487327575 \n",
      "At 999 epoch, Training Loss: 0.6070444609969851 \n",
      "At 999 epoch, Validation Loss: 0.7203252404928208 \n",
      "At 1000 epoch, Training Loss: 0.6161470234394073 \n",
      "At 1000 epoch, Validation Loss: 0.7239962786436082 \n",
      "At 1001 epoch, Training Loss: 0.6105742424726484 \n",
      "At 1001 epoch, Validation Loss: 0.7230156809091568 \n",
      "At 1002 epoch, Training Loss: 0.6053573306649923 \n",
      "At 1002 epoch, Validation Loss: 0.72317795753479 \n",
      "At 1003 epoch, Training Loss: 0.6113440409302707 \n",
      "At 1003 epoch, Validation Loss: 0.7234062612056734 \n",
      "At 1004 epoch, Training Loss: 0.6096317913383246 \n",
      "At 1004 epoch, Validation Loss: 0.7227484434843063 \n",
      "At 1005 epoch, Training Loss: 0.61488706022501 \n",
      "At 1005 epoch, Validation Loss: 0.7263398617506028 \n",
      "At 1006 epoch, Training Loss: 0.6111475422978403 \n",
      "At 1006 epoch, Validation Loss: 0.7237322002649307 \n",
      "At 1007 epoch, Training Loss: 0.6135201040655376 \n",
      "At 1007 epoch, Validation Loss: 0.7229538172483443 \n",
      "At 1008 epoch, Training Loss: 0.609092762321233 \n",
      "At 1008 epoch, Validation Loss: 0.7249217867851256 \n",
      "At 1009 epoch, Training Loss: 0.6080834623426199 \n",
      "At 1009 epoch, Validation Loss: 0.7205936938524246 \n",
      "At 1010 epoch, Training Loss: 0.6132668755948543 \n",
      "At 1010 epoch, Validation Loss: 0.7190475374460222 \n",
      "At 1011 epoch, Training Loss: 0.6101863097399474 \n",
      "At 1011 epoch, Validation Loss: 0.7224439531564713 \n",
      "At 1012 epoch, Training Loss: 0.6075746081769464 \n",
      "At 1012 epoch, Validation Loss: 0.7227713346481323 \n",
      "At 1013 epoch, Training Loss: 0.6128944028168916 \n",
      "At 1013 epoch, Validation Loss: 0.7237542748451233 \n",
      "At 1014 epoch, Training Loss: 0.6076507609337568 \n",
      "At 1014 epoch, Validation Loss: 0.7192220121622086 \n",
      "At 1015 epoch, Training Loss: 0.6132183380424977 \n",
      "At 1015 epoch, Validation Loss: 0.7219486653804779 \n",
      "At 1016 epoch, Training Loss: 0.6063475750386716 \n",
      "At 1016 epoch, Validation Loss: 0.7222244441509247 \n",
      "At 1017 epoch, Training Loss: 0.6061250530183312 \n",
      "At 1017 epoch, Validation Loss: 0.7261849462985994 \n",
      "At 1018 epoch, Training Loss: 0.6077771104872222 \n",
      "At 1018 epoch, Validation Loss: 0.7217812329530715 \n",
      "At 1019 epoch, Training Loss: 0.6076355271041392 \n",
      "At 1019 epoch, Validation Loss: 0.724620059132576 \n",
      "At 1020 epoch, Training Loss: 0.6060168966650958 \n",
      "At 1020 epoch, Validation Loss: 0.7244492113590241 \n",
      "At 1021 epoch, Training Loss: 0.610609106346965 \n",
      "At 1021 epoch, Validation Loss: 0.726472818851471 \n",
      "At 1022 epoch, Training Loss: 0.6120882937684656 \n",
      "At 1022 epoch, Validation Loss: 0.7248767733573914 \n",
      "At 1023 epoch, Training Loss: 0.6094954809173941 \n",
      "At 1023 epoch, Validation Loss: 0.720367619395256 \n",
      "At 1024 epoch, Training Loss: 0.6124681062996387 \n",
      "At 1024 epoch, Validation Loss: 0.724843591451645 \n",
      "At 1025 epoch, Training Loss: 0.6111199051141739 \n",
      "At 1025 epoch, Validation Loss: 0.7275223225355147 \n",
      "At 1026 epoch, Training Loss: 0.6140255220234394 \n",
      "At 1026 epoch, Validation Loss: 0.7207316935062409 \n",
      "At 1027 epoch, Training Loss: 0.6069126028567555 \n",
      "At 1027 epoch, Validation Loss: 0.722549244761467 \n",
      "At 1028 epoch, Training Loss: 0.6071221604943278 \n",
      "At 1028 epoch, Validation Loss: 0.720427969098091 \n",
      "At 1029 epoch, Training Loss: 0.6096328373998399 \n",
      "At 1029 epoch, Validation Loss: 0.7247282892465591 \n",
      "At 1030 epoch, Training Loss: 0.6156551983207462 \n",
      "At 1030 epoch, Validation Loss: 0.727946189045906 \n",
      "At 1031 epoch, Training Loss: 0.6130141295492645 \n",
      "At 1031 epoch, Validation Loss: 0.723235958814621 \n",
      "At 1032 epoch, Training Loss: 0.6132071454077963 \n",
      "At 1032 epoch, Validation Loss: 0.7204010605812072 \n",
      "At 1033 epoch, Training Loss: 0.6108722269535067 \n",
      "At 1033 epoch, Validation Loss: 0.7250512510538102 \n",
      "At 1034 epoch, Training Loss: 0.6157421626150608 \n",
      "At 1034 epoch, Validation Loss: 0.7201376527547837 \n",
      "At 1035 epoch, Training Loss: 0.6086587451398374 \n",
      "At 1035 epoch, Validation Loss: 0.7210581928491593 \n",
      "At 1036 epoch, Training Loss: 0.6061970796436069 \n",
      "At 1036 epoch, Validation Loss: 0.7254811525344849 \n",
      "At 1037 epoch, Training Loss: 0.6045781381428242 \n",
      "At 1037 epoch, Validation Loss: 0.7242022603750228 \n",
      "At 1038 epoch, Training Loss: 0.6115557624027134 \n",
      "At 1038 epoch, Validation Loss: 0.7230258375406264 \n",
      "At 1039 epoch, Training Loss: 0.6079940669238566 \n",
      "At 1039 epoch, Validation Loss: 0.7243234246969223 \n",
      "At 1040 epoch, Training Loss: 0.6057966209948062 \n",
      "At 1040 epoch, Validation Loss: 0.7234403163194657 \n",
      "At 1041 epoch, Training Loss: 0.6059578102082013 \n",
      "At 1041 epoch, Validation Loss: 0.7205806612968445 \n",
      "At 1042 epoch, Training Loss: 0.6070159532129761 \n",
      "At 1042 epoch, Validation Loss: 0.7207867711782456 \n",
      "At 1043 epoch, Training Loss: 0.613585413619876 \n",
      "At 1043 epoch, Validation Loss: 0.7222969740629197 \n",
      "At 1044 epoch, Training Loss: 0.6113156970590351 \n",
      "At 1044 epoch, Validation Loss: 0.7249906837940215 \n",
      "At 1045 epoch, Training Loss: 0.612608292326331 \n",
      "At 1045 epoch, Validation Loss: 0.7250327110290528 \n",
      "At 1046 epoch, Training Loss: 0.6087668240070342 \n",
      "At 1046 epoch, Validation Loss: 0.7214136898517607 \n",
      "At 1047 epoch, Training Loss: 0.6192093223333359 \n",
      "At 1047 epoch, Validation Loss: 0.7262517154216767 \n",
      "At 1048 epoch, Training Loss: 0.6067711122333999 \n",
      "At 1048 epoch, Validation Loss: 0.7217466950416564 \n",
      "At 1049 epoch, Training Loss: 0.6060071859508755 \n",
      "At 1049 epoch, Validation Loss: 0.7225051254034043 \n",
      "At 1050 epoch, Training Loss: 0.6066641431301834 \n",
      "At 1050 epoch, Validation Loss: 0.7204549401998521 \n",
      "At 1051 epoch, Training Loss: 0.6110536932945247 \n",
      "At 1051 epoch, Validation Loss: 0.7215854465961458 \n",
      "At 1052 epoch, Training Loss: 0.6122140515595672 \n",
      "At 1052 epoch, Validation Loss: 0.7231316715478898 \n",
      "At 1053 epoch, Training Loss: 0.6209252711385491 \n",
      "At 1053 epoch, Validation Loss: 0.7216607987880707 \n",
      "At 1054 epoch, Training Loss: 0.6069902449846271 \n",
      "At 1054 epoch, Validation Loss: 0.7248370110988618 \n",
      "At 1055 epoch, Training Loss: 0.6104474861174822 \n",
      "At 1055 epoch, Validation Loss: 0.726763778924942 \n",
      "At 1056 epoch, Training Loss: 0.6122624509036539 \n",
      "At 1056 epoch, Validation Loss: 0.72225761115551 \n",
      "At 1057 epoch, Training Loss: 0.6065610967576507 \n",
      "At 1057 epoch, Validation Loss: 0.7256092727184293 \n",
      "At 1058 epoch, Training Loss: 0.6124830968678 \n",
      "At 1058 epoch, Validation Loss: 0.7248678594827652 \n",
      "At 1059 epoch, Training Loss: 0.6079715047031645 \n",
      "At 1059 epoch, Validation Loss: 0.7207641035318373 \n",
      "At 1060 epoch, Training Loss: 0.6122855857014657 \n",
      "At 1060 epoch, Validation Loss: 0.7253102719783783 \n",
      "At 1061 epoch, Training Loss: 0.6087941240519286 \n",
      "At 1061 epoch, Validation Loss: 0.7216059982776643 \n",
      "At 1062 epoch, Training Loss: 0.6104914765805007 \n",
      "At 1062 epoch, Validation Loss: 0.7231059640645982 \n",
      "At 1063 epoch, Training Loss: 0.6117051929235457 \n",
      "At 1063 epoch, Validation Loss: 0.7163928508758547 \n",
      "At 1064 epoch, Training Loss: 0.6109852151945236 \n",
      "At 1064 epoch, Validation Loss: 0.7245286405086517 \n",
      "At 1065 epoch, Training Loss: 0.6125321488827465 \n",
      "At 1065 epoch, Validation Loss: 0.7222968906164169 \n",
      "At 1066 epoch, Training Loss: 0.6094151761382816 \n",
      "At 1066 epoch, Validation Loss: 0.725457775592804 \n",
      "At 1067 epoch, Training Loss: 0.608513153716922 \n",
      "At 1067 epoch, Validation Loss: 0.7208055943250656 \n",
      "At 1068 epoch, Training Loss: 0.6149938903748987 \n",
      "At 1068 epoch, Validation Loss: 0.7283882141113281 \n",
      "At 1069 epoch, Training Loss: 0.6170938372612 \n",
      "At 1069 epoch, Validation Loss: 0.7232447892427444 \n",
      "At 1070 epoch, Training Loss: 0.6052127923816442 \n",
      "At 1070 epoch, Validation Loss: 0.7250153958797457 \n",
      "At 1071 epoch, Training Loss: 0.6123438809067011 \n",
      "At 1071 epoch, Validation Loss: 0.7221533805131911 \n",
      "At 1072 epoch, Training Loss: 0.6102636855095626 \n",
      "At 1072 epoch, Validation Loss: 0.7213439732789992 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 1073 epoch, Training Loss: 0.6114041607826949 \n",
      "At 1073 epoch, Validation Loss: 0.7215243846178054 \n",
      "At 1074 epoch, Training Loss: 0.6130629532039167 \n",
      "At 1074 epoch, Validation Loss: 0.7231778919696807 \n",
      "At 1075 epoch, Training Loss: 0.6091765828430652 \n",
      "At 1075 epoch, Validation Loss: 0.7262220650911332 \n",
      "At 1076 epoch, Training Loss: 0.6024800090119242 \n",
      "At 1076 epoch, Validation Loss: 0.7246789813041687 \n",
      "At 1077 epoch, Training Loss: 0.607701731286943 \n",
      "At 1077 epoch, Validation Loss: 0.7233248949050904 \n",
      "At 1078 epoch, Training Loss: 0.6107250906527043 \n",
      "At 1078 epoch, Validation Loss: 0.7246962636709213 \n",
      "At 1079 epoch, Training Loss: 0.6080125436186787 \n",
      "At 1079 epoch, Validation Loss: 0.7243022382259369 \n",
      "At 1080 epoch, Training Loss: 0.6088783228769901 \n",
      "At 1080 epoch, Validation Loss: 0.7239057064056397 \n",
      "At 1081 epoch, Training Loss: 0.6084144834429027 \n",
      "At 1081 epoch, Validation Loss: 0.7250048369169235 \n",
      "At 1082 epoch, Training Loss: 0.6118358083069327 \n",
      "At 1082 epoch, Validation Loss: 0.7204724371433259 \n",
      "At 1083 epoch, Training Loss: 0.6067914117127655 \n",
      "At 1083 epoch, Validation Loss: 0.723416730761528 \n",
      "At 1084 epoch, Training Loss: 0.6111982841044661 \n",
      "At 1084 epoch, Validation Loss: 0.7235804438591004 \n",
      "At 1085 epoch, Training Loss: 0.6108857337385417 \n",
      "At 1085 epoch, Validation Loss: 0.7266938239336014 \n",
      "At 1086 epoch, Training Loss: 0.6110501343384386 \n",
      "At 1086 epoch, Validation Loss: 0.7224169582128525 \n",
      "At 1087 epoch, Training Loss: 0.6081606566905973 \n",
      "At 1087 epoch, Validation Loss: 0.7239202320575715 \n",
      "At 1088 epoch, Training Loss: 0.6124369747936724 \n",
      "At 1088 epoch, Validation Loss: 0.720195809006691 \n",
      "At 1089 epoch, Training Loss: 0.6076890092343092 \n",
      "At 1089 epoch, Validation Loss: 0.7222859531641007 \n",
      "At 1090 epoch, Training Loss: 0.6055717412382364 \n",
      "At 1090 epoch, Validation Loss: 0.721558439731598 \n",
      "At 1091 epoch, Training Loss: 0.6079844797030092 \n",
      "At 1091 epoch, Validation Loss: 0.7218811482191085 \n",
      "At 1092 epoch, Training Loss: 0.6177596382796763 \n",
      "At 1092 epoch, Validation Loss: 0.7177568227052689 \n",
      "At 1093 epoch, Training Loss: 0.615028180554509 \n",
      "At 1093 epoch, Validation Loss: 0.7209568321704862 \n",
      "At 1094 epoch, Training Loss: 0.6133884917944665 \n",
      "At 1094 epoch, Validation Loss: 0.7251825451850891 \n",
      "At 1095 epoch, Training Loss: 0.6122341521084307 \n",
      "At 1095 epoch, Validation Loss: 0.7214305013418199 \n",
      "At 1096 epoch, Training Loss: 0.611365681886673 \n",
      "At 1096 epoch, Validation Loss: 0.7189131498336792 \n",
      "At 1097 epoch, Training Loss: 0.6106333769857883 \n",
      "At 1097 epoch, Validation Loss: 0.7224195301532745 \n",
      "At 1098 epoch, Training Loss: 0.6086181052029136 \n",
      "At 1098 epoch, Validation Loss: 0.7241344690322875 \n",
      "At 1099 epoch, Training Loss: 0.6084526520222423 \n",
      "At 1099 epoch, Validation Loss: 0.720189967751503 \n",
      "At 1100 epoch, Training Loss: 0.608427784219384 \n",
      "At 1100 epoch, Validation Loss: 0.7227380633354187 \n",
      "At 1101 epoch, Training Loss: 0.6105508062988523 \n",
      "At 1101 epoch, Validation Loss: 0.7264307618141175 \n",
      "At 1102 epoch, Training Loss: 0.6109838392585516 \n",
      "At 1102 epoch, Validation Loss: 0.7234369963407516 \n",
      "At 1103 epoch, Training Loss: 0.6085518348962066 \n",
      "At 1103 epoch, Validation Loss: 0.7245615512132645 \n",
      "At 1104 epoch, Training Loss: 0.6112248044461015 \n",
      "At 1104 epoch, Validation Loss: 0.7207535237073899 \n",
      "At 1105 epoch, Training Loss: 0.6093197692185643 \n",
      "At 1105 epoch, Validation Loss: 0.7208528846502303 \n",
      "At 1106 epoch, Training Loss: 0.6141148038208487 \n",
      "At 1106 epoch, Validation Loss: 0.7246745496988296 \n",
      "At 1107 epoch, Training Loss: 0.610031836107373 \n",
      "At 1107 epoch, Validation Loss: 0.7228770226240158 \n",
      "At 1108 epoch, Training Loss: 0.6138293333351615 \n",
      "At 1108 epoch, Validation Loss: 0.7257255375385285 \n",
      "At 1109 epoch, Training Loss: 0.608834780752659 \n",
      "At 1109 epoch, Validation Loss: 0.7243407666683197 \n",
      "At 1110 epoch, Training Loss: 0.6101436283439399 \n",
      "At 1110 epoch, Validation Loss: 0.7222925871610641 \n",
      "At 1111 epoch, Training Loss: 0.6090984325855971 \n",
      "At 1111 epoch, Validation Loss: 0.7244422882795334 \n",
      "At 1112 epoch, Training Loss: 0.6126807380467653 \n",
      "At 1112 epoch, Validation Loss: 0.7272982388734818 \n",
      "At 1113 epoch, Training Loss: 0.6101439896970987 \n",
      "At 1113 epoch, Validation Loss: 0.7241138190031051 \n",
      "At 1114 epoch, Training Loss: 0.6124155085533858 \n",
      "At 1114 epoch, Validation Loss: 0.7246523976325988 \n",
      "At 1115 epoch, Training Loss: 0.61293583791703 \n",
      "At 1115 epoch, Validation Loss: 0.7240180522203445 \n",
      "At 1116 epoch, Training Loss: 0.6064693044871089 \n",
      "At 1116 epoch, Validation Loss: 0.7203063756227492 \n",
      "At 1117 epoch, Training Loss: 0.6095068085938686 \n",
      "At 1117 epoch, Validation Loss: 0.7232338786125184 \n",
      "At 1118 epoch, Training Loss: 0.6063108336180448 \n",
      "At 1118 epoch, Validation Loss: 0.7238321602344512 \n",
      "At 1119 epoch, Training Loss: 0.6084269296377893 \n",
      "At 1119 epoch, Validation Loss: 0.7244986802339554 \n",
      "At 1120 epoch, Training Loss: 0.6129899851977826 \n",
      "At 1120 epoch, Validation Loss: 0.7246029675006868 \n",
      "At 1121 epoch, Training Loss: 0.6034581687301399 \n",
      "At 1121 epoch, Validation Loss: 0.7218260645866394 \n",
      "At 1122 epoch, Training Loss: 0.6110298171639443 \n",
      "At 1122 epoch, Validation Loss: 0.7192317962646486 \n",
      "At 1123 epoch, Training Loss: 0.6074747372418642 \n",
      "At 1123 epoch, Validation Loss: 0.7264681816101074 \n",
      "At 1124 epoch, Training Loss: 0.6127662107348443 \n",
      "At 1124 epoch, Validation Loss: 0.7211441665887833 \n",
      "At 1125 epoch, Training Loss: 0.6148388648405672 \n",
      "At 1125 epoch, Validation Loss: 0.7251172989606858 \n",
      "At 1126 epoch, Training Loss: 0.6070253137499094 \n",
      "At 1126 epoch, Validation Loss: 0.7218389511108398 \n",
      "At 1127 epoch, Training Loss: 0.6104351229965685 \n",
      "At 1127 epoch, Validation Loss: 0.727373331785202 \n",
      "At 1128 epoch, Training Loss: 0.6054668366909028 \n",
      "At 1128 epoch, Validation Loss: 0.7205143690109252 \n",
      "At 1129 epoch, Training Loss: 0.6138343323022125 \n",
      "At 1129 epoch, Validation Loss: 0.7219073534011841 \n",
      "At 1130 epoch, Training Loss: 0.6085518147796388 \n",
      "At 1130 epoch, Validation Loss: 0.7244206905364989 \n",
      "At 1131 epoch, Training Loss: 0.6101630032062529 \n",
      "At 1131 epoch, Validation Loss: 0.7208528548479081 \n",
      "At 1132 epoch, Training Loss: 0.6104083973914384 \n",
      "At 1132 epoch, Validation Loss: 0.7251233011484146 \n",
      "At 1133 epoch, Training Loss: 0.605698636174202 \n",
      "At 1133 epoch, Validation Loss: 0.7192187696695327 \n",
      "At 1134 epoch, Training Loss: 0.615534207224846 \n",
      "At 1134 epoch, Validation Loss: 0.7230226069688797 \n",
      "At 1135 epoch, Training Loss: 0.6050421305000783 \n",
      "At 1135 epoch, Validation Loss: 0.7244206607341765 \n",
      "At 1136 epoch, Training Loss: 0.6055826965719462 \n",
      "At 1136 epoch, Validation Loss: 0.7238175600767136 \n",
      "At 1137 epoch, Training Loss: 0.6160008259117606 \n",
      "At 1137 epoch, Validation Loss: 0.7210504919290543 \n",
      "At 1138 epoch, Training Loss: 0.610033303871751 \n",
      "At 1138 epoch, Validation Loss: 0.7219470053911209 \n",
      "At 1139 epoch, Training Loss: 0.6068921700119974 \n",
      "At 1139 epoch, Validation Loss: 0.7217511385679245 \n",
      "At 1140 epoch, Training Loss: 0.6082003515213725 \n",
      "At 1140 epoch, Validation Loss: 0.7238040566444397 \n",
      "At 1141 epoch, Training Loss: 0.6076013451442122 \n",
      "At 1141 epoch, Validation Loss: 0.7225506037473679 \n",
      "At 1142 epoch, Training Loss: 0.6094782229512931 \n",
      "At 1142 epoch, Validation Loss: 0.7250745236873627 \n",
      "At 1143 epoch, Training Loss: 0.6110852431505914 \n",
      "At 1143 epoch, Validation Loss: 0.7267236977815629 \n",
      "At 1144 epoch, Training Loss: 0.6130710281431682 \n",
      "At 1144 epoch, Validation Loss: 0.721747487783432 \n",
      "At 1145 epoch, Training Loss: 0.6153710646554829 \n",
      "At 1145 epoch, Validation Loss: 0.720869180560112 \n",
      "At 1146 epoch, Training Loss: 0.6140024520456793 \n",
      "At 1146 epoch, Validation Loss: 0.7205768734216691 \n",
      "At 1147 epoch, Training Loss: 0.6126600228250028 \n",
      "At 1147 epoch, Validation Loss: 0.7244299829006197 \n",
      "At 1148 epoch, Training Loss: 0.6112392473965884 \n",
      "At 1148 epoch, Validation Loss: 0.7206559479236604 \n",
      "At 1149 epoch, Training Loss: 0.6097431335598233 \n",
      "At 1149 epoch, Validation Loss: 0.7206999987363816 \n",
      "At 1150 epoch, Training Loss: 0.613880269229412 \n",
      "At 1150 epoch, Validation Loss: 0.7241944849491119 \n",
      "At 1151 epoch, Training Loss: 0.6131198607385158 \n",
      "At 1151 epoch, Validation Loss: 0.7272387027740479 \n",
      "At 1152 epoch, Training Loss: 0.6132747557014228 \n",
      "At 1152 epoch, Validation Loss: 0.7214773207902908 \n",
      "At 1153 epoch, Training Loss: 0.6069924961775541 \n",
      "At 1153 epoch, Validation Loss: 0.7227187246084213 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 1154 epoch, Training Loss: 0.6107086382806302 \n",
      "At 1154 epoch, Validation Loss: 0.7217036098241805 \n",
      "At 1155 epoch, Training Loss: 0.609381967410445 \n",
      "At 1155 epoch, Validation Loss: 0.7221511214971541 \n",
      "At 1156 epoch, Training Loss: 0.6143801119178536 \n",
      "At 1156 epoch, Validation Loss: 0.7187602281570434 \n",
      "At 1157 epoch, Training Loss: 0.6092220816761255 \n",
      "At 1157 epoch, Validation Loss: 0.7249489039182663 \n",
      "At 1158 epoch, Training Loss: 0.6046492613852021 \n",
      "At 1158 epoch, Validation Loss: 0.7218215644359589 \n",
      "At 1159 epoch, Training Loss: 0.6081306140869859 \n",
      "At 1159 epoch, Validation Loss: 0.7193356126546859 \n",
      "At 1160 epoch, Training Loss: 0.6145849190652369 \n",
      "At 1160 epoch, Validation Loss: 0.7246403127908707 \n",
      "At 1161 epoch, Training Loss: 0.6098179586231708 \n",
      "At 1161 epoch, Validation Loss: 0.7220789045095444 \n",
      "At 1162 epoch, Training Loss: 0.6135564535856245 \n",
      "At 1162 epoch, Validation Loss: 0.7230809718370438 \n",
      "At 1163 epoch, Training Loss: 0.6082654723897577 \n",
      "At 1163 epoch, Validation Loss: 0.7214776635169983 \n",
      "At 1164 epoch, Training Loss: 0.6090247847139835 \n",
      "At 1164 epoch, Validation Loss: 0.7264364898204801 \n",
      "At 1165 epoch, Training Loss: 0.6158407852053641 \n",
      "At 1165 epoch, Validation Loss: 0.7250904798507691 \n",
      "At 1166 epoch, Training Loss: 0.6097234871238469 \n",
      "At 1166 epoch, Validation Loss: 0.7232279628515245 \n",
      "At 1167 epoch, Training Loss: 0.6097609292715787 \n",
      "At 1167 epoch, Validation Loss: 0.7262037366628646 \n",
      "At 1168 epoch, Training Loss: 0.6072803996503355 \n",
      "At 1168 epoch, Validation Loss: 0.7229340404272079 \n",
      "At 1169 epoch, Training Loss: 0.6112487424165007 \n",
      "At 1169 epoch, Validation Loss: 0.7225785732269286 \n",
      "At 1170 epoch, Training Loss: 0.6107150819152596 \n",
      "At 1170 epoch, Validation Loss: 0.7269326388835908 \n",
      "At 1171 epoch, Training Loss: 0.6064556945115326 \n",
      "At 1171 epoch, Validation Loss: 0.7211232006549835 \n",
      "At 1172 epoch, Training Loss: 0.604301663860679 \n",
      "At 1172 epoch, Validation Loss: 0.7236475080251694 \n",
      "At 1173 epoch, Training Loss: 0.6106637429445984 \n",
      "At 1173 epoch, Validation Loss: 0.7227079749107361 \n",
      "At 1174 epoch, Training Loss: 0.6123808328062293 \n",
      "At 1174 epoch, Validation Loss: 0.7223733544349671 \n",
      "At 1175 epoch, Training Loss: 0.6098832238465546 \n",
      "At 1175 epoch, Validation Loss: 0.722439107298851 \n",
      "At 1176 epoch, Training Loss: 0.6000942397862674 \n",
      "At 1176 epoch, Validation Loss: 0.7235153019428252 \n",
      "At 1177 epoch, Training Loss: 0.6123445719480511 \n",
      "At 1177 epoch, Validation Loss: 0.727666687965393 \n",
      "At 1178 epoch, Training Loss: 0.6082049049437049 \n",
      "At 1178 epoch, Validation Loss: 0.7212213516235353 \n",
      "At 1179 epoch, Training Loss: 0.611877442151308 \n",
      "At 1179 epoch, Validation Loss: 0.7209510624408723 \n",
      "At 1180 epoch, Training Loss: 0.6090027928352357 \n",
      "At 1180 epoch, Validation Loss: 0.7225780874490737 \n",
      "At 1181 epoch, Training Loss: 0.6131526274606585 \n",
      "At 1181 epoch, Validation Loss: 0.718747255206108 \n",
      "At 1182 epoch, Training Loss: 0.6124461021274329 \n",
      "At 1182 epoch, Validation Loss: 0.724823161959648 \n",
      "At 1183 epoch, Training Loss: 0.6092602595686911 \n",
      "At 1183 epoch, Validation Loss: 0.7260187596082687 \n",
      "At 1184 epoch, Training Loss: 0.6139835029840469 \n",
      "At 1184 epoch, Validation Loss: 0.7199861943721771 \n",
      "At 1185 epoch, Training Loss: 0.6053733985871074 \n",
      "At 1185 epoch, Validation Loss: 0.7247372359037397 \n",
      "At 1186 epoch, Training Loss: 0.6093180906027555 \n",
      "At 1186 epoch, Validation Loss: 0.7239614278078079 \n",
      "At 1187 epoch, Training Loss: 0.6124752569943668 \n",
      "At 1187 epoch, Validation Loss: 0.7216894835233689 \n",
      "At 1188 epoch, Training Loss: 0.610634185746312 \n",
      "At 1188 epoch, Validation Loss: 0.7199819654226303 \n",
      "At 1189 epoch, Training Loss: 0.6062187325209377 \n",
      "At 1189 epoch, Validation Loss: 0.7211020439863204 \n",
      "At 1190 epoch, Training Loss: 0.6068122871220114 \n",
      "At 1190 epoch, Validation Loss: 0.7218122124671936 \n",
      "At 1191 epoch, Training Loss: 0.610443476587534 \n",
      "At 1191 epoch, Validation Loss: 0.721100440621376 \n",
      "At 1192 epoch, Training Loss: 0.6108426727354526 \n",
      "At 1192 epoch, Validation Loss: 0.72377008497715 \n",
      "At 1193 epoch, Training Loss: 0.6169106196612117 \n",
      "At 1193 epoch, Validation Loss: 0.7225109398365022 \n",
      "At 1194 epoch, Training Loss: 0.6068069864064455 \n",
      "At 1194 epoch, Validation Loss: 0.7223084926605227 \n",
      "At 1195 epoch, Training Loss: 0.6080735940486195 \n",
      "At 1195 epoch, Validation Loss: 0.7195703387260437 \n",
      "At 1196 epoch, Training Loss: 0.608926229178905 \n",
      "At 1196 epoch, Validation Loss: 0.7231668144464493 \n",
      "At 1197 epoch, Training Loss: 0.6066615179181098 \n",
      "At 1197 epoch, Validation Loss: 0.7274933815002441 \n",
      "At 1198 epoch, Training Loss: 0.6088591556996106 \n",
      "At 1198 epoch, Validation Loss: 0.7237897664308549 \n",
      "At 1199 epoch, Training Loss: 0.6076610378921034 \n",
      "At 1199 epoch, Validation Loss: 0.7230358570814133 \n",
      "At 1200 epoch, Training Loss: 0.6023941155523063 \n",
      "At 1200 epoch, Validation Loss: 0.7248256862163545 \n",
      "At 1201 epoch, Training Loss: 0.6107358317822219 \n",
      "At 1201 epoch, Validation Loss: 0.7249417066574096 \n",
      "At 1202 epoch, Training Loss: 0.6139117550104859 \n",
      "At 1202 epoch, Validation Loss: 0.7209073185920715 \n",
      "At 1203 epoch, Training Loss: 0.6097646765410903 \n",
      "At 1203 epoch, Validation Loss: 0.7215703368186951 \n",
      "At 1204 epoch, Training Loss: 0.6079799834638834 \n",
      "At 1204 epoch, Validation Loss: 0.7236051440238953 \n",
      "At 1205 epoch, Training Loss: 0.6048740446567532 \n",
      "At 1205 epoch, Validation Loss: 0.7223184019327161 \n",
      "At 1206 epoch, Training Loss: 0.6084520101547238 \n",
      "At 1206 epoch, Validation Loss: 0.7249617159366608 \n",
      "At 1207 epoch, Training Loss: 0.6130134634673593 \n",
      "At 1207 epoch, Validation Loss: 0.7243058919906615 \n",
      "At 1208 epoch, Training Loss: 0.6097691364586354 \n",
      "At 1208 epoch, Validation Loss: 0.7199390858411788 \n",
      "At 1209 epoch, Training Loss: 0.607262234389782 \n",
      "At 1209 epoch, Validation Loss: 0.7227241098880768 \n",
      "At 1210 epoch, Training Loss: 0.611907378211618 \n",
      "At 1210 epoch, Validation Loss: 0.7222648948431014 \n",
      "At 1211 epoch, Training Loss: 0.6136499337852006 \n",
      "At 1211 epoch, Validation Loss: 0.7207482904195786 \n",
      "At 1212 epoch, Training Loss: 0.6103741109371185 \n",
      "At 1212 epoch, Validation Loss: 0.7204832077026367 \n",
      "At 1213 epoch, Training Loss: 0.6085752245038746 \n",
      "At 1213 epoch, Validation Loss: 0.7206721991300583 \n",
      "At 1214 epoch, Training Loss: 0.608838715776801 \n",
      "At 1214 epoch, Validation Loss: 0.71848806142807 \n",
      "At 1215 epoch, Training Loss: 0.61263068318367 \n",
      "At 1215 epoch, Validation Loss: 0.7277088701725005 \n",
      "At 1216 epoch, Training Loss: 0.6121039621531967 \n",
      "At 1216 epoch, Validation Loss: 0.7258943885564804 \n",
      "At 1217 epoch, Training Loss: 0.605474126711488 \n",
      "At 1217 epoch, Validation Loss: 0.7260273903608323 \n",
      "At 1218 epoch, Training Loss: 0.6059575073421001 \n",
      "At 1218 epoch, Validation Loss: 0.7221714854240417 \n",
      "At 1219 epoch, Training Loss: 0.6179271884262562 \n",
      "At 1219 epoch, Validation Loss: 0.7225990414619445 \n",
      "At 1220 epoch, Training Loss: 0.6028123106807468 \n",
      "At 1220 epoch, Validation Loss: 0.7239827722311019 \n",
      "At 1221 epoch, Training Loss: 0.6114507824182508 \n",
      "At 1221 epoch, Validation Loss: 0.7209778606891631 \n",
      "At 1222 epoch, Training Loss: 0.6107900671660897 \n",
      "At 1222 epoch, Validation Loss: 0.7222950398921966 \n",
      "At 1223 epoch, Training Loss: 0.6123966138809918 \n",
      "At 1223 epoch, Validation Loss: 0.7248002022504805 \n",
      "At 1224 epoch, Training Loss: 0.6063114251941443 \n",
      "At 1224 epoch, Validation Loss: 0.7258390426635741 \n",
      "At 1225 epoch, Training Loss: 0.6089786704629656 \n",
      "At 1225 epoch, Validation Loss: 0.7259000182151795 \n",
      "At 1226 epoch, Training Loss: 0.6108633931726216 \n",
      "At 1226 epoch, Validation Loss: 0.721388092637062 \n",
      "At 1227 epoch, Training Loss: 0.6090151678770783 \n",
      "At 1227 epoch, Validation Loss: 0.7231799513101578 \n",
      "At 1228 epoch, Training Loss: 0.6084824429824945 \n",
      "At 1228 epoch, Validation Loss: 0.7225269973278046 \n",
      "At 1229 epoch, Training Loss: 0.6111599069088696 \n",
      "At 1229 epoch, Validation Loss: 0.7247469872236252 \n",
      "At 1230 epoch, Training Loss: 0.6098945528268818 \n",
      "At 1230 epoch, Validation Loss: 0.7295091539621353 \n",
      "At 1231 epoch, Training Loss: 0.609278101474047 \n",
      "At 1231 epoch, Validation Loss: 0.7223328053951263 \n",
      "At 1232 epoch, Training Loss: 0.6065488876774905 \n",
      "At 1232 epoch, Validation Loss: 0.726367610692978 \n",
      "At 1233 epoch, Training Loss: 0.6036604184657336 \n",
      "At 1233 epoch, Validation Loss: 0.7214435249567032 \n",
      "At 1234 epoch, Training Loss: 0.6094860922545197 \n",
      "At 1234 epoch, Validation Loss: 0.7227136939764023 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 1235 epoch, Training Loss: 0.6103426009416578 \n",
      "At 1235 epoch, Validation Loss: 0.7244850009679794 \n",
      "At 1236 epoch, Training Loss: 0.6094882406294344 \n",
      "At 1236 epoch, Validation Loss: 0.7273751556873322 \n",
      "At 1237 epoch, Training Loss: 0.6107816308736805 \n",
      "At 1237 epoch, Validation Loss: 0.717911046743393 \n",
      "At 1238 epoch, Training Loss: 0.6152409642934799 \n",
      "At 1238 epoch, Validation Loss: 0.7218703299760818 \n",
      "At 1239 epoch, Training Loss: 0.6157121554017069 \n",
      "At 1239 epoch, Validation Loss: 0.7253462672233582 \n",
      "At 1240 epoch, Training Loss: 0.6090520098805432 \n",
      "At 1240 epoch, Validation Loss: 0.7190444201231002 \n",
      "At 1241 epoch, Training Loss: 0.6064845256507397 \n",
      "At 1241 epoch, Validation Loss: 0.7234444648027422 \n",
      "At 1242 epoch, Training Loss: 0.6142702061682939 \n",
      "At 1242 epoch, Validation Loss: 0.7239032953977584 \n",
      "At 1243 epoch, Training Loss: 0.6136739686131482 \n",
      "At 1243 epoch, Validation Loss: 0.7200433552265167 \n",
      "At 1244 epoch, Training Loss: 0.6085736434906722 \n",
      "At 1244 epoch, Validation Loss: 0.7222797185182573 \n",
      "At 1245 epoch, Training Loss: 0.6028352662920955 \n",
      "At 1245 epoch, Validation Loss: 0.7215117692947389 \n",
      "At 1246 epoch, Training Loss: 0.6102986834943291 \n",
      "At 1246 epoch, Validation Loss: 0.7217508614063263 \n",
      "At 1247 epoch, Training Loss: 0.6106167312711471 \n",
      "At 1247 epoch, Validation Loss: 0.7248383790254594 \n",
      "At 1248 epoch, Training Loss: 0.6092247255146501 \n",
      "At 1248 epoch, Validation Loss: 0.7216116338968277 \n",
      "At 1249 epoch, Training Loss: 0.6080077987164257 \n",
      "At 1249 epoch, Validation Loss: 0.7215171009302139 \n",
      "At 1250 epoch, Training Loss: 0.6098293554037812 \n",
      "At 1250 epoch, Validation Loss: 0.7228403747081755 \n",
      "At 1251 epoch, Training Loss: 0.6113692607730628 \n",
      "At 1251 epoch, Validation Loss: 0.7252961784601212 \n",
      "At 1252 epoch, Training Loss: 0.6096160866320133 \n",
      "At 1252 epoch, Validation Loss: 0.7265238434076309 \n",
      "At 1253 epoch, Training Loss: 0.6172348473221065 \n",
      "At 1253 epoch, Validation Loss: 0.7250490009784698 \n",
      "At 1254 epoch, Training Loss: 0.6093430738896133 \n",
      "At 1254 epoch, Validation Loss: 0.7219088137149811 \n",
      "At 1255 epoch, Training Loss: 0.612594112381339 \n",
      "At 1255 epoch, Validation Loss: 0.7246614754199981 \n",
      "At 1256 epoch, Training Loss: 0.6076775215566158 \n",
      "At 1256 epoch, Validation Loss: 0.7253552883863448 \n",
      "At 1257 epoch, Training Loss: 0.6112108219414951 \n",
      "At 1257 epoch, Validation Loss: 0.72398861348629 \n",
      "At 1258 epoch, Training Loss: 0.6096466492861511 \n",
      "At 1258 epoch, Validation Loss: 0.7226410210132598 \n",
      "At 1259 epoch, Training Loss: 0.6060670834034684 \n",
      "At 1259 epoch, Validation Loss: 0.7233557790517806 \n",
      "At 1260 epoch, Training Loss: 0.6083648972213265 \n",
      "At 1260 epoch, Validation Loss: 0.7250363916158675 \n",
      "At 1261 epoch, Training Loss: 0.6135679978877303 \n",
      "At 1261 epoch, Validation Loss: 0.7238687068223953 \n",
      "At 1262 epoch, Training Loss: 0.6095293276011943 \n",
      "At 1262 epoch, Validation Loss: 0.7256265103816985 \n",
      "At 1263 epoch, Training Loss: 0.6156880713999269 \n",
      "At 1263 epoch, Validation Loss: 0.7206032514572145 \n",
      "At 1264 epoch, Training Loss: 0.6151607323437931 \n",
      "At 1264 epoch, Validation Loss: 0.7249160736799241 \n",
      "At 1265 epoch, Training Loss: 0.6085303407162426 \n",
      "At 1265 epoch, Validation Loss: 0.721061035990715 \n",
      "At 1266 epoch, Training Loss: 0.6113944940268992 \n",
      "At 1266 epoch, Validation Loss: 0.7250094503164292 \n",
      "At 1267 epoch, Training Loss: 0.61181060411036 \n",
      "At 1267 epoch, Validation Loss: 0.7193352967500686 \n",
      "At 1268 epoch, Training Loss: 0.6098771452903746 \n",
      "At 1268 epoch, Validation Loss: 0.7222233414649963 \n",
      "At 1269 epoch, Training Loss: 0.6091745968908076 \n",
      "At 1269 epoch, Validation Loss: 0.7204029887914657 \n",
      "At 1270 epoch, Training Loss: 0.6117122679948805 \n",
      "At 1270 epoch, Validation Loss: 0.722944152355194 \n",
      "At 1271 epoch, Training Loss: 0.6082443848252297 \n",
      "At 1271 epoch, Validation Loss: 0.722117555141449 \n",
      "At 1272 epoch, Training Loss: 0.6163111198693514 \n",
      "At 1272 epoch, Validation Loss: 0.7217074364423752 \n",
      "At 1273 epoch, Training Loss: 0.6094295267015699 \n",
      "At 1273 epoch, Validation Loss: 0.7249284207820892 \n",
      "At 1274 epoch, Training Loss: 0.609183621406555 \n",
      "At 1274 epoch, Validation Loss: 0.7221587955951692 \n",
      "At 1275 epoch, Training Loss: 0.6104269482195372 \n",
      "At 1275 epoch, Validation Loss: 0.7231999963521957 \n",
      "At 1276 epoch, Training Loss: 0.6122791815549137 \n",
      "At 1276 epoch, Validation Loss: 0.7214858204126359 \n",
      "At 1277 epoch, Training Loss: 0.6100024592131374 \n",
      "At 1277 epoch, Validation Loss: 0.7228204727172853 \n",
      "At 1278 epoch, Training Loss: 0.6097868140786884 \n",
      "At 1278 epoch, Validation Loss: 0.7269098550081253 \n",
      "At 1279 epoch, Training Loss: 0.6105781134217976 \n",
      "At 1279 epoch, Validation Loss: 0.7251667588949202 \n",
      "At 1280 epoch, Training Loss: 0.6129558824002744 \n",
      "At 1280 epoch, Validation Loss: 0.7247452825307845 \n",
      "At 1281 epoch, Training Loss: 0.6092810880392787 \n",
      "At 1281 epoch, Validation Loss: 0.7286717921495438 \n",
      "At 1282 epoch, Training Loss: 0.6119809810072186 \n",
      "At 1282 epoch, Validation Loss: 0.722856456041336 \n",
      "At 1283 epoch, Training Loss: 0.6081860382109877 \n",
      "At 1283 epoch, Validation Loss: 0.7265740305185318 \n",
      "At 1284 epoch, Training Loss: 0.6124376412481067 \n",
      "At 1284 epoch, Validation Loss: 0.7199027568101882 \n",
      "At 1285 epoch, Training Loss: 0.6086425866931678 \n",
      "At 1285 epoch, Validation Loss: 0.7252698779106141 \n",
      "At 1286 epoch, Training Loss: 0.6084953840821976 \n",
      "At 1286 epoch, Validation Loss: 0.727901405096054 \n",
      "At 1287 epoch, Training Loss: 0.6100633759051564 \n",
      "At 1287 epoch, Validation Loss: 0.7206673294305801 \n",
      "At 1288 epoch, Training Loss: 0.6113117519766091 \n",
      "At 1288 epoch, Validation Loss: 0.7213920265436171 \n",
      "At 1289 epoch, Training Loss: 0.6124266438186171 \n",
      "At 1289 epoch, Validation Loss: 0.7271755844354628 \n",
      "At 1290 epoch, Training Loss: 0.6111873243004081 \n",
      "At 1290 epoch, Validation Loss: 0.7225238651037216 \n",
      "At 1291 epoch, Training Loss: 0.6093129117041823 \n",
      "At 1291 epoch, Validation Loss: 0.7264340311288834 \n",
      "At 1292 epoch, Training Loss: 0.6076528944075109 \n",
      "At 1292 epoch, Validation Loss: 0.7262304484844206 \n",
      "At 1293 epoch, Training Loss: 0.6082703292369843 \n",
      "At 1293 epoch, Validation Loss: 0.7212120115756988 \n",
      "At 1294 epoch, Training Loss: 0.6093651108443736 \n",
      "At 1294 epoch, Validation Loss: 0.7234093964099885 \n",
      "At 1295 epoch, Training Loss: 0.6055978871881961 \n",
      "At 1295 epoch, Validation Loss: 0.7244455188512803 \n",
      "At 1296 epoch, Training Loss: 0.6104409221559765 \n",
      "At 1296 epoch, Validation Loss: 0.7259429186582567 \n",
      "At 1297 epoch, Training Loss: 0.6115792077034706 \n",
      "At 1297 epoch, Validation Loss: 0.7217565327882767 \n",
      "At 1298 epoch, Training Loss: 0.6152269896119832 \n",
      "At 1298 epoch, Validation Loss: 0.7221821159124374 \n",
      "At 1299 epoch, Training Loss: 0.6159366339445109 \n",
      "At 1299 epoch, Validation Loss: 0.7219257593154909 \n",
      "At 1300 epoch, Training Loss: 0.6130848027765751 \n",
      "At 1300 epoch, Validation Loss: 0.7234200716018677 \n",
      "At 1301 epoch, Training Loss: 0.6129422966390853 \n",
      "At 1301 epoch, Validation Loss: 0.7189349710941314 \n",
      "At 1302 epoch, Training Loss: 0.6120896425098183 \n",
      "At 1302 epoch, Validation Loss: 0.721314612030983 \n",
      "At 1303 epoch, Training Loss: 0.6067235480993984 \n",
      "At 1303 epoch, Validation Loss: 0.7233874797821046 \n",
      "At 1304 epoch, Training Loss: 0.6124315869063142 \n",
      "At 1304 epoch, Validation Loss: 0.7259095847606657 \n",
      "At 1305 epoch, Training Loss: 0.6119715206325056 \n",
      "At 1305 epoch, Validation Loss: 0.72266748547554 \n",
      "At 1306 epoch, Training Loss: 0.6098298564553262 \n",
      "At 1306 epoch, Validation Loss: 0.7246229350566863 \n",
      "At 1307 epoch, Training Loss: 0.6120939349755649 \n",
      "At 1307 epoch, Validation Loss: 0.725553447008133 \n",
      "At 1308 epoch, Training Loss: 0.6115614600479606 \n",
      "At 1308 epoch, Validation Loss: 0.723656615614891 \n",
      "At 1309 epoch, Training Loss: 0.6076242681592698 \n",
      "At 1309 epoch, Validation Loss: 0.7247741252183915 \n",
      "At 1310 epoch, Training Loss: 0.6105173356831073 \n",
      "At 1310 epoch, Validation Loss: 0.7222716748714447 \n",
      "At 1311 epoch, Training Loss: 0.6113652683794503 \n",
      "At 1311 epoch, Validation Loss: 0.7232728451490402 \n",
      "At 1312 epoch, Training Loss: 0.6084633361548187 \n",
      "At 1312 epoch, Validation Loss: 0.7222545474767685 \n",
      "At 1313 epoch, Training Loss: 0.6083762176334858 \n",
      "At 1313 epoch, Validation Loss: 0.720917609333992 \n",
      "At 1314 epoch, Training Loss: 0.6109885726124045 \n",
      "At 1314 epoch, Validation Loss: 0.7232071459293368 \n",
      "At 1315 epoch, Training Loss: 0.6102976582944398 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 1315 epoch, Validation Loss: 0.7252820461988448 \n",
      "At 1316 epoch, Training Loss: 0.6138365052640437 \n",
      "At 1316 epoch, Validation Loss: 0.7231363445520402 \n",
      "At 1317 epoch, Training Loss: 0.6081444974988701 \n",
      "At 1317 epoch, Validation Loss: 0.7254482984542846 \n",
      "At 1318 epoch, Training Loss: 0.6107391972094777 \n",
      "At 1318 epoch, Validation Loss: 0.7215378403663635 \n",
      "At 1319 epoch, Training Loss: 0.6028475631028416 \n",
      "At 1319 epoch, Validation Loss: 0.7212016940116882 \n",
      "At 1320 epoch, Training Loss: 0.6150647930800914 \n",
      "At 1320 epoch, Validation Loss: 0.7270688384771347 \n",
      "At 1321 epoch, Training Loss: 0.6112005364149807 \n",
      "At 1321 epoch, Validation Loss: 0.7203507572412491 \n",
      "At 1322 epoch, Training Loss: 0.6085674967616796 \n",
      "At 1322 epoch, Validation Loss: 0.7211436212062835 \n",
      "At 1323 epoch, Training Loss: 0.6081262651830909 \n",
      "At 1323 epoch, Validation Loss: 0.7202630698680876 \n",
      "At 1324 epoch, Training Loss: 0.620520081743598 \n",
      "At 1324 epoch, Validation Loss: 0.720586535334587 \n",
      "At 1325 epoch, Training Loss: 0.6098376158624883 \n",
      "At 1325 epoch, Validation Loss: 0.7263588368892668 \n",
      "At 1326 epoch, Training Loss: 0.6106449428945777 \n",
      "At 1326 epoch, Validation Loss: 0.7243872493505478 \n",
      "At 1327 epoch, Training Loss: 0.6111717537045476 \n",
      "At 1327 epoch, Validation Loss: 0.7225372016429902 \n",
      "At 1328 epoch, Training Loss: 0.6064940631389623 \n",
      "At 1328 epoch, Validation Loss: 0.7238867163658141 \n",
      "At 1329 epoch, Training Loss: 0.6111981924623251 \n",
      "At 1329 epoch, Validation Loss: 0.7203997761011123 \n",
      "At 1330 epoch, Training Loss: 0.6080553092062475 \n",
      "At 1330 epoch, Validation Loss: 0.7230733096599578 \n",
      "At 1331 epoch, Training Loss: 0.6098581265658137 \n",
      "At 1331 epoch, Validation Loss: 0.7241558253765106 \n",
      "At 1332 epoch, Training Loss: 0.6110629674047229 \n",
      "At 1332 epoch, Validation Loss: 0.7242822647094727 \n",
      "At 1333 epoch, Training Loss: 0.6117960054427384 \n",
      "At 1333 epoch, Validation Loss: 0.7204582422971725 \n",
      "At 1334 epoch, Training Loss: 0.6100536860525608 \n",
      "At 1334 epoch, Validation Loss: 0.726662164926529 \n",
      "At 1335 epoch, Training Loss: 0.6106448937207462 \n",
      "At 1335 epoch, Validation Loss: 0.7212090283632278 \n",
      "At 1336 epoch, Training Loss: 0.6101368930190803 \n",
      "At 1336 epoch, Validation Loss: 0.7232749700546266 \n",
      "At 1337 epoch, Training Loss: 0.6119910500943658 \n",
      "At 1337 epoch, Validation Loss: 0.7254680186510085 \n",
      "At 1338 epoch, Training Loss: 0.6097132708877329 \n",
      "At 1338 epoch, Validation Loss: 0.721100351214409 \n",
      "At 1339 epoch, Training Loss: 0.6141915213316678 \n",
      "At 1339 epoch, Validation Loss: 0.7226113528013228 \n",
      "At 1340 epoch, Training Loss: 0.6163490448147056 \n",
      "At 1340 epoch, Validation Loss: 0.7239443987607956 \n",
      "At 1341 epoch, Training Loss: 0.6086641199886795 \n",
      "At 1341 epoch, Validation Loss: 0.7234046280384064 \n",
      "At 1342 epoch, Training Loss: 0.609804430603981 \n",
      "At 1342 epoch, Validation Loss: 0.7244026064872741 \n",
      "At 1343 epoch, Training Loss: 0.6075781967490911 \n",
      "At 1343 epoch, Validation Loss: 0.7208093106746675 \n",
      "At 1344 epoch, Training Loss: 0.609492475539446 \n",
      "At 1344 epoch, Validation Loss: 0.7227095574140548 \n",
      "At 1345 epoch, Training Loss: 0.6050093017518516 \n",
      "At 1345 epoch, Validation Loss: 0.7232306718826294 \n",
      "At 1346 epoch, Training Loss: 0.6102571550756696 \n",
      "At 1346 epoch, Validation Loss: 0.7260003209114074 \n",
      "At 1347 epoch, Training Loss: 0.6086245357990264 \n",
      "At 1347 epoch, Validation Loss: 0.7220132678747176 \n",
      "At 1348 epoch, Training Loss: 0.6128828495740892 \n",
      "At 1348 epoch, Validation Loss: 0.7206894814968109 \n",
      "At 1349 epoch, Training Loss: 0.6061912957578894 \n",
      "At 1349 epoch, Validation Loss: 0.7212050706148149 \n",
      "At 1350 epoch, Training Loss: 0.6118812739849093 \n",
      "At 1350 epoch, Validation Loss: 0.7231411010026932 \n",
      "At 1351 epoch, Training Loss: 0.6113310433924201 \n",
      "At 1351 epoch, Validation Loss: 0.7216117978096008 \n",
      "At 1352 epoch, Training Loss: 0.6093772456049918 \n",
      "At 1352 epoch, Validation Loss: 0.7232793837785721 \n",
      "At 1353 epoch, Training Loss: 0.6065157439559697 \n",
      "At 1353 epoch, Validation Loss: 0.7244356900453568 \n",
      "At 1354 epoch, Training Loss: 0.6085009647533297 \n",
      "At 1354 epoch, Validation Loss: 0.7234674274921418 \n",
      "At 1355 epoch, Training Loss: 0.6072118911892177 \n",
      "At 1355 epoch, Validation Loss: 0.7213207572698592 \n",
      "At 1356 epoch, Training Loss: 0.6077472742646937 \n",
      "At 1356 epoch, Validation Loss: 0.7226623952388764 \n",
      "At 1357 epoch, Training Loss: 0.6076769914478063 \n",
      "At 1357 epoch, Validation Loss: 0.7242641419172287 \n",
      "At 1358 epoch, Training Loss: 0.605828908085823 \n",
      "At 1358 epoch, Validation Loss: 0.7246498018503188 \n",
      "At 1359 epoch, Training Loss: 0.6172647826373576 \n",
      "At 1359 epoch, Validation Loss: 0.7233365774154663 \n",
      "At 1360 epoch, Training Loss: 0.6103358875960114 \n",
      "At 1360 epoch, Validation Loss: 0.7260200560092926 \n",
      "At 1361 epoch, Training Loss: 0.6113208845257759 \n",
      "At 1361 epoch, Validation Loss: 0.7187935203313827 \n",
      "At 1362 epoch, Training Loss: 0.6130187265574929 \n",
      "At 1362 epoch, Validation Loss: 0.7215844660997391 \n",
      "At 1363 epoch, Training Loss: 0.6138015169650318 \n",
      "At 1363 epoch, Validation Loss: 0.7213493078947069 \n",
      "At 1364 epoch, Training Loss: 0.6104797124862674 \n",
      "At 1364 epoch, Validation Loss: 0.7272542238235472 \n",
      "At 1365 epoch, Training Loss: 0.608414893224835 \n",
      "At 1365 epoch, Validation Loss: 0.7218224465847014 \n",
      "At 1366 epoch, Training Loss: 0.6136703137308358 \n",
      "At 1366 epoch, Validation Loss: 0.7214496374130248 \n",
      "At 1367 epoch, Training Loss: 0.6137879490852353 \n",
      "At 1367 epoch, Validation Loss: 0.7273921489715576 \n",
      "At 1368 epoch, Training Loss: 0.6098246127367019 \n",
      "At 1368 epoch, Validation Loss: 0.722196400165558 \n",
      "At 1369 epoch, Training Loss: 0.6149350717663763 \n",
      "At 1369 epoch, Validation Loss: 0.7235489636659622 \n",
      "At 1370 epoch, Training Loss: 0.6109127044677736 \n",
      "At 1370 epoch, Validation Loss: 0.7218261271715166 \n",
      "At 1371 epoch, Training Loss: 0.6088976021856067 \n",
      "At 1371 epoch, Validation Loss: 0.7240062683820724 \n",
      "At 1372 epoch, Training Loss: 0.6108488757163286 \n",
      "At 1372 epoch, Validation Loss: 0.721632143855095 \n",
      "At 1373 epoch, Training Loss: 0.6069148167967795 \n",
      "At 1373 epoch, Validation Loss: 0.7250779360532761 \n",
      "At 1374 epoch, Training Loss: 0.609157995134592 \n",
      "At 1374 epoch, Validation Loss: 0.7254439204931259 \n",
      "At 1375 epoch, Training Loss: 0.6101371321827174 \n",
      "At 1375 epoch, Validation Loss: 0.7186328828334809 \n",
      "At 1376 epoch, Training Loss: 0.6081784896552563 \n",
      "At 1376 epoch, Validation Loss: 0.7228744506835938 \n",
      "At 1377 epoch, Training Loss: 0.6142871353775267 \n",
      "At 1377 epoch, Validation Loss: 0.7209863543510437 \n",
      "At 1378 epoch, Training Loss: 0.6065869595855476 \n",
      "At 1378 epoch, Validation Loss: 0.722797730565071 \n",
      "At 1379 epoch, Training Loss: 0.6099252354353666 \n",
      "At 1379 epoch, Validation Loss: 0.7263757109642027 \n",
      "At 1380 epoch, Training Loss: 0.6076324015855784 \n",
      "At 1380 epoch, Validation Loss: 0.7252245604991913 \n",
      "At 1381 epoch, Training Loss: 0.6094671614468099 \n",
      "At 1381 epoch, Validation Loss: 0.7250855326652527 \n",
      "At 1382 epoch, Training Loss: 0.6112259216606618 \n",
      "At 1382 epoch, Validation Loss: 0.7219743251800538 \n",
      "At 1383 epoch, Training Loss: 0.6132068846374752 \n",
      "At 1383 epoch, Validation Loss: 0.722447469830513 \n",
      "At 1384 epoch, Training Loss: 0.6110402140766387 \n",
      "At 1384 epoch, Validation Loss: 0.7247423350811003 \n",
      "At 1385 epoch, Training Loss: 0.6062489237636326 \n",
      "At 1385 epoch, Validation Loss: 0.7241230100393296 \n",
      "At 1386 epoch, Training Loss: 0.611408904194832 \n",
      "At 1386 epoch, Validation Loss: 0.7195622891187667 \n",
      "At 1387 epoch, Training Loss: 0.608787208795547 \n",
      "At 1387 epoch, Validation Loss: 0.7222333043813706 \n",
      "At 1388 epoch, Training Loss: 0.6073074270039804 \n",
      "At 1388 epoch, Validation Loss: 0.7232296824455262 \n",
      "At 1389 epoch, Training Loss: 0.6134670313447718 \n",
      "At 1389 epoch, Validation Loss: 0.72032550573349 \n",
      "At 1390 epoch, Training Loss: 0.6090972170233726 \n",
      "At 1390 epoch, Validation Loss: 0.7262219041585922 \n",
      "At 1391 epoch, Training Loss: 0.6135714717209337 \n",
      "At 1391 epoch, Validation Loss: 0.728478753566742 \n",
      "At 1392 epoch, Training Loss: 0.6090668678283692 \n",
      "At 1392 epoch, Validation Loss: 0.7215258181095123 \n",
      "At 1393 epoch, Training Loss: 0.6071620427072046 \n",
      "At 1393 epoch, Validation Loss: 0.7203437238931656 \n",
      "At 1394 epoch, Training Loss: 0.6116372585296628 \n",
      "At 1394 epoch, Validation Loss: 0.727712658047676 \n",
      "At 1395 epoch, Training Loss: 0.6140129446983336 \n",
      "At 1395 epoch, Validation Loss: 0.7258714199066162 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 1396 epoch, Training Loss: 0.6098794076591731 \n",
      "At 1396 epoch, Validation Loss: 0.7232773423194884 \n",
      "At 1397 epoch, Training Loss: 0.6091445069760084 \n",
      "At 1397 epoch, Validation Loss: 0.7225793719291689 \n",
      "At 1398 epoch, Training Loss: 0.6102808769792322 \n",
      "At 1398 epoch, Validation Loss: 0.724188542366028 \n",
      "At 1399 epoch, Training Loss: 0.6021409120410683 \n",
      "At 1399 epoch, Validation Loss: 0.7256837874650955 \n",
      "At 1400 epoch, Training Loss: 0.6153021369129421 \n",
      "At 1400 epoch, Validation Loss: 0.7255779176950454 \n",
      "At 1401 epoch, Training Loss: 0.6109287217259406 \n",
      "At 1401 epoch, Validation Loss: 0.7255405277013779 \n",
      "At 1402 epoch, Training Loss: 0.6058427464216953 \n",
      "At 1402 epoch, Validation Loss: 0.7219895601272582 \n",
      "At 1403 epoch, Training Loss: 0.6153145633637909 \n",
      "At 1403 epoch, Validation Loss: 0.7238256007432937 \n",
      "At 1404 epoch, Training Loss: 0.6083691917359828 \n",
      "At 1404 epoch, Validation Loss: 0.7253229767084122 \n",
      "At 1405 epoch, Training Loss: 0.6078311577439308 \n",
      "At 1405 epoch, Validation Loss: 0.7250153720378876 \n",
      "At 1406 epoch, Training Loss: 0.6095986116677523 \n",
      "At 1406 epoch, Validation Loss: 0.7175220280885696 \n",
      "At 1407 epoch, Training Loss: 0.6139172878116369 \n",
      "At 1407 epoch, Validation Loss: 0.7222812443971635 \n",
      "At 1408 epoch, Training Loss: 0.609585278481245 \n",
      "At 1408 epoch, Validation Loss: 0.7219265729188918 \n",
      "At 1409 epoch, Training Loss: 0.6124364521354438 \n",
      "At 1409 epoch, Validation Loss: 0.7220379263162614 \n",
      "At 1410 epoch, Training Loss: 0.6161029152572158 \n",
      "At 1410 epoch, Validation Loss: 0.7254428982734679 \n",
      "At 1411 epoch, Training Loss: 0.6109520390629771 \n",
      "At 1411 epoch, Validation Loss: 0.7255497395992279 \n",
      "At 1412 epoch, Training Loss: 0.6133910156786442 \n",
      "At 1412 epoch, Validation Loss: 0.7231386274099351 \n",
      "At 1413 epoch, Training Loss: 0.609831452742219 \n",
      "At 1413 epoch, Validation Loss: 0.7225316911935806 \n",
      "At 1414 epoch, Training Loss: 0.6102022927254439 \n",
      "At 1414 epoch, Validation Loss: 0.7210182577371597 \n",
      "At 1415 epoch, Training Loss: 0.6062816772609945 \n",
      "At 1415 epoch, Validation Loss: 0.7238245904445648 \n",
      "At 1416 epoch, Training Loss: 0.6107889534905552 \n",
      "At 1416 epoch, Validation Loss: 0.7203979492187501 \n",
      "At 1417 epoch, Training Loss: 0.6119271151721472 \n",
      "At 1417 epoch, Validation Loss: 0.7225899934768677 \n",
      "At 1418 epoch, Training Loss: 0.6126638207584615 \n",
      "At 1418 epoch, Validation Loss: 0.7211804330348968 \n",
      "At 1419 epoch, Training Loss: 0.6094784237444402 \n",
      "At 1419 epoch, Validation Loss: 0.7230644106864929 \n",
      "At 1420 epoch, Training Loss: 0.6073913481086497 \n",
      "At 1420 epoch, Validation Loss: 0.7194165021181107 \n",
      "At 1421 epoch, Training Loss: 0.6114546071738006 \n",
      "At 1421 epoch, Validation Loss: 0.7226331710815429 \n",
      "At 1422 epoch, Training Loss: 0.6114773366600276 \n",
      "At 1422 epoch, Validation Loss: 0.7232549667358399 \n",
      "At 1423 epoch, Training Loss: 0.6044715885072945 \n",
      "At 1423 epoch, Validation Loss: 0.7237936109304427 \n",
      "At 1424 epoch, Training Loss: 0.6109878478571773 \n",
      "At 1424 epoch, Validation Loss: 0.7230188995599747 \n",
      "At 1425 epoch, Training Loss: 0.6083511579781773 \n",
      "At 1425 epoch, Validation Loss: 0.7229372650384903 \n",
      "At 1426 epoch, Training Loss: 0.6088740535080432 \n",
      "At 1426 epoch, Validation Loss: 0.720619311928749 \n",
      "At 1427 epoch, Training Loss: 0.6036051824688914 \n",
      "At 1427 epoch, Validation Loss: 0.7221733391284941 \n",
      "At 1428 epoch, Training Loss: 0.6140766605734821 \n",
      "At 1428 epoch, Validation Loss: 0.720956951379776 \n",
      "At 1429 epoch, Training Loss: 0.6074241671711205 \n",
      "At 1429 epoch, Validation Loss: 0.7239813208580017 \n",
      "At 1430 epoch, Training Loss: 0.6092011069878938 \n",
      "At 1430 epoch, Validation Loss: 0.7222157895565033 \n",
      "At 1431 epoch, Training Loss: 0.6125933323055505 \n",
      "At 1431 epoch, Validation Loss: 0.7249613106250762 \n",
      "At 1432 epoch, Training Loss: 0.6071144837886094 \n",
      "At 1432 epoch, Validation Loss: 0.7232645511627198 \n",
      "At 1433 epoch, Training Loss: 0.6114020604640239 \n",
      "At 1433 epoch, Validation Loss: 0.7248782187700271 \n",
      "At 1434 epoch, Training Loss: 0.6058180265128615 \n",
      "At 1434 epoch, Validation Loss: 0.7215769857168198 \n",
      "At 1435 epoch, Training Loss: 0.6108138438314198 \n",
      "At 1435 epoch, Validation Loss: 0.7236263453960418 \n",
      "At 1436 epoch, Training Loss: 0.6083250444382425 \n",
      "At 1436 epoch, Validation Loss: 0.7263419717550277 \n",
      "At 1437 epoch, Training Loss: 0.6126897899433972 \n",
      "At 1437 epoch, Validation Loss: 0.7240847200155258 \n",
      "At 1438 epoch, Training Loss: 0.6105330411344762 \n",
      "At 1438 epoch, Validation Loss: 0.7201482743024826 \n",
      "At 1439 epoch, Training Loss: 0.6104739300906661 \n",
      "At 1439 epoch, Validation Loss: 0.7215527832508087 \n",
      "At 1440 epoch, Training Loss: 0.6135264161974189 \n",
      "At 1440 epoch, Validation Loss: 0.7247958034276963 \n",
      "At 1441 epoch, Training Loss: 0.6140364762395617 \n",
      "At 1441 epoch, Validation Loss: 0.723111093044281 \n",
      "At 1442 epoch, Training Loss: 0.6110813714563846 \n",
      "At 1442 epoch, Validation Loss: 0.7223472148180009 \n",
      "At 1443 epoch, Training Loss: 0.613684386759996 \n",
      "At 1443 epoch, Validation Loss: 0.7227133810520172 \n",
      "At 1444 epoch, Training Loss: 0.6110364876687526 \n",
      "At 1444 epoch, Validation Loss: 0.7254231989383697 \n",
      "At 1445 epoch, Training Loss: 0.6170623883605003 \n",
      "At 1445 epoch, Validation Loss: 0.722471469640732 \n",
      "At 1446 epoch, Training Loss: 0.6055069744586947 \n",
      "At 1446 epoch, Validation Loss: 0.7255843549966811 \n",
      "At 1447 epoch, Training Loss: 0.6047120071947578 \n",
      "At 1447 epoch, Validation Loss: 0.722475302219391 \n",
      "At 1448 epoch, Training Loss: 0.6120853994041681 \n",
      "At 1448 epoch, Validation Loss: 0.7285218358039856 \n",
      "At 1449 epoch, Training Loss: 0.6129965022206304 \n",
      "At 1449 epoch, Validation Loss: 0.7220604062080384 \n",
      "At 1450 epoch, Training Loss: 0.6102655746042732 \n",
      "At 1450 epoch, Validation Loss: 0.7259505391120911 \n",
      "At 1451 epoch, Training Loss: 0.6092809421941636 \n",
      "At 1451 epoch, Validation Loss: 0.7237846553325653 \n",
      "At 1452 epoch, Training Loss: 0.6120248375460507 \n",
      "At 1452 epoch, Validation Loss: 0.7235147297382356 \n",
      "At 1453 epoch, Training Loss: 0.6056550247594714 \n",
      "At 1453 epoch, Validation Loss: 0.7254551708698272 \n",
      "At 1454 epoch, Training Loss: 0.6101780772209167 \n",
      "At 1454 epoch, Validation Loss: 0.7253910571336746 \n",
      "At 1455 epoch, Training Loss: 0.6089067133143546 \n",
      "At 1455 epoch, Validation Loss: 0.7215812474489213 \n",
      "At 1456 epoch, Training Loss: 0.6149173852056263 \n",
      "At 1456 epoch, Validation Loss: 0.7211096376180647 \n",
      "At 1457 epoch, Training Loss: 0.612004745006561 \n",
      "At 1457 epoch, Validation Loss: 0.7244294613599777 \n",
      "At 1458 epoch, Training Loss: 0.6138007096946237 \n",
      "At 1458 epoch, Validation Loss: 0.7230013132095336 \n",
      "At 1459 epoch, Training Loss: 0.6086081080138682 \n",
      "At 1459 epoch, Validation Loss: 0.7204830199480056 \n",
      "At 1460 epoch, Training Loss: 0.6095496047288183 \n",
      "At 1460 epoch, Validation Loss: 0.7256054401397705 \n",
      "At 1461 epoch, Training Loss: 0.6095017030835149 \n",
      "At 1461 epoch, Validation Loss: 0.7217356443405151 \n",
      "At 1462 epoch, Training Loss: 0.6045163981616495 \n",
      "At 1462 epoch, Validation Loss: 0.7204883962869644 \n",
      "At 1463 epoch, Training Loss: 0.6101864583790301 \n",
      "At 1463 epoch, Validation Loss: 0.7235216557979584 \n",
      "At 1464 epoch, Training Loss: 0.6143492043018342 \n",
      "At 1464 epoch, Validation Loss: 0.7240351051092148 \n",
      "At 1465 epoch, Training Loss: 0.6092802736908196 \n",
      "At 1465 epoch, Validation Loss: 0.722018885612488 \n",
      "At 1466 epoch, Training Loss: 0.6106737714260816 \n",
      "At 1466 epoch, Validation Loss: 0.7195555895566941 \n",
      "At 1467 epoch, Training Loss: 0.610725735127926 \n",
      "At 1467 epoch, Validation Loss: 0.7232385933399201 \n",
      "At 1468 epoch, Training Loss: 0.6104342292994261 \n",
      "At 1468 epoch, Validation Loss: 0.7267845988273621 \n",
      "At 1469 epoch, Training Loss: 0.6079833544790748 \n",
      "At 1469 epoch, Validation Loss: 0.7205731868743896 \n",
      "At 1470 epoch, Training Loss: 0.6163341559469703 \n",
      "At 1470 epoch, Validation Loss: 0.7240885406732558 \n",
      "At 1471 epoch, Training Loss: 0.6138493653386832 \n",
      "At 1471 epoch, Validation Loss: 0.7267059445381165 \n",
      "At 1472 epoch, Training Loss: 0.6142868982627991 \n",
      "At 1472 epoch, Validation Loss: 0.7242372751235963 \n",
      "At 1473 epoch, Training Loss: 0.6128845736384394 \n",
      "At 1473 epoch, Validation Loss: 0.722552001476288 \n",
      "At 1474 epoch, Training Loss: 0.6081365905702112 \n",
      "At 1474 epoch, Validation Loss: 0.723051643371582 \n",
      "At 1475 epoch, Training Loss: 0.6099798846989871 \n",
      "At 1475 epoch, Validation Loss: 0.7227064043283463 \n",
      "At 1476 epoch, Training Loss: 0.6041197352111342 \n",
      "At 1476 epoch, Validation Loss: 0.7237261682748795 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 1477 epoch, Training Loss: 0.6116507917642592 \n",
      "At 1477 epoch, Validation Loss: 0.7217859655618668 \n",
      "At 1478 epoch, Training Loss: 0.6048964738845828 \n",
      "At 1478 epoch, Validation Loss: 0.7259552001953125 \n",
      "At 1479 epoch, Training Loss: 0.6170310262590646 \n",
      "At 1479 epoch, Validation Loss: 0.7228781282901764 \n",
      "At 1480 epoch, Training Loss: 0.6137287013232705 \n",
      "At 1480 epoch, Validation Loss: 0.7247400045394896 \n",
      "At 1481 epoch, Training Loss: 0.6070867989212275 \n",
      "At 1481 epoch, Validation Loss: 0.7253125876188278 \n",
      "At 1482 epoch, Training Loss: 0.608153034001589 \n",
      "At 1482 epoch, Validation Loss: 0.7226353526115418 \n",
      "At 1483 epoch, Training Loss: 0.6113193802535533 \n",
      "At 1483 epoch, Validation Loss: 0.7236706644296645 \n",
      "At 1484 epoch, Training Loss: 0.6113011203706264 \n",
      "At 1484 epoch, Validation Loss: 0.7213210403919219 \n",
      "At 1485 epoch, Training Loss: 0.6156514078378676 \n",
      "At 1485 epoch, Validation Loss: 0.7214361280202866 \n",
      "At 1486 epoch, Training Loss: 0.6072723988443612 \n",
      "At 1486 epoch, Validation Loss: 0.7202403843402865 \n",
      "At 1487 epoch, Training Loss: 0.6169659703969961 \n",
      "At 1487 epoch, Validation Loss: 0.7228891521692276 \n",
      "At 1488 epoch, Training Loss: 0.6137875188142062 \n",
      "At 1488 epoch, Validation Loss: 0.7234718948602675 \n",
      "At 1489 epoch, Training Loss: 0.6095665194094178 \n",
      "At 1489 epoch, Validation Loss: 0.7244825512170793 \n",
      "At 1490 epoch, Training Loss: 0.6111167110502718 \n",
      "At 1490 epoch, Validation Loss: 0.7285349786281586 \n",
      "At 1491 epoch, Training Loss: 0.6089924007654188 \n",
      "At 1491 epoch, Validation Loss: 0.7224693030118942 \n",
      "At 1492 epoch, Training Loss: 0.614951053634286 \n",
      "At 1492 epoch, Validation Loss: 0.725861978530884 \n",
      "At 1493 epoch, Training Loss: 0.6088206686079499 \n",
      "At 1493 epoch, Validation Loss: 0.7232286036014557 \n",
      "At 1494 epoch, Training Loss: 0.6122548207640648 \n",
      "At 1494 epoch, Validation Loss: 0.720894968509674 \n",
      "At 1495 epoch, Training Loss: 0.609609152004123 \n",
      "At 1495 epoch, Validation Loss: 0.7202970951795578 \n",
      "At 1496 epoch, Training Loss: 0.6080226657912137 \n",
      "At 1496 epoch, Validation Loss: 0.7202911972999573 \n",
      "At 1497 epoch, Training Loss: 0.6124859444797037 \n",
      "At 1497 epoch, Validation Loss: 0.7247064918279648 \n",
      "At 1498 epoch, Training Loss: 0.6032444067299365 \n",
      "At 1498 epoch, Validation Loss: 0.7223331928253173 \n",
      "At 1499 epoch, Training Loss: 0.6119304023683068 \n",
      "At 1499 epoch, Validation Loss: 0.7217252403497696 \n",
      "At 1500 epoch, Training Loss: 0.6086673885583883 \n",
      "At 1500 epoch, Validation Loss: 0.7206891685724259 \n"
     ]
    }
   ],
   "source": [
    "n_epoch=1500\n",
    "model,train_loss,valid_loss=train(n_epoch,loaders,model,optimizer,criterion,scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc1.weight',\n",
       "              tensor([[-0.1656,  0.0392, -0.0051, -0.2042,  0.1813, -0.0169,  0.1077, -0.0742,\n",
       "                        0.1655, -0.0224, -0.1163,  0.1937, -0.1649, -0.0184,  0.0381,  0.1334,\n",
       "                       -0.0251,  0.0939, -0.0118, -0.1188,  0.0775,  0.0005,  0.0784],\n",
       "                      [-0.0006,  0.0801,  0.2120,  0.0813, -0.2377,  0.2340, -0.1544, -0.0193,\n",
       "                        0.0576,  0.0682, -0.0900,  0.0157, -0.0516,  0.1615,  0.2130,  0.1983,\n",
       "                        0.0340, -0.1824, -0.0631, -0.0096,  0.0703,  0.1750, -0.1239],\n",
       "                      [ 0.1957, -0.0664, -0.0235, -0.1441,  0.0232, -0.0572,  0.0526, -0.0403,\n",
       "                        0.0847,  0.0285,  0.1969, -0.0562,  0.1238,  0.0974,  0.0263, -0.2512,\n",
       "                       -0.1707, -0.1192, -0.1197,  0.1437,  0.0799,  0.0138, -0.0081],\n",
       "                      [ 0.1225,  0.0065,  0.1563, -0.1751,  0.2427, -0.1543, -0.0152, -0.2534,\n",
       "                        0.1579,  0.1454, -0.0976, -0.0492,  0.0746,  0.1406,  0.0803,  0.2094,\n",
       "                       -0.2023, -0.0500,  0.1475, -0.0703, -0.0993, -0.0051,  0.0675],\n",
       "                      [ 0.1806, -0.0436,  0.1122, -0.0658,  0.0268,  0.1120,  0.0236,  0.1254,\n",
       "                        0.0751,  0.0143, -0.1226,  0.1692,  0.1809,  0.0346,  0.0324,  0.0469,\n",
       "                        0.0913,  0.1150, -0.1340,  0.2442, -0.1776,  0.1121, -0.0575],\n",
       "                      [ 0.2375, -0.0366, -0.1614, -0.1457,  0.0259, -0.0509,  0.0441, -0.0648,\n",
       "                        0.0458,  0.0278, -0.1992, -0.1301, -0.0805,  0.0587,  0.1653,  0.0044,\n",
       "                       -0.0509, -0.0777,  0.0499, -0.1681,  0.1730,  0.1384, -0.1044],\n",
       "                      [-0.0305,  0.0979, -0.1513, -0.1662,  0.0890, -0.1275,  0.0885,  0.0219,\n",
       "                       -0.1703, -0.0704,  0.1621, -0.0060,  0.0329, -0.1006, -0.1989, -0.0583,\n",
       "                        0.1349,  0.1140, -0.0390,  0.1205,  0.1524,  0.0661,  0.1729],\n",
       "                      [-0.0814,  0.1385,  0.0132, -0.0994, -0.1606, -0.0786, -0.0267, -0.1766,\n",
       "                        0.1361,  0.1861, -0.0424, -0.1509,  0.1148,  0.0290,  0.0786, -0.1641,\n",
       "                       -0.0414,  0.2018,  0.0135, -0.1290, -0.0542,  0.1432, -0.1043],\n",
       "                      [-0.1106, -0.1069,  0.0822, -0.0055, -0.0124,  0.1579,  0.1816,  0.2463,\n",
       "                        0.1971, -0.0287, -0.1801,  0.0041,  0.1777,  0.1930, -0.1820, -0.1035,\n",
       "                       -0.0398,  0.1003, -0.2199,  0.2313,  0.0215,  0.1226, -0.0274],\n",
       "                      [ 0.0516,  0.1417,  0.2063,  0.1607,  0.0324,  0.0975,  0.1561,  0.1539,\n",
       "                       -0.0370,  0.0025, -0.0031,  0.0166,  0.0707,  0.0053, -0.1506, -0.1213,\n",
       "                       -0.0387, -0.0325,  0.0849, -0.1979, -0.1356,  0.1234, -0.0593],\n",
       "                      [-0.1705,  0.0126, -0.0261, -0.1223,  0.0688,  0.0276, -0.1315,  0.0081,\n",
       "                       -0.0908, -0.2214, -0.1912,  0.1122, -0.1614,  0.0332,  0.0338, -0.0133,\n",
       "                       -0.1381,  0.1080, -0.1355, -0.1068, -0.0173, -0.1452, -0.0942],\n",
       "                      [ 0.0301, -0.0925, -0.1203,  0.0118,  0.2279, -0.1389, -0.1517,  0.0190,\n",
       "                       -0.0548, -0.1778,  0.0864,  0.0828,  0.1150, -0.0929, -0.0371,  0.1917,\n",
       "                       -0.2191,  0.0811,  0.2233,  0.1413,  0.0327,  0.0727, -0.0953],\n",
       "                      [ 0.0211,  0.0601, -0.1829, -0.1484, -0.0034,  0.1085, -0.0643, -0.0237,\n",
       "                        0.1534,  0.0704, -0.2078,  0.1069,  0.0836, -0.0457,  0.0367,  0.1051,\n",
       "                        0.0646, -0.2072,  0.0080, -0.0644, -0.0880,  0.0511, -0.1873],\n",
       "                      [ 0.1364,  0.0496, -0.0832, -0.0488, -0.0233,  0.1368, -0.0737,  0.0294,\n",
       "                       -0.1632,  0.1584,  0.0902,  0.0137,  0.0478,  0.0091, -0.1781,  0.1345,\n",
       "                        0.2094,  0.0833,  0.2137, -0.0581,  0.2604,  0.0860,  0.0640],\n",
       "                      [ 0.0254, -0.2054, -0.0941,  0.1027, -0.0496,  0.1023, -0.0992,  0.0804,\n",
       "                       -0.1051, -0.1084, -0.0419, -0.1259, -0.1370, -0.0859, -0.2276,  0.0874,\n",
       "                       -0.0674, -0.1062,  0.0375, -0.3035,  0.0786,  0.1055,  0.1577],\n",
       "                      [-0.0958, -0.1302, -0.1451,  0.1271, -0.1058, -0.0374, -0.1774,  0.0326,\n",
       "                        0.1116,  0.1494,  0.2273,  0.1791, -0.1969, -0.1550, -0.1122,  0.1621,\n",
       "                        0.0307, -0.1939, -0.1437,  0.1822,  0.1968,  0.0078, -0.1483],\n",
       "                      [-0.1053,  0.1541, -0.1978, -0.2306, -0.2353, -0.1099,  0.0691,  0.0797,\n",
       "                        0.2041, -0.0973,  0.0488,  0.1699,  0.0219, -0.0493, -0.0459, -0.1192,\n",
       "                        0.2297, -0.1501, -0.1047, -0.0682,  0.0753,  0.1524,  0.0576],\n",
       "                      [ 0.0089, -0.1540, -0.1117,  0.2177,  0.0243, -0.1388,  0.1292,  0.2253,\n",
       "                        0.1129, -0.0283, -0.0289,  0.0873,  0.1134, -0.2156, -0.1578, -0.0257,\n",
       "                       -0.0759, -0.0710,  0.1346,  0.2186, -0.1403,  0.1574, -0.0576],\n",
       "                      [-0.0808, -0.0874,  0.0754,  0.0888, -0.0396, -0.2044, -0.2477, -0.0838,\n",
       "                       -0.0394, -0.1853, -0.1351, -0.0909, -0.1476,  0.1456, -0.0963, -0.1654,\n",
       "                        0.1922,  0.0316, -0.1187,  0.0046, -0.2084, -0.0286, -0.1638],\n",
       "                      [ 0.0949,  0.1368, -0.0614,  0.1192, -0.1525, -0.1184,  0.0631,  0.1747,\n",
       "                        0.0730,  0.1096,  0.1944,  0.1956, -0.1061,  0.1390, -0.0770,  0.0689,\n",
       "                       -0.0937,  0.1828,  0.0571,  0.2614, -0.0799, -0.0972,  0.0018],\n",
       "                      [-0.0894,  0.1780, -0.1444,  0.0097,  0.0348, -0.0823, -0.0670, -0.2379,\n",
       "                        0.1295,  0.1781, -0.1221,  0.1239, -0.1404, -0.0271, -0.2180,  0.0193,\n",
       "                        0.0297, -0.0707,  0.0289,  0.0615, -0.1242, -0.0836,  0.1023],\n",
       "                      [-0.0478, -0.2401,  0.1712,  0.0693,  0.0648, -0.0202,  0.2212, -0.1752,\n",
       "                        0.1225,  0.0829, -0.1429,  0.0873,  0.1823,  0.0027,  0.1238, -0.0149,\n",
       "                        0.0818, -0.1457,  0.0133, -0.0932, -0.1022, -0.0888, -0.1015],\n",
       "                      [-0.1350,  0.0962,  0.1435, -0.1380,  0.1533, -0.0415, -0.1962,  0.1572,\n",
       "                        0.0476, -0.1126,  0.1661, -0.0716, -0.1683,  0.1344, -0.1950, -0.1250,\n",
       "                       -0.1201, -0.0879,  0.1770, -0.1117, -0.1797, -0.1156,  0.1629],\n",
       "                      [ 0.1165, -0.0469, -0.1235,  0.1658, -0.1079,  0.1071,  0.2033, -0.0580,\n",
       "                        0.1735,  0.2476,  0.1068, -0.0533,  0.1064,  0.1155,  0.0615,  0.2033,\n",
       "                        0.0388, -0.0541,  0.0188,  0.0916, -0.1290, -0.0540,  0.1759],\n",
       "                      [-0.0998,  0.2203,  0.1276,  0.1350,  0.1299, -0.1446,  0.1728, -0.1993,\n",
       "                       -0.0686,  0.1823,  0.1799, -0.0450,  0.0422, -0.1817, -0.0064,  0.0226,\n",
       "                       -0.1039, -0.0363,  0.0215,  0.0351, -0.1170, -0.1959,  0.0624],\n",
       "                      [-0.0767,  0.0149, -0.1317, -0.0949, -0.0686, -0.1407, -0.0736,  0.0434,\n",
       "                        0.0586,  0.1069, -0.1320,  0.0991,  0.0686, -0.1433, -0.0328,  0.0035,\n",
       "                       -0.0811,  0.0105,  0.0648,  0.0998, -0.0568,  0.0431, -0.1978],\n",
       "                      [ 0.0722,  0.0889,  0.0003, -0.0737, -0.1746, -0.2208, -0.1757,  0.0004,\n",
       "                       -0.1328,  0.0802, -0.0512,  0.0058,  0.1416,  0.1687, -0.0942, -0.0698,\n",
       "                        0.1606, -0.1030, -0.0109, -0.0913, -0.2387,  0.1446,  0.1370],\n",
       "                      [-0.0622,  0.0029, -0.0718, -0.1664, -0.0829, -0.1143,  0.1044,  0.0546,\n",
       "                        0.0569,  0.1232,  0.0251, -0.0109,  0.0977, -0.0678, -0.1198,  0.0296,\n",
       "                        0.1031, -0.2166,  0.0171,  0.1819,  0.0575,  0.2048,  0.0347],\n",
       "                      [ 0.0166,  0.0134,  0.1395,  0.0533,  0.2463, -0.2286,  0.0319, -0.0300,\n",
       "                       -0.1744,  0.2106, -0.1454, -0.1115,  0.1168, -0.0873, -0.1999,  0.1447,\n",
       "                        0.0964, -0.0898, -0.0583, -0.1367,  0.1383, -0.0433,  0.0939],\n",
       "                      [-0.2495, -0.0928,  0.1242,  0.1305, -0.1322, -0.0861,  0.0548, -0.1217,\n",
       "                        0.0927, -0.0676,  0.0636,  0.0620, -0.0327, -0.2005, -0.2060, -0.1086,\n",
       "                        0.2123,  0.2515, -0.0988, -0.1056,  0.1240,  0.0777,  0.0943],\n",
       "                      [-0.1078,  0.1108, -0.0998,  0.0451,  0.0530,  0.1862, -0.1467,  0.0445,\n",
       "                       -0.0155, -0.1207, -0.1438, -0.0461, -0.1020,  0.1036,  0.1359,  0.1001,\n",
       "                       -0.0879, -0.1515, -0.0432,  0.0703, -0.0024, -0.0600, -0.1472],\n",
       "                      [ 0.0706,  0.0968,  0.1101, -0.0366,  0.1619, -0.0608,  0.0366, -0.1404,\n",
       "                        0.0663, -0.1434,  0.2135, -0.0060,  0.1296, -0.1933,  0.1760, -0.0208,\n",
       "                        0.1603, -0.1741,  0.1823,  0.0851,  0.1347, -0.1023,  0.0244]])),\n",
       "             ('fc1.bias',\n",
       "              tensor([ 1.9092e-05,  8.6598e-05, -1.6393e-05,  9.2150e-05,  1.0596e-04,\n",
       "                       8.9784e-05,  7.1503e-06, -7.3277e-06,  5.3024e-06,  6.3748e-06,\n",
       "                      -1.1449e-04,  1.4010e-05, -6.7840e-06, -1.0066e-04, -1.6315e-04,\n",
       "                       6.7367e-06, -1.5621e-05,  7.0458e-05,  1.2093e-04, -4.2540e-05,\n",
       "                       3.0666e-05, -2.2189e-05, -1.1151e-05,  2.3838e-04, -1.8429e-05,\n",
       "                      -1.7549e-06,  1.8407e-05,  4.4088e-05, -2.9431e-06, -2.7720e-05,\n",
       "                      -3.0498e-07,  2.0404e-04])),\n",
       "             ('norm1.weight',\n",
       "              tensor([0.9398, 1.0371, 0.9118, 0.9503, 0.9025, 0.9753, 1.0310, 0.9768, 0.9612,\n",
       "                      0.9715, 1.0409, 0.9750, 0.9943, 0.9766, 0.9854, 0.9770, 0.9330, 1.0176,\n",
       "                      1.0295, 0.9434, 0.9543, 0.9820, 0.9653, 0.9838, 0.9834, 0.9206, 0.9019,\n",
       "                      0.9636, 1.0072, 1.0078, 0.9111, 0.9962])),\n",
       "             ('norm1.bias',\n",
       "              tensor([-0.0286,  0.0134,  0.0028,  0.0008, -0.0894,  0.0318,  0.0391, -0.0013,\n",
       "                      -0.0113, -0.0316, -0.0002, -0.0154,  0.0191,  0.0150,  0.0638, -0.0274,\n",
       "                      -0.0157, -0.0148,  0.0552,  0.0035, -0.0803,  0.0205,  0.0692, -0.0076,\n",
       "                       0.0103, -0.0419, -0.0486, -0.0468,  0.0213,  0.1013, -0.0725,  0.0252])),\n",
       "             ('norm1.running_mean',\n",
       "              tensor([ 0.0791,  0.3330,  0.0092,  0.1807,  0.5478, -0.1483,  0.0666, -0.1317,\n",
       "                       0.3540,  0.2592, -0.6828,  0.0732, -0.1747,  0.5447, -0.4951, -0.1239,\n",
       "                      -0.1310,  0.1194, -0.8033,  0.5377, -0.2468,  0.0323, -0.2850,  0.6615,\n",
       "                       0.0548, -0.3095, -0.2212,  0.0939, -0.0041, -0.1200, -0.2148,  0.3816])),\n",
       "             ('norm1.running_var',\n",
       "              tensor([0.0006, 0.0010, 0.0015, 0.0006, 0.0008, 0.0011, 0.0010, 0.0006, 0.0010,\n",
       "                      0.0009, 0.0011, 0.0007, 0.0011, 0.0009, 0.0008, 0.0019, 0.0010, 0.0008,\n",
       "                      0.0009, 0.0013, 0.0009, 0.0010, 0.0013, 0.0008, 0.0013, 0.0007, 0.0009,\n",
       "                      0.0004, 0.0012, 0.0005, 0.0006, 0.0012])),\n",
       "             ('norm1.num_batches_tracked', tensor(800)),\n",
       "             ('fc2.weight',\n",
       "              tensor([[-2.9119e-02, -3.8131e-02, -4.9202e-02,  1.1437e-01, -6.8473e-04,\n",
       "                       -1.2474e-01, -1.3077e-01, -1.0592e-01, -1.2200e-01,  3.0830e-02,\n",
       "                       -2.0773e-01, -7.2399e-02, -6.6978e-02, -7.3356e-02, -2.8623e-02,\n",
       "                       -1.4771e-02,  1.0915e-01, -9.9879e-02, -1.7938e-01, -1.5257e-01,\n",
       "                        3.2362e-02,  1.4110e-01, -1.1644e-01, -2.3454e-02,  1.6485e-01,\n",
       "                        7.2231e-02, -3.4186e-03, -1.9900e-02, -4.2216e-02, -9.7178e-02,\n",
       "                       -7.5002e-02, -8.5641e-02],\n",
       "                      [-4.0897e-02,  1.1255e-01,  8.5157e-03, -1.1642e-02,  8.7167e-02,\n",
       "                       -1.1261e-01,  7.1577e-02,  6.6013e-02,  7.6103e-02,  1.6761e-01,\n",
       "                        1.4550e-02, -5.5223e-02, -7.8617e-02,  1.4987e-01, -4.4347e-02,\n",
       "                        1.1769e-01, -1.8223e-02,  6.0724e-02,  2.7857e-02,  8.6589e-02,\n",
       "                       -1.0425e-01, -4.8025e-02, -2.9307e-02,  8.4314e-02,  1.5196e-01,\n",
       "                        4.8777e-02, -1.3699e-01,  1.0127e-01,  4.5523e-02,  1.6650e-01,\n",
       "                       -7.3824e-02,  2.6403e-02],\n",
       "                      [-1.0577e-01, -1.8693e-01, -1.1700e-01,  1.1034e-01, -1.7637e-01,\n",
       "                       -1.4625e-01, -1.9872e-02,  1.0841e-01, -1.1716e-01,  5.1848e-03,\n",
       "                        9.1277e-02, -9.3717e-02,  1.0477e-01, -1.2325e-01,  1.2469e-01,\n",
       "                       -3.1216e-02,  8.7820e-02, -1.3774e-01, -6.2653e-02,  1.2049e-01,\n",
       "                        9.4587e-02, -8.4452e-02,  2.0320e-02,  2.5179e-02,  1.4465e-01,\n",
       "                        1.2280e-01,  1.1802e-01, -1.4436e-01,  1.9866e-01, -8.1870e-02,\n",
       "                        8.6856e-02, -5.3449e-02],\n",
       "                      [-1.2231e-01,  5.1053e-02,  5.1995e-02,  7.0824e-02,  4.1329e-03,\n",
       "                        3.7389e-02,  5.4281e-02,  3.6733e-02, -7.8960e-02, -4.6071e-02,\n",
       "                       -1.0830e-01,  1.4226e-01,  8.8169e-02,  4.6666e-02,  4.3688e-02,\n",
       "                       -2.0562e-01,  2.1702e-02, -6.9563e-02, -1.2858e-01, -1.9612e-02,\n",
       "                        1.8435e-02,  9.2223e-02, -4.8665e-02,  1.4116e-01,  3.0813e-02,\n",
       "                       -8.9326e-03,  1.2369e-02, -9.9592e-02,  1.4895e-01, -1.4946e-01,\n",
       "                       -1.2188e-02, -6.3452e-02],\n",
       "                      [ 1.5384e-02, -1.1579e-01, -4.2695e-03, -5.6752e-04,  8.4510e-02,\n",
       "                        1.1920e-01,  1.4589e-01, -4.2608e-02, -5.4342e-02, -1.9808e-01,\n",
       "                        1.1395e-01, -9.4522e-02,  1.1319e-01,  6.7248e-02, -5.0243e-02,\n",
       "                       -9.9538e-02, -9.5299e-02, -2.0882e-02, -4.1690e-02, -6.5277e-02,\n",
       "                        4.2681e-02, -7.8728e-02,  1.7079e-01, -2.0818e-01, -1.3798e-01,\n",
       "                       -1.5290e-01, -1.1580e-01, -5.5395e-02,  1.2338e-01, -6.8169e-02,\n",
       "                       -2.2363e-02,  9.5374e-02],\n",
       "                      [ 2.3013e-02,  1.7480e-01,  1.0731e-01, -4.2246e-02,  1.5554e-02,\n",
       "                       -4.2778e-02, -3.8007e-02,  7.7422e-02, -1.5721e-01,  8.2668e-03,\n",
       "                        1.6976e-01, -3.0769e-03,  1.0797e-01, -1.2223e-01,  9.0072e-03,\n",
       "                        8.4031e-02,  1.1468e-01, -2.6266e-02,  8.6403e-02,  1.4288e-02,\n",
       "                       -1.4274e-01, -1.4685e-01, -6.6507e-02, -8.5612e-03,  2.9854e-02,\n",
       "                       -3.7010e-02,  9.8385e-03,  1.2401e-01, -1.0245e-01,  8.4432e-02,\n",
       "                       -7.7991e-02, -9.2099e-04],\n",
       "                      [-8.2577e-02,  8.8839e-02,  1.0033e-01,  4.8553e-02, -9.0703e-02,\n",
       "                        5.6102e-02, -1.4917e-01, -8.2725e-02, -4.8221e-02, -1.2301e-01,\n",
       "                       -9.7776e-02,  9.0510e-02,  7.3626e-02, -1.0522e-01, -2.5310e-02,\n",
       "                        1.3259e-01, -1.1334e-01,  2.2267e-01, -1.4787e-01, -1.0164e-01,\n",
       "                       -8.9820e-02,  7.6875e-02, -1.4284e-01, -1.9770e-04, -1.1899e-01,\n",
       "                       -1.5302e-01, -1.0515e-01,  8.5076e-02, -3.5632e-02,  4.9292e-02,\n",
       "                       -5.2429e-02,  1.0871e-01],\n",
       "                      [ 1.2700e-01, -1.5966e-01, -1.7043e-01,  1.2320e-01,  1.1914e-01,\n",
       "                       -9.2747e-02,  2.0261e-01,  4.3124e-03,  1.3939e-01, -6.8567e-02,\n",
       "                       -3.7004e-02,  2.8461e-02,  1.2604e-01,  5.9304e-02, -2.4984e-02,\n",
       "                        9.1609e-03, -1.5059e-01, -6.6434e-02, -5.5981e-02,  1.4039e-01,\n",
       "                        1.3695e-01, -5.6105e-02, -4.3627e-02, -3.1554e-02, -6.3070e-02,\n",
       "                        2.3532e-02,  3.9421e-02, -6.7091e-02,  1.4910e-01,  1.3655e-01,\n",
       "                       -1.4722e-02,  6.9419e-02],\n",
       "                      [ 4.5547e-02, -1.4080e-01,  5.5856e-02,  7.3690e-02,  2.1440e-02,\n",
       "                       -4.7516e-02,  4.6310e-02, -1.0173e-01, -3.7159e-02,  1.1032e-01,\n",
       "                       -1.5621e-01, -5.5069e-02, -1.9431e-01,  1.0197e-01,  2.9111e-02,\n",
       "                        3.4926e-02,  2.4388e-02,  1.2922e-01,  9.8379e-02,  1.4048e-01,\n",
       "                       -3.9852e-02,  1.6581e-01, -1.5625e-01, -1.5870e-02,  3.5839e-02,\n",
       "                        6.2966e-02, -9.8483e-02,  1.2528e-01,  1.2961e-01,  1.3665e-01,\n",
       "                       -1.2778e-02, -1.0332e-01],\n",
       "                      [-1.8827e-01, -5.2507e-03, -1.1549e-01, -1.8672e-01, -1.2452e-01,\n",
       "                       -2.8229e-02,  3.2226e-02,  1.5140e-01,  2.0185e-02, -3.5863e-03,\n",
       "                       -3.6609e-02, -1.3300e-01, -1.6998e-01,  8.4450e-02,  1.7160e-01,\n",
       "                       -2.3748e-02, -4.7941e-02,  6.8547e-02,  1.6788e-01, -3.1488e-02,\n",
       "                       -1.2088e-01,  1.2581e-01,  1.3707e-01, -1.4369e-01,  1.6864e-01,\n",
       "                       -2.8506e-02, -5.4622e-02,  4.1072e-02,  5.8691e-02,  1.1239e-01,\n",
       "                       -1.7547e-01, -1.0030e-01],\n",
       "                      [-5.7006e-02,  1.8120e-01, -8.6169e-05, -4.1470e-02,  3.7607e-02,\n",
       "                        1.0264e-01, -1.5163e-01, -1.1120e-01,  6.6966e-02,  1.9017e-02,\n",
       "                        7.5524e-02, -1.0597e-01, -5.1753e-02,  3.5544e-02,  9.9103e-02,\n",
       "                       -1.3340e-01, -1.5315e-02, -3.1891e-02, -1.4436e-01,  1.0926e-01,\n",
       "                        1.4099e-01, -6.9868e-02, -1.3243e-01,  1.1993e-01, -1.1459e-03,\n",
       "                       -1.8611e-01, -9.6646e-02,  3.5075e-02,  8.0929e-02,  5.3867e-02,\n",
       "                        1.6802e-01,  4.4379e-03],\n",
       "                      [ 2.7976e-02,  1.7984e-02,  7.9116e-02, -9.5466e-02,  6.9363e-02,\n",
       "                        1.7112e-01, -1.2128e-01, -1.3974e-01, -5.9709e-02,  1.3307e-01,\n",
       "                       -1.5845e-01,  2.2473e-02, -1.9564e-01, -1.3752e-01, -1.7116e-01,\n",
       "                       -1.0248e-01,  4.5300e-02,  2.5284e-02, -8.1839e-02,  5.9246e-02,\n",
       "                       -4.7886e-02,  8.0026e-02, -6.2688e-02, -1.0868e-01,  8.6811e-02,\n",
       "                        7.0319e-02, -1.3877e-02,  1.2261e-01, -5.8867e-02, -6.0807e-02,\n",
       "                       -5.4765e-02,  2.0598e-01],\n",
       "                      [-1.0962e-01, -3.0540e-02,  7.5054e-02,  9.0568e-02, -1.2250e-01,\n",
       "                        1.6477e-01, -1.1476e-01, -1.3816e-01,  7.5601e-02, -1.4508e-02,\n",
       "                        1.4958e-01, -8.4505e-02, -1.0104e-01,  1.4473e-01, -2.5424e-02,\n",
       "                       -5.6653e-02, -9.2685e-02, -4.0269e-02,  1.9572e-01, -8.4907e-03,\n",
       "                        7.9876e-02,  6.7283e-03, -6.2080e-02, -9.5448e-02, -1.5150e-01,\n",
       "                       -6.5807e-02,  1.2035e-01,  1.8734e-01, -9.4407e-02, -1.8311e-01,\n",
       "                       -7.6787e-02, -1.6027e-01],\n",
       "                      [ 8.2573e-02,  6.2474e-02,  6.4430e-02, -2.0918e-02,  8.6571e-02,\n",
       "                       -1.8935e-01, -1.7727e-01,  7.0351e-02,  1.0627e-01, -1.6642e-03,\n",
       "                        9.7463e-02,  7.0004e-02,  9.1153e-02, -4.8245e-02,  5.2907e-02,\n",
       "                       -7.5487e-02, -1.1351e-01, -8.2734e-02,  1.2366e-01,  2.3834e-02,\n",
       "                        8.3922e-02,  6.6528e-02, -5.8562e-02, -5.3176e-02, -1.9468e-02,\n",
       "                       -1.2083e-01, -1.7279e-01, -1.1347e-01,  8.5082e-02,  8.1531e-02,\n",
       "                        1.3985e-01, -6.0169e-03],\n",
       "                      [ 4.9537e-02, -1.7717e-01,  1.2888e-01, -1.4934e-01, -7.5318e-03,\n",
       "                        3.3104e-04, -1.5874e-01, -1.3048e-01, -1.1791e-01,  1.0549e-03,\n",
       "                        1.2331e-01,  4.7468e-02,  1.4996e-01,  1.4755e-01,  2.0095e-01,\n",
       "                       -1.1829e-01,  4.5378e-03,  1.1676e-01,  1.2285e-03,  1.0267e-02,\n",
       "                        1.2710e-02,  1.4244e-01,  1.3479e-02, -1.8030e-01, -1.0299e-01,\n",
       "                       -4.5889e-02,  1.3133e-01, -3.4007e-02, -1.6826e-01,  1.0683e-01,\n",
       "                        6.6749e-03, -4.5046e-02],\n",
       "                      [-6.6835e-02,  1.5771e-01, -8.0689e-02,  4.7229e-02,  1.3188e-01,\n",
       "                        6.7373e-02, -1.0684e-01,  6.9531e-03,  1.3294e-01,  2.9120e-02,\n",
       "                       -1.1974e-01, -3.7797e-02,  8.8399e-02, -7.5778e-02, -1.9234e-01,\n",
       "                       -1.4349e-02, -1.4307e-01,  1.1014e-01,  5.9502e-02,  7.3057e-02,\n",
       "                        1.0437e-01,  8.1972e-02,  7.1622e-03,  8.2379e-05,  3.6074e-03,\n",
       "                        1.4718e-01, -3.4286e-02,  1.1149e-01, -1.2757e-01, -1.5267e-01,\n",
       "                       -1.2369e-01, -3.2678e-02]])),\n",
       "             ('norm2.weight',\n",
       "              tensor([1.0170, 0.9367, 1.0032, 0.9913, 0.9951, 0.9759, 0.9818, 0.9873, 0.9595,\n",
       "                      0.9245, 0.9234, 0.9775, 0.9740, 0.9607, 0.9794, 1.0389])),\n",
       "             ('norm2.bias',\n",
       "              tensor([ 0.0070, -0.0330, -0.0401,  0.0129,  0.0023, -0.0317,  0.0093,  0.0220,\n",
       "                      -0.0135, -0.0073, -0.0697, -0.0007, -0.0166, -0.0354, -0.0527,  0.0312])),\n",
       "             ('norm2.running_mean',\n",
       "              tensor([-0.5347,  0.3748, -0.0462, -0.0320, -0.2117,  0.1193, -0.2637,  0.1949,\n",
       "                       0.1551, -0.0804,  0.0048, -0.1949, -0.2161,  0.0591, -0.0229, -0.0165])),\n",
       "             ('norm2.running_var',\n",
       "              tensor([0.0833, 0.1546, 0.1753, 0.0547, 0.1056, 0.0627, 0.1562, 0.0858, 0.1023,\n",
       "                      0.1665, 0.0930, 0.0943, 0.1332, 0.1595, 0.1238, 0.1757])),\n",
       "             ('norm2.num_batches_tracked', tensor(800)),\n",
       "             ('fc3.weight',\n",
       "              tensor([[ 0.1315, -0.1070,  0.2713,  0.0682,  0.2508,  0.0235,  0.0949,  0.1333,\n",
       "                        0.1618, -0.0659, -0.0833,  0.1448,  0.0593,  0.1731, -0.0918, -0.2504],\n",
       "                      [ 0.1402, -0.0960, -0.0314,  0.2177, -0.0778, -0.1859,  0.2756,  0.1778,\n",
       "                        0.1884, -0.2043, -0.1519,  0.0745, -0.2898, -0.0774,  0.0851, -0.1931],\n",
       "                      [-0.2387,  0.1733, -0.1388,  0.0986,  0.1472,  0.0070,  0.2520, -0.2087,\n",
       "                        0.1243,  0.0936,  0.1569,  0.1513,  0.2171, -0.1956,  0.1259,  0.1747],\n",
       "                      [ 0.0542,  0.1292, -0.1631, -0.2199, -0.1622, -0.0330, -0.0215,  0.0356,\n",
       "                        0.1018,  0.1799, -0.1526,  0.0616, -0.0883,  0.0339,  0.1962, -0.0501],\n",
       "                      [ 0.1103, -0.0340, -0.2298,  0.1981, -0.0631, -0.1755,  0.1013, -0.0432,\n",
       "                        0.0695, -0.0668, -0.0516,  0.1784,  0.0216, -0.2061, -0.0743, -0.2790],\n",
       "                      [-0.2184,  0.0926, -0.2081, -0.0636,  0.0460,  0.1474,  0.0115,  0.0926,\n",
       "                        0.1507,  0.0890, -0.1227,  0.0219, -0.1033, -0.2306,  0.2918, -0.0696],\n",
       "                      [ 0.1294, -0.1186, -0.0105,  0.0393,  0.0469, -0.1541, -0.1879, -0.2565,\n",
       "                        0.1713,  0.2075, -0.0220, -0.2558,  0.2039,  0.0482,  0.1762, -0.1412],\n",
       "                      [ 0.1442,  0.0038,  0.0795,  0.0488, -0.0803, -0.2123, -0.1441, -0.1932,\n",
       "                       -0.0212, -0.0846,  0.0193,  0.1018,  0.0052,  0.0331,  0.1789,  0.2315]])),\n",
       "             ('norm3.weight',\n",
       "              tensor([1.0112, 0.9574, 0.9753, 0.9716, 0.9890, 1.0055, 1.0066, 0.9882])),\n",
       "             ('norm3.bias',\n",
       "              tensor([-0.0004,  0.0340, -0.0105,  0.0113, -0.0224,  0.0308, -0.0050,  0.0234])),\n",
       "             ('norm3.running_mean',\n",
       "              tensor([ 0.3556, -0.0381,  0.3433, -0.0545, -0.2044, -0.0344, -0.0668,  0.0498])),\n",
       "             ('norm3.running_var',\n",
       "              tensor([0.1277, 0.1966, 0.1652, 0.1333, 0.1632, 0.1829, 0.1687, 0.0866])),\n",
       "             ('norm3.num_batches_tracked', tensor(800)),\n",
       "             ('fc4.weight',\n",
       "              tensor([[-0.1511,  0.2039, -0.1088, -0.2708, -0.1344, -0.2349, -0.0728,  0.0566],\n",
       "                      [ 0.2541, -0.3278, -0.1683, -0.0021, -0.2793, -0.2151,  0.2089, -0.0272],\n",
       "                      [ 0.2162,  0.1151,  0.1223,  0.2994, -0.2261, -0.3479,  0.3261,  0.1870],\n",
       "                      [ 0.1891,  0.1779, -0.1394, -0.0307, -0.1302,  0.2821,  0.3419,  0.2168],\n",
       "                      [ 0.2487, -0.1833,  0.3211, -0.1325,  0.2774,  0.1220, -0.1323, -0.3257]])),\n",
       "             ('norm4.weight',\n",
       "              tensor([1.0569, 0.9198, 1.0329, 0.9482, 1.0458])),\n",
       "             ('norm4.bias',\n",
       "              tensor([ 0.0920, -0.0681,  0.0412, -0.0401,  0.0702])),\n",
       "             ('norm4.running_mean',\n",
       "              tensor([-0.2734, -0.2204,  0.2623,  0.3714,  0.0611])),\n",
       "             ('norm4.running_var',\n",
       "              tensor([0.0982, 0.2098, 0.1228, 0.0939, 0.1076])),\n",
       "             ('norm4.num_batches_tracked', tensor(800)),\n",
       "             ('fc5.weight',\n",
       "              tensor([[-0.4167,  0.2467, -0.3544,  0.3145, -0.2600]])),\n",
       "             ('fc5.bias', tensor([0.2397]))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(\"ccFraud.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x_epochs=np.linspace(1,n_epoch,n_epoch).astype(dtype=int)\n",
    "plt.plot(x_epochs,train_loss)\n",
    "plt.plot(x_epochs,valid_loss)\n",
    "plt.xlabel(\"N Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid()\n",
    "plt.legend([\"Training\",\"Validation\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated AUC: 0.5843169522659218\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "k=np.linspace(0,1,10)\n",
    "TPR=[]\n",
    "FPR=[]\n",
    "for n in k:\n",
    "    outputs=[]\n",
    "    targets=[]\n",
    "    for data,target in test_loader:\n",
    "        output=model(data.float())\n",
    "        output=torch.sigmoid(output.cpu())\n",
    "        output=list(np.where(output<n,0,1)[:])\n",
    "        target=list(target.cpu().numpy())\n",
    "        outputs+=output\n",
    "        targets+=target\n",
    "\n",
    "    outputs_of_model=np.stack(outputs,axis=0).squeeze().astype(dtype=int)\n",
    "    targets_of_model=np.array(targets).astype(dtype=int)\n",
    "    \n",
    "    #detect True Negatives, False Positives, False Negatives, True Positives\n",
    "    TN=np.array(targets_of_model[outputs_of_model==0]==0).sum()\n",
    "    FP=np.array(targets_of_model[outputs_of_model==1]==0).sum()\n",
    "    FN=np.array(targets_of_model[outputs_of_model==0]==1).sum()\n",
    "    TP=np.array(targets_of_model[outputs_of_model==1]==1).sum()\n",
    "\n",
    "    TPR.append(TP/(TP+FN))\n",
    "    FPR.append(FP/(FP+TN))\n",
    "    \n",
    "#Plot True Positive Rate and False Positive Rate\n",
    "plt.plot(FPR,TPR)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "\n",
    "AUC=0.0;\n",
    "for i in range(len(TPR)-1):\n",
    "    AUC+=np.abs((TPR[i]+TPR[i+1])*(FPR[i+1]-FPR[i])/2)\n",
    "  \n",
    "  \n",
    "print(\"Estimated AUC: {}\".format(AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 599\n",
      "True Negatives: 529\n",
      "False Positives: 446\n",
      "False Negatives: 420\n",
      "Model Precision: 0.5732057416267943\n",
      "Model Recall: 0.5878312070657508\n",
      "F1-score: 0.5804263565891472\n"
     ]
    }
   ],
   "source": [
    "outputs=[]\n",
    "targets=[]\n",
    "for data,target in test_loader:\n",
    "    output=model(data.float())\n",
    "    output=torch.sigmoid(output.cpu())\n",
    "    output=list(np.where(output<0.5,0,1)[:])\n",
    "    target=list(target.cpu().numpy())\n",
    "    outputs+=output\n",
    "    targets+=target\n",
    "\n",
    "outputs_of_model=np.stack(outputs,axis=0).squeeze().astype(dtype=int)\n",
    "targets_of_model=np.array(targets).astype(dtype=int)\n",
    "\n",
    "\n",
    "TN=np.array(targets_of_model[outputs_of_model==0]==0).sum()\n",
    "FP=np.array(targets_of_model[outputs_of_model==1]==0).sum()\n",
    "FN=np.array(targets_of_model[outputs_of_model==0]==1).sum()\n",
    "TP=np.array(targets_of_model[outputs_of_model==1]==1).sum()\n",
    "\n",
    "\n",
    "print(\"True Positives: {}\".format(TP))\n",
    "print(\"True Negatives: {}\".format(TN))\n",
    "print(\"False Positives: {}\".format(FP))\n",
    "print(\"False Negatives: {}\".format(FN))\n",
    "\n",
    "\n",
    "precision=TP/(TP+FP)\n",
    "recall=TP/(TP+FN)\n",
    "print(\"Model Precision: {}\".format(precision))\n",
    "print(\"Model Recall: {}\".format(recall))\n",
    "\n",
    "F1=2*precision*recall/(precision+recall)\n",
    "\n",
    "print(\"F1-score: {}\".format(F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5656970912738215"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(TP+TN)/(TP+FP+TN+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
